


\documentclass[a4paper,twoside,12pt,openright,notitlepage]{report}\usepackage[]{graphicx}\usepackage[]{color}
%% maxwidth is the original width if it is less than linewidth
%% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%
\let\hlipl\hlkwb

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt} % 11pt or 12pt?
\usepackage[econ,final,onpaper]{fepphdteses}
%% - prodeb/prodef/pdeqb/pdeec: choose a doctoral program
%% - libre: for any kind of work that is not the thesis (e.g. monografia, workplan, etc.)
%% - jury: copy for the Jury (uses a second committee page)
%% - onpaper: links are not shown (for paper printable versions)
%% - linenum: to include line numbers
%% - backrefs: include back references from bibliography to citation place
%% - final: final copy

\graphicspath{{./figure/}}
\usepackage{rotating}
\usepackage{varioref}
\usepackage{tabu}
\usepackage{titlesec}
\usepackage{time}
\usepackage[nolists,tablesfirst,nomarkers]{endfloat}
%%Macros
\input{./mymacros}
\hfuzz3.5pt


\makeindex
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\begin{document}
\citeindextrue
\setcounter{secnumdepth}{4}

\labelformat{section}{Section~#1}
\labelformat{subsection}{Section~#1}
\labelformat{equation}{Equation~(#1)}
\labelformat{chapter}{Chapter~#1}
\labelformat{table}{Table~#1}
\labelformat{figure}{Figure~#1}

\titleformat{\paragraph}
{\normalfont\normalsize\bfseries}{\theparagraph}{1em}{}
\titlespacing*{\paragraph}
{0pt}{3.25ex plus 1ex minus .2ex}{1.5ex plus .2ex}


        %% Title Page
\author{\bf Artur Aiguzhinov}
\title{Predicting and understanding the rankings of financial analysts}
\supervised{Supervised by:}
\supervisor{Carlos Soares}
\supervisor{Ana Paula Serra}
\thesisdate{2015}

	%% Committee Page
%\committeemember{President}{}
%\committeemember{Referee}{}
%\committeemember{Referee}{}
%\committeemember{Referee}{}
%\signature


\begin{Prolog}
\onehalfspacing
\input{./prolog/bio}
\clearemptydoublepage

\input{./prolog/ack}
\clearemptydoublepage

\input{./prolog/abs-pt}
\clearemptydoublepage

\input{./prolog/abs-en}
\clearemptydoublepage

\pdfbookmark[0]{Table of Contents}{contents}   		%% The Table of Contents (Do not modify)
\singlespace
		\tableofcontents
		\clearemptydoublepage

		\listoffigures
		\addcontentsline{toc}{chapter}{\listfigurename}			%% The List of Figures (Do not modify)
		\clearemptydoublepage

		\listoftables
		\addcontentsline{toc}{chapter}{\listtablename}			%% The List of Tables (Do not modify)
		\clearemptydoublepage

\end{Prolog}

\StartBody


\chapter{Introduction}
\label{intro}
\input{./1intro/introduction}
\cleardoublepage

\chapter{}
\label{ch1}

%\begin{abstract}
%\input{./abstract/ch1-abstract}
%\end{abstract}
%\textit{keywords}: financial analysts; rankings; target price forecasts; earnings forecasts; portfolio management \\
%\textit{JEL}: G11; G14; G24; G29







%' <<set-parent, echo=FALSE, cache=FALSE>>=
%' set_parent('~/Dropbox/workspace/Projects/Thesis/paper.Rnw')
%' @


\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}



\end{knitrout}


\section{Industry Rankings of Financial Analysts}
\label{ch1-sec:ranking}

In the financial literature there has been a long debate on whether financial analysts produce valuable  advice. Some argue that following the advice of financial analysts,  translated as recommendations of buying, holding, or selling a particular stock, does not yield  abnormal returns, i.e.,  returns that are above the required return to compensate for risk. The Efficient Market Hypothesis~\citep{fama1970ecm} states that financial markets are efficient and that any public available information  regarding a stock would be immediately reflected in prices; hence, it would be  impossible to generate abnormal returns based upon past information.

Yet, several authors have since stressed that  there are information-gathering costs and information is not immediately reflected on prices~\citep{grossman1980iie}. As such, prices may not  reflect all the available information at all time because if this were the case, those who spent resources to collect and analyze   information would not have an incentive to do it, because there would not get any compensation for it.

%Many trading strategies try to forecast the price movements relying on the historical prices or estimate the intrinsic value of a company. Obviously, this type of research is associated with significant amount of up-front costs to acquire databases, software, etc. On the other hand, financial analysts have these tools and, presumably, skills to identify  stocks that worth be invested. Thus, for an investor, it is cheaper to follow the recommendations of financial analysts rather than perform a proper stock market analysis.


Some authors show that financial analysts' recommendations create value to investors \citep{womack1996,barber2001}\footnote{\cite{womack1996} finds that  post-recommendation excess returns are not mean-reverting, but are significant and in the direction forecast by the analysts.~\cite{barber2001} finds that over the period of 1986-1996 a portfolio of stocks with the most (least) favorable consensus analyst recommendations yields an average abnormal return of 4.13 (-4.91)\%.}. Assuming that some analysts produce valuable advice it makes sense to rank analysts based on the accuracy of their recommendations.

StarMine rankings are based on financial analysts' accuracy either on TP or EPS forecasts. To rank analysts based on EPS forecasts, StarMine developed a proprietary metric called a Single-stock Estimating Score (SES). This score measures ``... [a] relative accuracy; that is, analysts are compared against their peers. An analyst's SES can range from 0 to 100, with 50 representing the average analyst. To get a score higher than 50, an analyst must make estimates that are both significantly different from and more accurate than other analysts' estimates''\footnote{\url{http://excellence.thomsonreuters.com/award/starmine?award=Analyst+Awards&award_group=Overall+Analyst+Awards}}.


As for target price ranking, StarMine's methodology compares the portfolios based on analysts recommendations. Portfolios are constructed as follows. For each ``Buy'' recommendation, the portfolio is one unit long the stock and simultaneously one unit short the benchmark. ``Strong buy'' gets a larger investment of two units long the stock and two units short the benchmark. ``Hold'' invests one unit in the benchmark (i.e., an excess return of zero). ``Sell'' recommendations work in the reverse way. StarMine re-balances its calculations at the end of each month to adjust for analysts revisions (adding, dropping or altering a rating), and when a stock enters or exits an industry grouping.


Recent evidence suggests that top ranked financial analyst affect market participants: prices seem to react more to the recommendations issued by the top-ranked analysts~\citep{emery2009}. As such, StarMine ranking based models can be used to identify such analysts and generate superior estimates (e.g., SmartEstimates\footnote{\url{http://www.starmine.com/index.phtml?page_set=sm_products&sub_page_set=sm_professional&topic=analytical&section=accurate_estimates}}).
%Evidence shows that market response to analysts' recommendations is stronger when analysts issue them with a good forecasting tracking record~\citep{park2000analyst,loh2006aef}. Thus, it seems to be the case that only a subset of analysts influences the stock prices and, as such, only that subset of analysts deserves to be followed~\citep{loh2011}.



The goal of our study is to evaluate if and how rankings  add value to investors.  With this purpose, we develop several sets of active trading strategies, selecting the stocks most favored by analysts. The first strategy is based on the consensus estimate (giving equal weights to analysts' recommendations). The second set of strategies takes in consideration the analysts' target price and EPS accuracy ranks to form ``smart estimates''. For the latter set of strategies, we analyse different time information sets to define the accuracy of the analysts.


We compare the performance of the strategies based upon two types of rankings (target price and EPS forecast accuracy). By doing this, we indirectly address the ongoing debate in the literature on whether analysts, when issuing the target price reports, rely on simple growth-based models or use more complex models (such as the residual income model of~\cite{ohlson1995}). For example,~\cite{bradshaw2004} suggests that analysts' EPS forecasts are consistent with their price targets and that analysts use growth models based on EPS forecasts to estimate stocks target prices. Differently,~\cite{simon2011} argue that the most accurate analysts rely on more complex models in setting their price targets\footnote{A further major development in the theoretical accounting literature on equity valuation models is the abnormal earnings growth (AEG) model of~\cite{ohlson2005}, which relates share price to the level of expected earnings per share.}.



% and it is, in a sense, an upper bound of all trading strategies.

%In addition, for each strategy, we have three timing scenarios of when we know the analysts' information: we use the \tr{} timing for the case when the information is known at time $t$ (a hypothetical case); we use the \naive{} timing for the $t-1$ case, and we use the \default{} timing~\citep{aiguzhinov2010} for the $t=1 \ldots t-1$ case.

%We claim that the ranking strategies outperform the consensus ones as well as the passive strategy.


%we investigate the problem with two ranking  methods that are used further as the inputs for the trading strategy. The first is the \naive{} rankings and this is simply the last known analysts' actual rankings. The second method is taken from the Machine Learning literature~\citep{aiguzhinov2010} and it is so-called the \default{} ranking. It is the average rank of an analyst since the beginning of the sample period and it encompasses the average analyst's relative performance. We formalize these methods in the appropriate section of the paper.

\section{Trading Strategies}
%- BL model
%- Information sets
\label{ch1-sec:trading}
Our trading strategy uses the framework for  active portfolio management proposed by~\cite{black1992}.  The model incorporates ``views'' in a CAPM framework, forming optimal portfolios in a mean-variance optimization setting. ``Views'' are expectations on individual stocks' future performance.

While in the CAPM model expected returns are a function of systematic risk, in the BL model some stocks can be over- or under-priced and, therefore, their alphas are non-zero. The model blends the subjective views of investors about future performance of a stock with the market implicit returns given by CAPM.

\cite{da2011bl} apply the BL model and use the consensus expected returns as a proxy for views. They report that the resulting strategy outperforms a passive buy-and-hold strategy. Our approach is similar to theirs but we define views not only based on consensus estimates but also on smart estimates that account for previous analysts' TP and EPS accuracy.

Here below we kept the notation in~\cite{black1992}.  $Q$ is the vector of  expected returns for the eligible stocks; $\Omega$ matrix is the confidence of $Q$. Altogether these two reflect the views of a particular analyst or a set of analysts.

To proxy expected returns we compare the analyst' 12-month target price (TP) with today's stock price. Confidence $\Omega$ for stock is based on variation  of forecasts across analysts which is similar to the measure of dispersion in analysts' opinions outlined in~\cite{diether2002}.


We define a trading strategy as follows (\ref{fig:bl}):
\begin{enumerate}
\item  At the beginning of quarter $t$ for each stock $i$,   we define $Q$ and $\Omega$ (see \ref{def-q} and \ref{def-omega});

\item Using the market price information available at the last day of quarter $t-1$, we obtain the market implicit returns for each stock $i$,  and the variance/co-variance matrix;

\item We apply the BL model to get  optimal portfolio weights on the basis of combining views and implicit returns. We  buy or sell stocks accordingly. At the beginning of $t+1$, based on the new views, we set the new portfolio weights following  steps 1--3.
\end{enumerate}


\begin{figure}
\begin{center}
\includegraphics[width=\linewidth]{figure/Black-litterman}
\end{center}
\caption{Trading strategy timeline}
\label{fig:bl}
\ Black-Litterman model inputs are at the beginning of $t$ we apply the BL model and form the active portfolio. At the end of $t$, we evaluate performance.
\end{figure}


\subsection{Defining $Q$}
\label{def-q}

For the consensus strategy, we use median of expected returns for a particular stock $i$:
\begin{equation}
\label{consq}
Q_{cons,i}= \mathrm{median} \left\{r_{j,i}\right\}
%\frac{1}{N} \sum_{j=1}^{N} r_{j,i}
\end{equation}
%$N$ is the number of analysts with a valid TP report and
where $r_{j,i}=TP_{j,i}/P_{i}-1$  is last known analyst's $j$ expected return computed using the analyst price target $TP_{j,i}$ and stock price $P_{i}$\footnote{Consistent with the literature, we use stock price 3 days \emph{ex-ante} the TP announcement. This is done to avoid any information leakage around new TP announcement day~\citep{bonini2010}}.

For the strategies that weight the analysts' estimates of expected return the weight of each analyst $j$ is based on his/her rank such that the top analyst has the weight of 1 and then the weights diminish as the rank increases.


\begin{equation}
\label{eq:weight}
w_{j,i}=1-\frac{rank_{j,i}-\min_i{ \{rank \} }}{\max_i{\{rank \}}}
\end{equation}

The expected rank-weighted return is thus:
\begin{equation}
\label{rankq}
Q_{rank,i}=\frac{\sum_{j=1}^{N} (w_{j,i} \times r_{j,i})}{\sum_{j=1}^{N} w_{j,i}}
\end{equation}
$N$ is the number of analysts.

As mentioned above, we use both target price and EPS accuracy rankings.

\subsubsection{Target Price ranking}
%%% change s to i for stock
Analysts are ranked on the basis of Proportional Mean Absolute Forecast Error (PMAFE) that measures the accuracy of a forecast ~\citep{clement1999,brown2001,ertimur2007}. First,  we define the forecast daily error  $FE_{j,i}$ as the absolute value of the difference between analyst' target price $TP_{j}$ and the daily stock price $P$ for each stock $i$:

\begin{equation}
\label{dfe}
FE_{j,i}^{TP}=|{P_{i}-TP_{j,i}}|
\end{equation}
The PMAFE is given as:
\begin{equation}
\label{tp:pmafe}
PMAFE_{j,i}^{TP}=\frac{FE_{j,i}^{TP}}{\overline{FE_{i}^{TP}}}
\end{equation}
where $\overline{{FE}_{i}^{TP}}$ is the average forecasting error across analysts. The target price is fixed over the quarter unless it gets revised.

The rank  that enters \ref{eq:weight} is average analyst's $PMAFE^{TP}$ over a particular quarter:
\begin{equation}
\overline{PMAFE_{j,i}^{TP}}=\frac{1}{D} \sum_{t=1}^{D} PMAFE_{j,t,i}^{TP}
\end{equation}

\begin{equation}
\label{tp:rank}
rank_{j,i}=\mathrm{rank}_{j=1}^{N} \left\{ \overline{PMAFE_{j,i}^{TP}} \right\}
\end{equation}
where $D$ are the number of trading days in a quarter and $N$ is the number of equity research firms with a valid TP. \ref{fig:example} shows an example.


\subsubsection{EPS ranking}
\label{ch1:sec-eps}
To compute the EPS rankings, we apply the same procedure as above:
\begin{equation}
FE_{j,i}^{EPS}=|{ACT_{i}-PRED_{j,i}}|
\end{equation}
\begin{equation}
PMAFE_{j,i}^{EPS}= \frac{FE_{j,i}^{EPS}}{\overline{FE_{i}^{EPS}}}
\end{equation}
\begin{equation}
\label{ch1-eps:rank}
rank_{j,i}=\mathrm{rank}_{j=1}^{N} \left\{ PMAFE_{j,i}^{EPS} \right\}
\end{equation}
where $ACT_{i}$ and $PRED_{j,i}$ are the actual quarterly EPS and  analyst $j$'s EPS forecast for stock $i$.


\subsection{Defining the confidence of expected returns $\Omega$}
\label{def-omega}
The confidence of $Q$ is given by the coefficient of variation (CV) of forecasting errors. For each stock $i$ is given by:

\begin{equation}
\label{eq-cv}
CV_{i} = \frac{\sigma_i (FE_{i})}{\overline{FE}_{i}}
\end{equation}
where $\sigma_i$ and $\overline{FE}_i$ are the standard deviation and the mean of the forecast errors across analysts for either TP or EPS. A low value of $CV$ reflects consensual estimates of either future prices or EPS.



\subsection{Information sets to define the views}
\label{inf-set}
To proceed with the trading strategy, we need to establish which information we  will be using to build the rankings. These rankings will be the inputs to compute the weighted return estimates (``smart estimates''). Different analysts' ranks are obtained  if we select different time horizons. If we use only the most  recent information, we will capture the recent performance of the analysts. This, of course, is more sensitive to unique episodes (e.g., a quarter which has been surprisingly good or bad). If, alternatively, we opt to incorporate the entire analyst performance, the ranking is less affected by such events, yet it may not reflect the current analyst ability. We use two information sets: the first uses only the  information about the analyst' performance in period $t-1$; the second, uses all the available  information for that particular analyst. We call the former the \naive{} set and the latter the \default{} set.

In addition to these sets,  we also create a hypothetical scenario that assumes we anticipate perfectly the future analyst accuracy performance  that would only be available at the end of $t$.  This represents the perfect foresight strategy. The perfect foresight refers to analyst rankings not stock prices. Therefore, it serves a performance reference point to evaluate the other trading strategies. We call this the \tr{} set.

Formalizing information sets considered are:
\begin{itemize}
\item  the \tr{} set%-- a perfect foresight information:
\begin{equation}
\label{q:true}
\widehat{Q_{t}}=Q_{t}
\end{equation}

\item  the \naive{} set % -- $t-$ information:
\begin{equation}
\label{q:naive}
\widehat{Q_{t}}=Q_{t-1}
\end{equation}

\item  the \default{}  set%-- the entire history of analysts performance:
%\begin{itemize}
%\item ranking based views:
\begin{equation}
\label{q:default}
\widehat{Q_{t}} = \frac{1}{T} \sum_{t=1}^{T} Q_{t}
\end{equation}
%\item consensus based views:
%\begin{equation}
%\label{default.cons}
%\widehat{consQ_{t,i}} = \frac{1}{T} \sum_{T=1}^{T-1} consQ_{t,i}
%\end{equation}
%\end{itemize}
\end{itemize}
where $Q_{t}$ is the analysts' expected rank-weighted stock return (\ref{rankq})

\section{Data and preliminary results}
\label{ch1-sec:rankings}

\subsection{Database and sample}
We focus on the  S\&P500 stocks. We extract the target price information and the EPS forecasts from ThomsonReuters  I/B/E/S detailed history database. The  S\&P500 constituents' list and the stock daily prices are from ThomsonReuters DataStream.


Over the sample period, the total number of equity research firms\footnote{We use words ``analyst'' and ``equity research firm'' interchangeably.} in TP dataset is 351, covering 498 S\&P500 stocks. Given the fact that financial analysts commonly issue TP with the one year horizon\footnote{According to Wharton Research Data Services (WRDS), 92.33\% of all price targets reported in I/B/E/S have a 12-month horizon~\citep{glushkov2009}.}, we assume that analysts keep their TP forecasts valid for one calendar year unless it is revised. After one year we assume that TP recommendation expires.

Consistent with other studies on analysts' expected returns that work with price targets ~\citep{bradshaw2002,brav2003,da2011}, we truncate the sample of $TP/P-1$ at the 5\textsuperscript{th} percentile (values below -0.114) and at the 95\textsuperscript{th} percentile (values above 0.805). This is done due to occurrence of the extreme values. Most of these extreme values are driven by misalignment errors found on I/B/E/S data\footnote{We found some differences between the  DataStream and I/B/E/S the databases. In some cases the stock-splits and the dividends were not properly adjusted.}. To implement ranking, we require that a stock had at least three equity research firms per quarter and that a equity research firm has to be active in covering a particular stock for at least 3 years (12 quarters). After all the  data requirements, our final sample number of equity research firms issued target prices is 158 covering 448 S\&P500 stocks. Overall, the number of observations ($\mathrm{Stock} \times \mathrm{ERF} \times  \mathrm{Quarter}$) is reduced  from 131 068 (initial) to 100 974 (filtered).

In the case of EPS forecasts, the initial file of quarterly EPS forecast consists of 560 ERFs covering 3517 stocks. Considering the ranking data requirement, our final sample of EPS forecasts consist of  157 ERFs covering 402 S\&P500 stocks. The total number of observation is 80 185.


Given that we have two different ranking datasets (based on TP and EPS), we, further, consider S\&P500 stocks that are part of both datasets. We call this sample of stocks as the \same{} and the full sample of S\&P500 stocks as the \all{}.

\ref{tab:ret-stat} shows the distribution of the final sample of target price and EPS data. Panel A shows the number of quarterly target prices per stock. For the sample period (1999-2009), we report that each stock in the \same{} and the \all{} stock sets had on average of 6.915 and 6.71, respectively, quarterly target price reports. Panel B  presents similar statistics of the number of EPS forecasts. The average number of quarterly forecasts for the  \same{} and the \all{} is 6.563 and 6.571, respectively.

\begin{table}
  \caption{Sample Statistics}
  \label{tab:ret-stat}
\ This table shows the average number of target prices  (panel A) and EPS forecasts (panel B) per stock per quarter. Stocks in the \all{} sample are subsamples of the S\&P 500, stocks in the \same{} sample integrate both the EPS and TP datasets.

\begin{tabularx}{\linewidth}{r*{11}{Y}}
\toprule
    & \multicolumn{2}{c}{Min}&\multicolumn{2}{c}{Mean}& \multicolumn{2}{c}{Median}& \multicolumn{2}{c}{Max}&\multicolumn{2}{c}{Std.dev}\\
&\all{}&\same{}&\all{}&\same{}&\all{}&\same{}&\all{}&\same{}&\all{}&\same{}\\
\midrule
%Panel A: TP&&&&&\\
\multicolumn{11}{l}{\textbf{Panel A: TP}} \\
\midrule
%Year & Min& Mean & Median &Max& Std.dev\\
 1999 &    3 &    3 & 4.286 & 4.326 &    4 &    4 &   11 &   11 & 1.657 & 1.668 \\ 
  2000 &    3 &    3 & 4.873 & 4.944 &    4 &    4 &   14 &   14 & 2.009 & 2.017 \\ 
  2001 &    3 &    3 & 5.537 & 5.691 &    5 &    5 &   16 &   16 & 2.453 & 2.481 \\ 
  2002 &    3 &    3 & 6.411 & 6.611 &    6 &    6 &   19 &   19 & 3.108 & 3.145 \\ 
  2003 &    3 &    3 & 7.021 & 7.252 &    6 &    6 &   21 &   21 & 3.524 & 3.573 \\ 
  2004 &    3 &    3 & 7.477 & 7.728 &    7 &    7 &   22 &   22 & 3.671 & 3.726 \\ 
  2005 &    3 &    3 & 7.667 & 7.946 &    7 &    7 &   24 &   24 & 3.736 & 3.744 \\ 
  2006 &    3 &    3 & 7.754 & 8.037 &    7 &    7 &   22 &   22 & 3.561 & 3.532 \\ 
  2007 &    3 &    3 & 7.394 & 7.691 &    7 &    7 &   22 &   22 & 3.494 & 3.429 \\ 
  2008 &    3 &    3 & 6.708 & 6.973 &    6 &    6 &   18 &   18 & 3.023 & 2.964 \\ 
  2009 &    3 &    3 & 5.510 & 5.627 &    5 &    5 &   19 &   19 & 2.412 & 2.433 \\ 
   \midrule 
Total &    3 &    3 & 6.710 & 6.915 &    6 &    6 &   24 &   24 & 3.336 & 3.356 \\ 
  
\end{tabularx}

\begin{tabularx}{\linewidth}{r*{11}{Y}}
\midrule
\multicolumn{11}{l}{\textbf{Panel B: EPS}} \\
\midrule
 1999 &    3 &    3 & 5.673 & 5.643 &    5 &    5 &   17 &   17 & 2.991 & 2.936 \\ 
  2000 &    3 &    3 & 5.433 & 5.426 &    5 &    5 &   17 &   17 & 2.774 & 2.733 \\ 
  2001 &    3 &    3 & 6.020 & 6.048 &    5 &    5 &   18 &   18 & 2.972 & 2.987 \\ 
  2002 &    3 &    3 & 6.338 & 6.357 &    5 &    5 &   21 &   21 & 3.264 & 3.274 \\ 
  2003 &    3 &    3 & 6.523 & 6.532 &    6 &    6 &   24 &   24 & 3.430 & 3.443 \\ 
  2004 &    3 &    3 & 6.947 & 6.978 &    6 &    6 &   22 &   22 & 3.749 & 3.771 \\ 
  2005 &    3 &    3 & 7.205 & 7.216 &    6 &    6 &   24 &   24 & 3.774 & 3.777 \\ 
  2006 &    3 &    3 & 7.494 & 7.467 &    7 &    7 &   26 &   26 & 3.843 & 3.820 \\ 
  2007 &    3 &    3 & 7.134 & 7.091 &    6 &    6 &   21 &   21 & 3.532 & 3.489 \\ 
  2008 &    3 &    3 & 6.378 & 6.353 &    6 &    6 &   22 &   22 & 3.062 & 3.039 \\ 
  2009 &    3 &    3 & 5.811 & 5.773 &    5 &    5 &   20 &   20 & 2.783 & 2.753 \\ 
   \midrule 
Total &    3 &    3 & 6.571 & 6.563 &    6 &    6 &   26 &   26 & 3.423 & 3.412 \\ 
  
\bottomrule
\end{tabularx}
\end{table}

We apply the ranking procedure outline in \ref{def-q} to the two datasets. For target price rankings, we use the average daily errors within one quarter as the measure of analysts' forecasting ability (\ref{tp:pmafe}).

\ref{tab:example} and \ref{fig:example} illustrate an example illustrate how we estimate the \textit{PMAFE}. Four analysts had valid target prices for Amazon for second quarter of 1999. We plot the daily Amazon price against the ERFs' target prices. \ref{tab:example} shows the resulting TP and EPS rankings. On the bases of the average daily errors, LEGG is the most accurate in forecasting stock price and  DLJ is the least accurate. For the EPS case (panel B), PACCREST is the most accurate in EPS forecasting and RBRTSON is the least.

\begin{table}
  \caption{Example of ranking}
  \label{tab:example}
\ This table shows target prices (panel A) and EPS forecasts (panel B) rankings for Amazon (AMZN) for the second quarter of 1999. We apply (\ref{tp:rank}) to obtain the ranks of the ERFs. \emph{TP} are target prices; $\overline{PMAFE}^{TP}$ is the daily average proportional mean adjusted TP error. For the EPS case, $PRED$ are the EPS forecasts issued by the analysts; $PMAFE^{EPS}$ is the proportional mean-adjusted forecast error of quarterly EPS forecasts.

\begin{tabularx}{\linewidth}{r*{4}{Y}}
    \toprule
    \multicolumn{4}{c}{\textbf{Panel A: TP}} \\
ERF/Analyst & \emph{TP} & $\overline{PMAFE}^{TP}$ & $rank^{TP}$ \\ 
  \midrule 
LEGG & 58.50 & 0.05 & 1.00 \\ 
  MONTSEC & 87.50 & 0.46 & 2.00 \\ 
  KAUFBRO & 125.00 & 1.65 & 3.00 \\ 
  DLJ & 140.00 & 1.97 & 4.00 \\ 
  
\midrule
\end{tabularx}
\begin{tabularx}{\linewidth}{r*{4}{Y}}
    \multicolumn{4}{c}{\textbf{Panel B: EPS}} \\
ERF/Analyst & $PRED$ & $PMAFE^{EPS}$ & $rank^{EPS}$ \\ 
  \midrule 
MONTSEC & -0.120 & 0.023 & 1.000 \\ 
  BEAR & -0.130 & 0.068 & 2.500 \\ 
  PACCREST & -0.130 & 0.068 & 2.500 \\ 
  BACHE & -0.135 & 0.091 & 4.000 \\ 
  RBRTSON & -0.140 & 0.114 & 5.000 \\ 
  FBOSTON & -0.145 & 0.137 & 6.000 \\ 
  
\bottomrule
\end{tabularx}
\end{table}

\begin{figure}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}
\includegraphics[width=\maxwidth]{figure/example-fig-1} 

\end{knitrout}
\caption{Amazon daily stock price and ERFs target prices}
\label{fig:example}
\ Target price and actual prices for Amazon  the second quarter of 1999.
\end{figure}


%The daily frequency allows us to capture the analysts' performance with better precision. Panel A of table (\ref{tab:example}) shows an example of rankings based on target prices. As we observe, during the second quarter of 1999, four analysts have valid target prices for Amazon (AMZN) stock. At the end of the quarter, we calculate the average daily error for each of the analysts and apply equations (\ref{tp:pmafe}) and (\ref{tp:rank}) to obtain the ranks of each of the analysts. Figure (\ref{fig:example}) demonstrates the daily AMZN stock price and the levels of the analysts' target prices. The model gives to brokers the follow ranking: LEGG(1), MONTSEC(2), KAUFBRO(3), DLJ(4). In this example, LEGG is the most accurate in forecasting the AMZN stock and DLJ is the least.
\subsection{Ranking contingency results}
\label{ch1-tab:rank-contin}
We consider  three terciles (\textit{top}, \textit{medium}, \textit{bottom}). In one particular quarter ($t$), we place  analysts at one of these bins which corresponds to a tercile. We, then,  check analysts position at the immediate next quarter ($t+1$) and after one year ($t+4$).

Beforehand, we convert the rankings into scores as follows:
\begin{equation}
\label{eq:score}
score_{j,i}=\frac{rank_{j,i}}{\max{rank_i}}
\end{equation}

To get the cross-sectional values of scores across different stocks, we take the average of $score_{j,i}$
\begin{equation}
\label{eq:mean-score}
\overline{score_{j}}= \frac{1}{k} \sum_{s=1}^{k} score_{j,i}
\end{equation}
where $k$ is number of stocks followed by a particular analyst $j$.

\ref{tab:rank-stat} shows a contingency analysis of the ranks.
%Chi square statistics include/ expected frequiency test
%Journ. of Fin economics
Panel A shows the dynamics of each tercile for rankings based on target price  accuracy. We observe that analysts exhibit strong ranking consistency as, on average, they stay at the same tercile after one quarter. For the \all{} stocks,  of the top (bottom) most accurate (inaccurate) analysts in the previous quarter 67.659\% (69.358\%) remain in that same tercile after one quarter. After one year the corresponding figures are lower respectively 46.717\% and 40.877\% for the top and bottom terciles\footnote{For the case of the \same{} stock sample, the results of ranking consistency analysis are similar.}.

In the case of EPS ranking  (panel B) 49.087\% and  28.387\% ( 47.286\% and  31.964\%) of the analysts  remained in the top and bottom terciles, respectively,  after one quarter (year) in the \all{} stock samples.

These results are consistent with the recent findings of~\cite{hilary2013} on analyst forecast consistency.

% Descritive rankings
 \begin{table}
  \caption{Analysts' accuracy consistency}
  \label{tab:rank-stat}

\ This contingency table shows changes in analysts'  \textit{top}, \textit{middle}, \textit{bottom} ranking bins. Panel A (Panel B) depicts the dynamics of the analysts' ranks  based on the accuracy in target prices (EPS forecasts). Stocks in the \all{} sample are subsamples of the S\&P 500, stocks in the \same{} sample integrate both the EPS and TP datasets.

\begin{tabularx}{\linewidth}{r*{10}{Y}}
    \toprule
&&\multicolumn{2}{c}{$top$}&\multicolumn{2}{c}{$middle$}&\multicolumn{2}{c}{$bottom$}&\multicolumn{2}{c}{$Sum$}\\
&&\all{}&\same{}&\all{}&\same{}&\all{}&\same{}&\all{}&\same{}\\
\midrule
\multirow{10}{*}{$t$}&\multicolumn{2}{l}{\textbf{Panel A: TP}}&\multicolumn{6}{c}{$t+1$} \\

 &$top$ & 67.7 & 67.8 & 22.1 & 22.0 & 10.2 & 10.2 & 100.0 & 100.0 \\ 
  &$middle$ & 30.6 & 29.9 & 47.6 & 48.2 & 21.7 & 21.9 & 100.0 & 100.0 \\ 
  &$bottom$ & 13.7 & 13.5 & 16.9 & 17.0 & 69.4 & 69.5 & 100.0 & 100.0 \\ 
    &&\multicolumn{8}{c}{$t+4$}\\ 
&$top$ & 46.7 & 46.5 & 27.7 & 28.0 & 25.5 & 25.5 & 100.0 & 100.0 \\ 
  &$middle$ & 39.1 & 39.2 & 29.0 & 29.0 & 31.9 & 31.7 & 100.0 & 100.0 \\ 
  &$bottom$ & 32.8 & 32.2 & 26.4 & 26.4 & 40.9 & 41.4 & 100.0 & 100.0 \\ 
  
\end{tabularx}
\begin{tabularx}{\linewidth}{r*{10}{Y}}
\midrule
\multirow{10}{*}{$t$}&\multicolumn{2}{l}{\textbf{Panel B: EPS}}&\multicolumn{6}{c}{$t+1$} \\
%&&$top$&$middle$&$bottom$&Sum\\
 &$top$ & 49.1 & 49.1 & 25.6 & 25.6 & 25.8 & 25.8 & 100.5 & 100.5 \\ 
  &$middle$ & 48.5 & 48.6 & 26.0 & 25.8 & 25.9 & 25.8 & 100.4 & 100.2 \\ 
  &$bottom$ & 47.1 & 46.9 & 25.4 & 25.5 & 28.4 & 28.3 & 100.8 & 100.7 \\ 
    &&\multicolumn{8}{c}{$t+4$}\\ 
&$top$ & 47.3 & 47.5 & 28.0 & 27.9 & 25.9 & 25.8 & 101.2 & 101.2 \\ 
  &$middle$ & 46.4 & 46.5 & 26.6 & 26.5 & 27.7 & 27.7 & 100.6 & 100.6 \\ 
  &$bottom$ & 43.2 & 43.2 & 27.5 & 27.5 & 32.0 & 32.1 & 102.7 & 102.7 \\ 
  
\bottomrule
\end{tabularx}
\end{table}

%the next quarter  $top_{t+1}$ group is as follows: round(pt.cont.tab[[1]][1,1],3)*100\%  of analysts stay at the same ranking category from the $top_{t-1}$; round(pt.cont.tab[[1]][2,1],3)*100\% move from the $middle_{t-1}$; round(pt.cont.tab[[1]][3,1],3)*100\% from the $bottom_{t-1}$. It is clear to observe that the $top_{t}$ category, on the average, mostly consists of brokers from the past quarter \emph{middle} rank. %Observe that for the $noRank_{t}$ category, most of the contribution comes from the same category of the past quarter (round(cont.tab[[1]][4,4],3)*100\%). This suggests that analysts are not frequent in issuing target price reports.

%This also represents ... variation to justify the effor of  trying to update the ranking every quarter in order to
%An interesting observation is for the \emph{noRank} category:  round(cont.tab[[2]][4,1],3)*100\% of the analysts who did not have rankings in the current quarter once issued a target price report move to the \emph{top} in the following quarter.

%Panel B depicts the similar analysis for the EPS forecast rankings. For this case, we observe that the composition of the $top_{t}$ is quite similar to the  one we see in target price case: the most contributive group for $top_{t}$ is the \emph{middle} ranks from $t-1$. %However, one interesting observation is that $noRank_{t-1}$ shows significant contribution for each of the ranking groups in $t$: for the \emph{top} ranking it contributes round(eps.cont.tab[[1]][4,1],3)*100\%, for the \emph{middle} --- round(eps.cont.tab[[1]][4,2],3)*100\%, and for the \emph{bottom} --- round(eps.cont.tab[[1]][4,3],3)*100\%. This suggests that analysts, probably, issue the EPS forecasts after the availability of firm's management EPS forecast as that could entail new information about a firm~\citep{hassel1986}.



\subsection{Views: descriptive statistics}

\ref{tab:view-stat} presents the descriptive statistics of the analysts' expected returns conditional on the different information sets.
%Panel A shows the expected returns based on the analysts consensus and Panel B and C are based on the smart estimates (TP and EPS).

\begin{table}
  \caption{Descriptive statistics of views}
  \label{tab:view-stat}
\ This table shows the descriptive statistics of views (expected returns) based on the consensus (median) among the analysts (panel A); target price rankings (panel B); and EPS forecasts rankings (panel C). Stocks in the \all{} sample are subsamples of the S\&P 500, stocks in the \same{} sample integrate both the EPS and TP datasets.

\begin{tabularx}{\linewidth}{r*{7}{Y}}
\toprule
& \multicolumn{2}{c}{Mean (in \%)}&\multicolumn{2}{c}{Median (in \%)}& \multicolumn{2}{c}{Std.dev}\\
&\all{}&\same{}&\all{}&\same{}&\all{}&\same{}\\
\midrule
  \multicolumn{7}{c}{\textbf{Panel A: Consensus}} \\ 
 \midrule 
\tr{} & 18.610 & 18.985 & 16.889 & 17.185 & 0.120 & 0.119 \\ 
  \naive{} & 18.610 & 18.985 & 16.889 & 17.185 & 0.120 & 0.119 \\ 
  \default{} & 18.610 & 18.985 & 16.889 & 17.185 & 0.120 & 0.119 \\ 
  
\end{tabularx}

\begin{tabularx}{\linewidth}{r*{7}{Y}}
  \midrule
  \multicolumn{7}{c}{\textbf{Panel B: TP}} \\ 
 \midrule 
\tr{} & 14.876 & 15.191 & 13.380 & 13.604 & 0.096 & 0.096 \\ 
  \naive{} & 15.742 & 16.102 & 14.314 & 14.541 & 0.098 & 0.098 \\ 
  \default{} & 12.459 & 12.714 & 10.591 & 10.798 & 0.089 & 0.089 \\ 
  
  \end{tabularx}

\begin{tabularx}{\linewidth}{r*{7}{Y}}
  \multicolumn{7}{c}{\textbf{Panel C: EPS}} \\ 
 \midrule 
\tr{} & 14.689 & 14.769 & 13.214 & 13.252 & 0.101 & 0.102 \\ 
  \naive{} & 14.884 & 14.959 & 13.392 & 13.420 & 0.103 & 0.103 \\ 
  \default{} & 12.843 & 12.916 & 11.410 & 11.439 & 0.089 & 0.089 \\ 
  
\bottomrule
\end{tabularx}
\end{table}



The expected returns  are computed comparing TP estimates with actual prices. To form the smart strategies we compute rank-weighted estimates where weights are given either by the TP or the EPS ranks.

\cite{bradshaw2002} reports analyst average expected returns for the period of 2000--2009 and 206 ERFs of 24\%.~\cite{da2011} report an average expected return of 40\% for the period of 1996--2004.~\cite{zhou2013} finds an average expected return of 96\% for the sample period of 2000--2009. These figures suggest that analysts are overly optimistic.
%these values from the historical perspective, i.e., how the values of expected stock returns go inline with historical stock returns.~\cite{bodie2009} show that arithimatic average rate of return for the U.S. large stocks (S\&P 500) for the period of 1926--2005 is 10.17\% and the average rate of excess return is 8.39\% with the risk premium estimated 6--8\%. While it is not the best idea to extrapolate the historical values, still, we can say that the expected stocks return should be around 14--16\%. Clearly, the values presented in the literature shows that analysts are very optimistic in issuing target price reports.

Panel A of \ref{tab:view-stat} show the statistics for the consensus expectations as defined in \ref{consq}. As mentioned above in \ref{inf-set}, the consensus views have equal weights among the analysts, regardless of their ranks; thus, for the case of \tr{}, \naive{}, and \default{}, the median is the same regardless of knowing or not the present or past rankings ($Q_{cons}$ in  \ref{consq}). As such, the mean, median, and standard deviation in the \tr{}, \naive{}, and \default{} information sets are the same. For the sample of \all{} stocks the mean expected return is 18.61\%. However, since views also include the confidence (\ref{eq-cv}), which is based on analysts past performance, the results of the trading strategy based on consensus expectations will be different for the \naive{} and \default{} information sets.

%and for the \same{} sample of stocks the mean is test.a['Total',,'CONS','mean','same'][[2]]*100 \%.

Panel B of the table shows the TP accuracy weighted average expected returns. For the sample of \all{} stocks the weighted average returns for \tr{}, \naive{}, and \default{} information sets are respectively 14.876\%, 15.742\%, and 12.459\%.

%and for the \same{} (test.a['Total',,'PT','mean','same'][[1]]*100 \%, test.a['Total',,'PT','mean','same'][[2]]*100 \%, and test.a['Total',,'PT','mean','same'][[3]]*100\%)

Panel C shows the EPS based weighted expected return. The average return for the \tr{}, \naive{}, and \default{} information sets are respectively 14.689\%, 14.884\%, and 12.843\%.


The statistics for the subsample of stocks that integrate both the TP of the EPS samples are similar.

%and for the \same{} stock sample (test.a['Total',,'EPS','mean','same'][[1]]*100 \%, test.a['Total',,'EPS','mean','same'][[2]]*100 \%, and test.a['Total',,'EPS','mean','same'][[3]]*100\%)

Overall compared to the consensus the ranked weighted expected returns (``smart estimates'') are less optimistic. The \default{} information set shows the lowest values of expected returns among all information sets. For the different stock sets,  the \same{} sample of stocks has higher values of expected returns compared to the \all{} stocks.

Table \ref{tab:stocks} shows the number of active stocks for each of the trading strategies (\textit{CONS}, \textit{TP}, and \textit{EPS}) conditional on considered information sets.
%obtained from the rankings based on the target price accuracy. We observe a reduction in average per stock expected return with the mean of  round(rank.views.full.period['true','mean'],3)*100\%. The case of rankings based on EPS forecasts (panel C) shows even more moderate average stock expected return of  round(pt.eps.rank.views.full.period['true','mean'],3)*100\%. Overall, compared to the consensus, both ranking-based methods exhibit less optimistic expected returns and are pretty close to the historical averages.

\begin{table}
\small\addtolength{\tabcolsep}{-2pt}
  \caption{Number of active stocks}
  \label{tab:stocks}
\ This table shows number of active stocks in each
of the trading strategies conditional on different information sets: \tr{} (panel A), \naive{} (panel B) and \default{} (panel C). Stocks in the \all{} sample are subsamples of the S\&P 500, stocks in the \same{} sample integrate both the EPS and TP datasets.

%\resizebox{\textwidth}{!}{%
%\begin{tabular}{rrrrrrr}
\begin{tabularx}{\linewidth}{r*{7}{Y}}
\toprule
Year&\multicolumn{2}{c}{\textit{CONS}}& \multicolumn{2}{c}{\textit{TP}} &\multicolumn{2}{c}{\textit{EPS}}\\
&\all{}&\same{}&\all{}&\same{}&\all{}&\same{}\\
  \multicolumn{7}{l}{\textbf{Panel A: \tr{}}} \\ 
 \midrule 
1999 &   71 &   67 &   71 &   67 &   42 &   42 \\ 
  2000 &  257 &  235 &  253 &  232 &  191 &  191 \\ 
  2001 &  310 &  281 &  302 &  273 &  244 &  244 \\ 
  2002 &  360 &  321 &  353 &  316 &  289 &  289 \\ 
  2003 &  374 &  334 &  369 &  330 &  299 &  299 \\ 
  2004 &  385 &  341 &  375 &  331 &  318 &  318 \\ 
  2005 &  401 &  352 &  395 &  347 &  337 &  337 \\ 
  2006 &  415 &  358 &  403 &  351 &  337 &  337 \\ 
  2007 &  421 &  361 &  418 &  359 &  351 &  351 \\ 
  2008 &  422 &  360 &  413 &  353 &  339 &  339 \\ 
  2009 &  405 &  351 &  346 &  306 &  306 &  306 \\ 
   \midrule 
Total &  442 &  378 &  442 &  378 &  375 &  375 \\ 
  
\end{tabularx}

\begin{tabularx}{\linewidth}{r*{7}{Y}}
  \multicolumn{7}{l}{\textbf{Panel B: \naive{}}} \\ 
 \midrule 
1999 &   71 &   67 &   71 &   67 &   41 &   41 \\ 
  2000 &  257 &  235 &  257 &  235 &  185 &  186 \\ 
  2001 &  310 &  281 &  310 &  281 &  247 &  247 \\ 
  2002 &  360 &  321 &  360 &  321 &  287 &  287 \\ 
  2003 &  374 &  334 &  374 &  334 &  307 &  307 \\ 
  2004 &  385 &  341 &  385 &  341 &  318 &  318 \\ 
  2005 &  401 &  352 &  401 &  352 &  337 &  337 \\ 
  2006 &  415 &  358 &  415 &  358 &  343 &  343 \\ 
  2007 &  421 &  361 &  421 &  361 &  352 &  352 \\ 
  2008 &  422 &  360 &  422 &  360 &  349 &  349 \\ 
  2009 &  405 &  351 &  405 &  351 &  321 &  321 \\ 
   \midrule 
Total &  442 &  378 &  442 &  378 &  376 &  376 \\ 
  
\end{tabularx}

\begin{tabularx}{\linewidth}{r*{7}{Y}}
  \multicolumn{7}{l}{\textbf{Panel C: \default{}}} \\ 
 \midrule 
1999 &   71 &   67 &   71 &   67 &   51 &   51 \\ 
  2000 &  257 &  235 &  257 &  235 &  204 &  204 \\ 
  2001 &  310 &  281 &  310 &  281 &  261 &  261 \\ 
  2002 &  360 &  321 &  360 &  321 &  302 &  302 \\ 
  2003 &  374 &  334 &  374 &  334 &  317 &  317 \\ 
  2004 &  385 &  341 &  385 &  341 &  332 &  332 \\ 
  2005 &  401 &  352 &  401 &  352 &  344 &  344 \\ 
  2006 &  415 &  358 &  415 &  358 &  353 &  353 \\ 
  2007 &  421 &  361 &  421 &  361 &  358 &  358 \\ 
  2008 &  422 &  360 &  422 &  360 &  359 &  359 \\ 
  2009 &  405 &  351 &  405 &  351 &  350 &  350 \\ 
   \midrule 
Total &  442 &  378 &  442 &  378 &  377 &  377 \\ 
  
\bottomrule
\end{tabularx}
\end{table}


%put in to the different section

\section{Empirical Results}
\label{ch1-sec:results}

We report the results from different trading strategies in Table \ref{tab:strategy}. We split the table into four panels. Panel A shows  the performance for  the passive (market) strategy \textit{Market}. Panels B to D compare the consensus , the TP rank weighted and the EPS rank weighted trading strategies for each of the information availability sets.


\begin{table}[hp]
  \caption{Trading strategies performance: entire period}
  \label{tab:strategy}
  \ This table shows the performance statistics of the different trading strategies. Panel A presents the results for the passive strategy. Panels B, C, and D show the results for the perfect foresight scenario (\tr{}), and, respectively, the scenarios for which we use the most recent  (\naive{}) and  all ranking history of analysts (\default{}) to weight the TP/EPS estimates. Within each panel, we show the strategy results of three views regarding expected return: \textit{CONS} uses the median of the analysts estimates; \textit{TP} is based upon TP accuracy ranking; and \textit{EPS} is based upon  EPS accuracy ranking. Stocks in the \all{} sample are subsamples of the S\&P 500, stocks in the \same{} sample integrate both the EPS and TP datasets. The trading period goes from 2000Q1 until 2009Q4.

\begin{tabularx}{\linewidth}{c*{5}{Y}}
  \toprule
  %\multicolumn{5}{l}{\textbf{Panel A: Market}} \\
Strategy&Annualized return (in \%)&Annualized Std. dev (in \%)&Sharpe ratio&Average num. stock&Average turnover rate \\  \multicolumn{5}{l}{\textbf{Panel A}} \\ 
\textit{Market} & -3.032 & 16.654 & -0.182 &  499 & 0.053 \\ 
  \midrule 

\end{tabularx}

\resizebox{\textwidth}{!}{%
\begin{tabular}{rrrrrrrrrrr}
  \multicolumn{11}{l}{\textbf{Panel B: \tr{} }} \\
  &\all{}&\same{}&\all{}&\same{}&\all{}&\same{}&\all{}&\same{}&\all{}&\same {}\\

  \textit{CONS}&0.116 & 0.434 & 15.948 & 15.995 & 0.007 & 0.027 &  283 &  251 & 0.256 & 0.251 \\ 
   \textit{TP}&4.325 & 4.549 & 14.697 & 14.794 & 0.294 & 0.307 &  283 &  251 & 0.345 & 0.327 \\ 
   \textit{EPS}&0.574 & 0.719 & 15.528 & 15.505 & 0.037 & 0.046 &  205 &  205 & 0.496 & 0.494 \\ 
   \midrule 

\end{tabular}}

\resizebox{\textwidth}{!}{%
\begin{tabular}{rrrrrrrrrrr}
  \multicolumn{11}{l}{\textbf{Panel C: \naive{}}} \\
  \textit{CONS}&0.116 & 0.434 & 15.948 & 15.995 & 0.007 & 0.027 &  283 &  251 & 0.256 & 0.251 \\ 
   \textit{TP}&0.282 & 0.621 & 15.662 & 15.682 & 0.018 & 0.040 &  284 &  251 & 0.264 & 0.256 \\ 
   \textit{EPS}&-0.303 & -0.349 & 16.088 & 16.096 & -0.019 & -0.022 &  206 &  206 & 0.410 & 0.408 \\ 
   \midrule 

\end{tabular}}

\resizebox{\textwidth}{!}{%
\begin{tabular}{rrrrrrrrrrr}
  \multicolumn{11}{l}{\textbf{Panel D: \default{}}} \\
  \textit{CONS}&0.314 & 0.686 & 15.773 & 15.825 & 0.020 & 0.043 &  283 &  251 & 0.228 & 0.223 \\ 
   \textit{TP}&0.689 & 1.056 & 15.565 & 15.485 & 0.044 & 0.068 &  284 &  251 & 0.256 & 0.248 \\ 
   \textit{EPS}&0.746 & 0.717 & 15.444 & 15.481 & 0.048 & 0.046 &  245 &  245 & 0.256 & 0.256 \\ 
  
\bottomrule
\end{tabular}}
\end{table}


\subsection{Passive strategy}

The passive strategy generates an annualized cumulative return of -3.032\% with a Sharpe Ratio  of -0.182 over the period 1999-2001. The average number of stocks held per quarter was 499.975 and the turnover ratio was 0.053, which reflects solely the inclusion and deletions  of the S\&P 500 constituent list.

\subsection{Perfect foresight strategy}
\label{ch1-sec:perfect}
Panel B presents the results for the case of the \tr{} information set. The annualized cumulative returns for each of the active strategies (\textit{TP} and \textit{EPS}) are, respectively, for the \all{} stock sample: 4.325\% and  0.574\%. For the \same{} sample of stocks these are respectively 4.549\% and 0.719\%. The two smart strategies outperform the passive benchmark (-3.032\%).  The consensus strategy annualized returns for the \all{} and the \same{} sample of stocks are respectively   0.116\% and  0.434\%


The results show, as expected, that  investors would better off if they knew in advance who the top analysts in terms of TP or EPS accuracy would be. In any case, the results suggest that the advice of analysts, as a group, are valuable.
%Yet the top analyst

%The result confirms that these strategies set an upper bound for the cumulative return yield. It follows, that we setup a boundary for the results of the feasible strategies: the upper bound is the perfect foresight strategy and the lower bound is the market benchmark.

The \textit{TP} ranking strategy dominates also when we look at the risk-adjusted returns. The Sharpe Ratio for the \all{} (\same{}) stock sample is 0.294 against 0.037, 0.007, and -0.182, respectively for  the \textit{EPS}, \textit{CONS} and the \textit{Market} strategies. In addition, the \textit{TP} strategy dominates  the others if we consider the shorter trading periods (5 years). \ref{tab:substrategy} shows the Sharpe Ratio for six 5-year holding periods. The \textit{TP} strategy wins over the others in every period. The results for the subsample of stocks that integrate both the TP of the EPS datasets are similar.


\begin{table}
  \caption{Trading strategies performance: sub-periods, \all{} sample}
  \label{tab:substrategy}
\ This table presents the annualized return (in \%) and the Sharpe ratio of each of the trading strategies: the passive (\textit{Market}) and the active (consensus and smart estimates) calculated for different holding periods. Panel A represents the perfect foresight information set; panels B and C show, respectively, the results of the strategies using the most recent and all history analysts' performance.

\resizebox{\textwidth}{!}{%
%\begin{tabularx}{\linewidth}{r*{9}{Y}}
\begin{tabular}{rrrrrrrrr}
\toprule
Period&\multicolumn{2}{c}{\textit{Market}}&\multicolumn{2}{c}{\textit{CONS}}&\multicolumn{2}{c}{\textit{TP}}&\multicolumn{2}{c}{\textit{EPS}} \\

& Ann.ret&SR& Ann.ret&SR& Ann.ret&SR& Ann.ret&SR\\

  \multicolumn{9}{l}{\textbf{Panel A: \tr{}}} \\
2000Q1/2004Q4 & -3.401 & -0.201 & 2.663 & 0.167 & 5.844 & 0.395 & 1.890 & 0.124 \\ 
  2001Q1/2005Q4 & -1.539 & -0.093 & 2.669 & 0.170 & 6.119 & 0.425 & 3.945 & 0.270 \\ 
  2002Q1/2006Q4 & 2.567 & 0.196 & 5.734 & 0.447 & 9.124 & 0.757 & 5.877 & 0.492 \\ 
  2003Q1/2007Q4 & 7.919 & 0.925 & 9.548 & 1.106 & 14.715 & 1.915 & 10.681 & 1.364 \\ 
  2004Q1/2008Q4 & -5.667 & -0.435 & -4.708 & -0.343 & 0.908 & 0.070 & -3.133 & -0.232 \\ 
  2005Q1/2009Q4 & -2.662 & -0.158 & -2.367 & -0.146 & 2.827 & 0.189 & -0.725 & -0.045 \\ 
   \midrule 
All period & -3.032 & -0.182 & 0.116 & 0.007 & 4.325 & 0.294 & 0.574 & 0.037 \\ 
  
\midrule
\end{tabular}}

\resizebox{\textwidth}{!}{%
\begin{tabular}{rrrrrrrrr}
%\begin{tabularx}{\linewidth}{r*{9}{Y}}
  \multicolumn{9}{l}{\textbf{Panel B: \naive{}}} \\
2000Q1/2004Q4 & -3.401 & -0.201 & 2.663 & 0.167 & 2.620 & 0.168 & 1.172 & 0.074 \\ 
  2001Q1/2005Q4 & -1.539 & -0.093 & 2.669 & 0.170 & 3.266 & 0.214 & 2.659 & 0.171 \\ 
  2002Q1/2006Q4 & 2.567 & 0.196 & 5.734 & 0.447 & 6.075 & 0.490 & 5.485 & 0.455 \\ 
  2003Q1/2007Q4 & 7.919 & 0.925 & 9.548 & 1.106 & 10.068 & 1.248 & 9.337 & 1.155 \\ 
  2004Q1/2008Q4 & -5.667 & -0.435 & -4.708 & -0.343 & -4.148 & -0.305 & -4.541 & -0.332 \\ 
  2005Q1/2009Q4 & -2.662 & -0.158 & -2.367 & -0.146 & -2.003 & -0.125 & -1.757 & -0.106 \\ 
   \midrule 
All period & -3.032 & -0.182 & 0.116 & 0.007 & 0.282 & 0.018 & -0.303 & -0.019 \\ 
  
\midrule
\end{tabular}}
\resizebox{\textwidth}{!}{%
\begin{tabular}{rrrrrrrrr}
%\begin{tabularx}{\linewidth}{r*{9}{Y}}
  \multicolumn{5}{l}{\textbf{Panel C: \default{}}} \\
2000Q1/2004Q4 & -3.401 & -0.201 & 3.102 & 0.197 & 3.133 & 0.205 & 3.501 & 0.232 \\ 
  2001Q1/2005Q4 & -1.539 & -0.093 & 3.016 & 0.194 & 3.727 & 0.249 & 3.954 & 0.264 \\ 
  2002Q1/2006Q4 & 2.567 & 0.196 & 5.410 & 0.417 & 5.793 & 0.464 & 5.712 & 0.468 \\ 
  2003Q1/2007Q4 & 7.919 & 0.925 & 9.419 & 1.071 & 10.335 & 1.245 & 10.002 & 1.202 \\ 
  2004Q1/2008Q4 & -5.667 & -0.435 & -4.860 & -0.358 & -3.941 & -0.289 & -4.223 & -0.309 \\ 
  2005Q1/2009Q4 & -2.662 & -0.158 & -2.398 & -0.149 & -1.697 & -0.105 & -1.936 & -0.121 \\ 
   \midrule 
All period & -3.032 & -0.182 & 0.314 & 0.020 & 0.689 & 0.044 & 0.746 & 0.048 \\ 
  
\bottomrule
\end{tabular}}
\end{table}

\begin{table}
  \caption{Trading strategies performance: sub-periods, \same{} subsample}
  \label{tab:substrategy-same}
\ This table presents the annualized return (in \%) and the Sharpe ratio of each of the trading strategies for the \same{} set of stocks: the passive (\textit{Market}) and the active (consensus and smart estimates) calculated for different holding periods. Panel A represents the perfect foresight information set; panels B and C show, respectively, the results of the strategies using the most recent and all history analysts' performance.

\resizebox{\textwidth}{!}{%
%\begin{tabularx}{\linewidth}{r*{9}{Y}}
\begin{tabular}{rrrrrrrrr}
\toprule
Period&\multicolumn{2}{c}{\textit{Market}}&\multicolumn{2}{c}{\textit{CONS}}&\multicolumn{2}{c}{\textit{TP}}&\multicolumn{2}{c}{\textit{EPS}} \\

& Ann.ret&SR& Ann.ret&SR& Ann.ret&SR& Ann.ret&SR\\

  \multicolumn{9}{l}{\textbf{Panel A: \tr{}}} \\
2000Q1/2004Q4 & -3.401 & -0.201 & 3.378 & 0.213 & 6.609 & 0.451 & 2.116 & 0.139 \\ 
  2001Q1/2005Q4 & -1.539 & -0.093 & 2.825 & 0.180 & 6.226 & 0.432 & 4.161 & 0.286 \\ 
  2002Q1/2006Q4 & 2.567 & 0.196 & 5.836 & 0.468 & 8.985 & 0.768 & 5.827 & 0.486 \\ 
  2003Q1/2007Q4 & 7.919 & 0.925 & 9.417 & 1.100 & 14.249 & 1.891 & 10.672 & 1.361 \\ 
  2004Q1/2008Q4 & -5.667 & -0.435 & -4.826 & -0.349 & 0.398 & 0.030 & -3.126 & -0.232 \\ 
  2005Q1/2009Q4 & -2.662 & -0.158 & -2.426 & -0.148 & 2.528 & 0.166 & -0.658 & -0.041 \\ 
   \midrule 
All period & -3.032 & -0.182 & 0.434 & 0.027 & 4.549 & 0.307 & 0.719 & 0.046 \\ 
  
\midrule
\end{tabular}}

\resizebox{\textwidth}{!}{%
\begin{tabular}{rrrrrrrrr}
%\begin{tabularx}{\linewidth}{r*{9}{Y}}
  \multicolumn{9}{l}{\textbf{Panel B: \naive{}}} \\
2000Q1/2004Q4 & -3.401 & -0.201 & 3.378 & 0.213 & 3.331 & 0.216 & 1.073 & 0.067 \\ 
  2001Q1/2005Q4 & -1.539 & -0.093 & 2.825 & 0.180 & 3.523 & 0.231 & 2.598 & 0.167 \\ 
  2002Q1/2006Q4 & 2.567 & 0.196 & 5.836 & 0.468 & 6.218 & 0.516 & 5.436 & 0.450 \\ 
  2003Q1/2007Q4 & 7.919 & 0.925 & 9.417 & 1.100 & 9.972 & 1.244 & 9.325 & 1.153 \\ 
  2004Q1/2008Q4 & -5.667 & -0.435 & -4.826 & -0.349 & -4.252 & -0.311 & -4.552 & -0.332 \\ 
  2005Q1/2009Q4 & -2.662 & -0.158 & -2.426 & -0.148 & -2.019 & -0.124 & -1.752 & -0.105 \\ 
   \midrule 
All period & -3.032 & -0.182 & 0.434 & 0.027 & 0.621 & 0.040 & -0.349 & -0.022 \\ 
  
\midrule
\end{tabular}}
\resizebox{\textwidth}{!}{%
\begin{tabular}{rrrrrrrrr}
%\begin{tabularx}{\linewidth}{r*{9}{Y}}
  \multicolumn{5}{l}{\textbf{Panel C: \default{}}} \\
2000Q1/2004Q4 & -3.401 & -0.201 & 3.909 & 0.250 & 3.852 & 0.257 & 3.470 & 0.230 \\ 
  2001Q1/2005Q4 & -1.539 & -0.093 & 3.254 & 0.210 & 4.014 & 0.271 & 3.919 & 0.261 \\ 
  2002Q1/2006Q4 & 2.567 & 0.196 & 5.461 & 0.433 & 5.915 & 0.488 & 5.686 & 0.465 \\ 
  2003Q1/2007Q4 & 7.919 & 0.925 & 9.297 & 1.067 & 10.199 & 1.253 & 9.988 & 1.199 \\ 
  2004Q1/2008Q4 & -5.667 & -0.435 & -4.979 & -0.364 & -4.091 & -0.299 & -4.241 & -0.310 \\ 
  2005Q1/2009Q4 & -2.662 & -0.158 & -2.437 & -0.150 & -1.665 & -0.102 & -1.963 & -0.122 \\ 
   \midrule 
All period & -3.032 & -0.182 & 0.686 & 0.043 & 1.056 & 0.068 & 0.717 & 0.046 \\ 
  
\bottomrule
\end{tabular}}
\end{table}

While this is an hypothetical setting, given that it is not possible to know in advance which analyst will rank first, it suggests that if we can predict the rankings with some accuracy this will be a useful investment trading tool. One of the possibilities is using methods developed in the Machine Learning literature (e.g.,~\cite{aiguzhinov2010,brazdil2003}), where this type of problem (referred as a label ranking problem) has been broadly studied. For example,~\cite{aiguzhinov2010} propose a label ranking algorithm using Bayesian approach to predict the rankings.



\subsection{Feasible strategies}
Panel C of \ref{tab:strategy} shows the performance of the different smart strategies and the consensus strategy in the \naive{} information set. We report the results of forming portfolios with the available stocks within each dataset (\all{}) and the subsample of stocks that include both the TP or the EPS datasets (\same{}).

The \textit{TP} and \textit{CONS} active strategies outperform the \textit{Market} (-3.032\%)  and show positive cumulative annualized returns for the \all{} (\same{}) sample of stocks of 0.282\% (0.621\%) and  0.116\% (0.434\%) respectively. The active strategy based on EPS forecasts, has negative annualized cumulative returns of  -0.303\% and -0.349\% for the \all{} and the \same{} samples respectively.

The risk-adjusted results for this information set shows that, as in the case with the \tr{}, the dominant trading strategy is the \textit{TP} strategy regardless of forming portfolios with \all{} stocks or with the \same{} subsample.
%Sharpe ratios for the \all{} stocks of  bl.results['TP',2,'ann.sr',sets[1]] and for the \same{} of  bl.results['TP',2,'ann.sr',sets[2]] which is higher than that Sharpe Ratios of the \textit{CONS} and  the \textit{EPS} strategies of  bl.results['CONS',2,'ann.sr',sets[1]] and bl.results['EPS',2,'ann.sr',sets[1]] respectively for the \all{} sample  and  of  bl.results['CONS',2,'ann.sr',sets[2]] and bl.results['EPS',2,'ann.sr',sets[2]] respectively for the \same{} sample of stocks.
The results in panel B of \ref{tab:substrategy} show as well that this strategy outperforms the others for all of the shorter trading periods.



Panel D of \ref{tab:strategy} shows interesting and slightly different findings. On one hand, when all the analyst forecast performance track record is included to set the rankings, we observe an increase in annualized cumulative returns and risk-adjusted returns for the smart strategies. Particularly in the case of the EPS strategy, the results suggest that strategies that weight the estimates with accuracy rankings obtained using more information show better performance: when we consider the \all{} sample of stocks, the strategy based on the accuracy of EPS forecasts outperforms the other strategies (annualized cumulative returns of 0.746\%, 0.689\%, 0.314\%, respectively for the \textit{EPS}, \textit{TP}, and \textit{CONS}). The TP strategy dominates when we consider the subsample of stocks that are included in both the TP and EPS datasets (0.717\%, 1.056\%, 0.686\% respectively for the \textit{EPS}, \textit{TP} and \textit{CONS} strategies) but the returns and risk-adjusted improve as well when compared to the figures in Panel C.



%On one hand, when consider \all{} sample of stocks, the  strategy based on the accuracy of EPS forecasts  outperforms the strategies based on the target prices (bl.results['EPS',3,1,sets[1]]\%, bl.results['TP',3,1,sets[1]]\%, bl.results['CONS',3,1,sets[1]]\%, respectively for the \textit{EPS}, \textit{TP}, and \textit{CONS} strategies). On the other hand, the result changes when consider \same{} sample of stocks (bl.results['TP',3,1,sets[2]]\%, bl.results['EPS',3,1,sets[2]]\%, bl.results['CONS',3,1,sets[2]]\%, respectively for the \textit{TP}, \textit{EPS}, and \textit{CONS} strategies).

The analysis of the sub-periods performance of the \textit{TP} and \textit{EPS} -based strategies depicted in \ref{tab:substrategy} shows that the latter outperforms the former in terms of the annualized cumulative return only for the first two periods: in  2000Q1/2004Q4  3.501\% vs. 3.133\%  in 2001Q1/2005Q4 3.954\%  vs. 3.727\% respectively for the \textit{EPS} and the \textit{TP} strategies. For the all the other sup-periods, the annualized cumulative returns of the \textit{EPS} strategy are lower than those of the \textit{TP} strategy.
%This can imply that the quality of EPS forecasts have changed in response to the new SEC legislation that took place in the early 2000s.
\ref{tab:substrategy-same} shows the sub-period results when we consider only the sample of stocks that integrates the TP and EPS dataset. In this setting (\same{}), the \textit{TP} strategy dominates the \textit{EPS} and \textit{CONS} strategies in every sub-period.


%The sub-period results analysis and the dominant out-performance of the \same{} set of stocks relative to the \all{} set can be attributed to the fact that \all{} set for the \textit{EPS} strategy included the stocks


To further investigate the prevailance  of the strategies based on smart estimates  as opposed to consensus (and \Market{}), we perform a pairwise hypothesis test with \emph{null}-hypothesis stating that the difference between the annualized cumulative returns based on smart estimate strategies and those of the consensus strategy (and \Market{}) is equal to zero. \ref{tab:sig} presents t-statistic and the corresponding p-values of this test. Panel A shows the results for the case of ``All vs. \Market{}". We report that all active strategies resulted in the statisitcally significant (at 1\% level) prevailance over the \Market{} for different information sets (\tr{}, \naive{}, and \default{}) as well as different stock sets (\all{} and \same{}).

Panel B of the table presents the t-statistic for the case of ``All vs. \textit{CONS}''. Given the results, we reject the \emph{null}-hypothesis in all of the experiment instances (informations sets and stock sets). In terms of the positive gains, the \textit{TP} strategy demonstrated a statistically significant positive performance over the consensus in all information  sets and for both stock sets. On the other hand, the \textit{EPS} strategy resulted in the statistically significant negative performance over the \textit{CONS} strategy except for the case of \default{} information and \all{} stocks. The results of the test confirms that the strategy based on the rankings of the analysts who issue more accurate target prices outperforms, in terms of the annualized cumulative returns, the strategy based on the consensus among analysts regarding stock target prices.



\ref{fig:bl-results} shows the graphical representation of the cumulative portfolio wealth for  the passive and smart strategies in all the information sets. The $y$-axis is the dollar value of wealth and the $x$-axis is the time starting at January 2000 and ending at December  2009. The active investment management strategies in the \tr{} panel outperform the \emph{Market} and the final value  of the portfolio of the \textit{TP} strategy is well above those of the other alternative strategies.


\begin{figure}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}
\includegraphics[width=\maxwidth]{figure/bl-results-fig-1} 

\end{knitrout}
\caption{Performance of the BL model}
\label{fig:bl-results}
\ Quarterly performance of the cumulative portfolio wealth for all strategies. Panel \tr{} shows the case of the known future information; \naive{} is the case of ranking information know at $t-1$, and the \default{} is the case of using all ranking information for up to $t-1$. \textit{TP} is the strategy with rankings based on the accuracy in target prices, \textit{CONS} is the strategy based on the consensus among the analysts regarding a stock's expected return. \textit{EPS} is the strategy with rankings based on the accuracy of EPS forecasts. Stocks in the \all{} sample are subsamples of the S\&P 500, stocks in the \same{} sample integrate both the EPS and TP datasets. The trading period ranges from 2000Q1 until 2009Q4.
\end{figure}





\begin{longtabu} to \linewidth{l*{5}{Y}}
\caption{Significance of cumulative returns} \\
\label{tab:sig} \\
\multicolumn{5}{l}{\parbox{\linewidth}{The table demonstrates a pairwise statisitical test in difference of the cumulative returns of all strategies vs. \Market{} (Panel A) and vs. \textit{CONS} strategy (Panel B). Case of \tr{} shows  the known future information; \naive{} is the case of ranking information know at $t-1$, and the \default{} is the case of using all ranking information for up to $t-1$. \textit{TP} is the strategy with rankings based on the accuracy in target prices, \textit{CONS} is the strategy based on the consensus among the analysts regarding a stock's expected return. \textit{EPS} is the strategy with rankings based on the accuracy of EPS forecasts. Stocks in the \all{} sample are subsamples of the S\&P 500, stocks in the \same{} sample integrate both the EPS and TP datasets.}}\\
\toprule
  &\multicolumn{2}{c}{\all{}}&\multicolumn{2}{c}{\same{}}\\
  &t value& Pr$(>\vert t\vert)$ & t value & Pr$(>\vert t\vert)$\\
\midrule
\endfirsthead

&\multicolumn{2}{c}{\all{}}&\multicolumn{2}{c}{\same{}}\\
  &t value& Pr$(>\vert t\vert)$ & t value & Pr$(>\vert t\vert)$\\
\midrule
\endhead

\multicolumn{5}{r}{\textit{\footnotesize{... continued next page}}}\\
\endfoot
\endlastfoot

\multicolumn{5}{c}{\textbf{Panel A: \Market{} }} \\
&\multicolumn{4}{l}{\textbf{\tr{}}} \\
  \textit{CONS}&15.247 & 0.000 & 16.857 & 0.000 \\ 
   \textit{TP}&12.661 & 0.000 & 13.799 & 0.000 \\ 
   \textit{EPS}&11.664 & 0.000 & 12.001 & 0.000 \\ 
   \midrule 

&\multicolumn{4}{l}{\textbf{\naive{} }} \\
  \textit{CONS}&15.247 & 0.000 & 16.857 & 0.000 \\ 
   \textit{TP}&14.015 & 0.000 & 15.481 & 0.000 \\ 
   \textit{EPS}&12.655 & 0.000 & 12.640 & 0.000 \\ 
   \midrule 

&\multicolumn{4}{l}{\textbf{\default{} }} \\
  \textit{CONS}&15.858 & 0.000 & 17.456 & 0.000 \\ 
   \textit{TP}&14.314 & 0.000 & 15.833 & 0.000 \\ 
   \textit{EPS}&15.625 & 0.000 & 15.690 & 0.000 \\ 
   \midrule 

\multicolumn{5}{c}{\textbf{Panel B: \textit{CONS} }} \\
&\multicolumn{4}{l}{\textbf{\tr{} }} \\
  \textit{TP}&9.284 & 0.000 & 9.625 & 0.000 \\ 
   \textit{EPS}&-2.352 & 0.024 & -6.358 & 0.000 \\ 
   \textit{Market}&-15.247 & 0.000 & -16.857 & 0.000 \\ 
   \midrule 

&\multicolumn{4}{l}{\textbf{\naive{} }} \\
  \textit{TP}&2.032 & 0.049 & 1.706 & 0.096 \\ 
   \textit{EPS}&-20.130 & 0.000 & -27.074 & 0.000 \\ 
   \textit{Market}&-15.247 & 0.000 & -16.857 & 0.000 \\ 
   \midrule 

&\multicolumn{4}{l}{\textbf{\default{} }} \\
  \textit{TP}&3.182 & 0.003 & 2.892 & 0.006 \\ 
   \textit{EPS}&10.492 & 0.000 & -3.008 & 0.005 \\ 
   \textit{Market}&-15.858 & 0.000 & -17.456 & 0.000 \\ 
   \midrule 

\multicolumn{5}{c}{\textbf{Panel C: \textit{TP} }} \\
&\multicolumn{4}{l}{\textbf{\tr{} }} \\
  \textit{CONS}&-9.284 & 0.000 & -9.625 & 0.000 \\ 
   \textit{EPS}&-12.876 & 0.000 & -15.090 & 0.000 \\ 
   \textit{Market}&-12.661 & 0.000 & -13.799 & 0.000 \\ 
   \midrule 

&\multicolumn{4}{l}{\textbf{\naive{} }} \\
  \textit{CONS}&-2.032 & 0.049 & -1.706 & 0.096 \\ 
   \textit{EPS}&-17.847 & 0.000 & -23.272 & 0.000 \\ 
   \textit{Market}&-14.015 & 0.000 & -15.481 & 0.000 \\ 
   \midrule 

&\multicolumn{4}{l}{\textbf{\default{} }} \\
  \textit{CONS}&-3.182 & 0.003 & -2.892 & 0.006 \\ 
   \textit{EPS}&18.770 & 0.000 & -14.975 & 0.000 \\ 
   \textit{Market}&-14.314 & 0.000 & -15.833 & 0.000 \\ 
   \bottomrule 

\end{longtabu}

%' \begin{table}
%' \caption{Dunn Test}
%' \label{ch1:tab-dun}
%' \ The table presents the Dunn test
%'
%' \begin{tabularx}{\linewidth}{r*{3}{Y}}
%' \toprule
%' Pairs&Z-stat&p-value\\
%' \multicolumn{3}{l}{\bfseries Panel A: \tr{}}\\
%' \midrule
%' <<dun-stat-tr,echo=F,results='asis'>>=
%' print(xtable(dun_t[Method=='true',.(pair,V2,V3)]),only.contents=T,include.rownames=F,NA.string='-',include.colnames=FALSE,hline.after=NULL, format.args=list(big.mark = " ", decimal.mark = "."), sanitize.text.function=function(x) x)
%' @
%' \midrule
%' \multicolumn{3}{l}{\bfseries Panel B: \naive{}}\\
%' <<dun-stat-naive,echo=F,results='asis'>>=
%' print(xtable(dun_t[Method=='recent',.(pair,V2,V3)]),only.contents=T,include.rownames=F,NA.string='-',include.colnames=FALSE,hline.after=NULL, format.args=list(big.mark = " ", decimal.mark = "."), sanitize.text.function=function(x) x)
%' @
%' \midrule
%' \multicolumn{3}{l}{\bfseries Panel C: \default{}}\\
%' <<dun-stat-default,echo=F,results='asis'>>=
%' print(xtable(dun_t[Method=='all-time',.(pair,V2,V3)]),only.contents=T,include.rownames=F,NA.string='-',include.colnames=FALSE,hline.after=NULL, format.args=list(big.mark = " ", decimal.mark = "."), sanitize.text.function=function(x) x)
%' @
%' \bottomrule
%' \end{tabularx}
%' \end{table}

In sum the results of the feasible information sets outlined above suggest that  it is worthwhile to follow the analysts, particularly the top ranked analysts, and are supportive of~\cite{desai2000ass} in that smart strategies based upon analyst accuracy rankings are beneficial for investors.

Further the results show that the values of the annualized cumulative returns are higher in the \default{} information set when compared with those yield by the strategies that use only the most \naive{} ranking information set. This seems to suggest that investors should estimate analysts forecasting skills over a long period of time rather than focusing on the most recent analyst accuracy performance.


%Second, the results show that the values of the annualid cumulative return are higher in the \default{} information sets when compared with \naive{}. This suggests that analysts accumulate forecasting skills overtime which brings more value to investors rather than recent analyst's performance that, presumably, takes advantage of the price momentum.



%The results of trading strategies suggest that the dominant trading strategy is \textit{TP} strategy which is based on the accuracy of analsyts' TP reports. Thus, an investor is better off when following the analysts who issue more accurate target price; hence, rankings are usefull for the investors.

%do identify the best analysts and this findings supports the argument of~\cite{desai2000ass}. In addition, the results show that keeping all available information about analysts' performance does not create any value. As we see, the \default{} strategy is the worse in term of the annualized cumulative returns.

Finally, the results suggest that, from an investor's point of view, following analysts who are accurate in setting price targets is more valuable than following those that are good at forecasting EPS.  This result contradicts the findings of~\cite{bradshaw2004} and supports the argument of~\cite{simon2011} that stock recommendations of the most accurate analysts are not based upon simple valuation models.

%\section{Conclusions}
%\label{ch1-sec:conclusion}

%Some institutions, such as StarMine (ThomsonReuters), rank financial analysts based on EPS and target price accuracy. These rankings are published and are relevant: stocks favored by top ranked analysts will probably receive more attention from investors. Therefore, there is a growing interest in understanding the relative performance of strategies based upon analysts with different forecast accuracy.

%We use the Black-Litterman model. The views are TP or EPS rank-weighted means of analysts forecasted returns. We developed simulations of trading strategies using different information sets to compute the ranks. If we consider that only the information known prior to time $t$ is used to obtain the ranks, investors would be better off following the strategy that weights more heavily the estimates issued  by the most accurate TP forecast analysts and considering the whole performance tracking record of the of the analysts.


%For future work we will developed new methods to forecast analysts rankings so as to get closer to the upper bound of perfect foresight of rankings.
\processdelayedfloats
\cleardoublepage

\chapter{}
\label{ch2}
%\begin{abstract}
%\input{./abstract/ch2-abstract}
%\end{abstract}
%\textit{keywords}: label ranking; naive Bayes \\








%\section{Introduction}
%Label ranking is an increasingly popular topic in the machine learning literature. It studies the problem of learning a mapping from instances to rankings over a finite number of pre-defined labels. In some sense, it is a variation of the conventional classification problem; however, in contrast to the classification settings, where the objective is to assign examples to a specific class, in label ranking we are interested in assigning a complete preference order of labels to every example~\citep{cheng2009}.

%Many different algorithms have been adapted to deal with label ranking such as: decision-trees for label ranking~\citep{cheng2009}, algorithm based on Plackett-Luce model~\citep{cheng2010}, pairwise comparison~\citep{hullermeier}, and k-NN for label ranking~\citep{brazdil2003}. \ref{ch2:lr-summary} outlines the recent developments in solving a label ranking problem.

%In this paper, we introduce the ranking similarity approach. We propose an adaptation of the naive Bayes (NB) algorithm for label ranking. Despite its limitations, NB is an algorithm with successful results in many applications~\citep{domingos1997}. Additionally, the Bayesian framework is well understood in many domains. For instance, we apply this method on the problem of predicting the rankings of financial analysts since in the Financial Economics the Bayesian models are widely used (e.g., the Black-Litterman model for active portfolio management~\citep{black1992}).

%The main idea lies in replacing the probabilities in the Bayes theorem with the distance between rankings. This can be done because it has been shown that there is a parallel between the concepts of distance and likelihood~\citep{vogt2007}. We develop two versions of the algorithm: for discrete and continuous cases.

%The paper is organized as follows: \ref{ch2-sec:learning} provides the formalization of the label ranking problem; \ref{ch2-sec:naivebayes} briefly  describes the naive Bayes algorithm for classification; \ref{ch2-sec:adapting} shows the adaptation of the NB algorithm for label ranking (NB4LR); \ref{ch2-sec:cases} provides some extensions of NB4LR; namely, the scenario of features having a continuous values (\ref{ch2-sec:nbr.cont})  and a case when rankings are part of time series (\ref{ch2-sec:time}); \ref{ch2-sec:data} outlines the datasets used for the experiments;
%section \ref{ch2-sec:metalearning} explains  the problem of metalearning, which will be the application domain for the empirical evaluation;
%\ref{ch2-sec:results} presents empirical results; finally, \ref{ch2-sec:conclusion} concludes with the goals for future work.

 \section{Learning label rankings}
 \label{ch2-sec:learning}

The formalization of a label ranking problem is the following~\citep{vembu2009}. Let $\mathcal{X} \subseteq \{\mathcal{V}_1,\ldots,\mathcal{V}_m\}$ be an instance space of nominal variables, such that $\mathcal{V}_a=\{v_{a,1}, \ldots, v_{a,n_a}\}$ is the domain of nominal variable $a$.  Also, let $\mathcal{L} = \{\lambda_1,\ldots,\lambda_k\}$ be a set of labels, and $\mathcal{Y} = y_{\mathcal{L}}$ be the output space of all possible total orders%
\footnote{A total order is a complete, transitive, and asymmetric relation $\succ$ on $\mathcal{L}$, where $\lambda_i \succ \lambda_j$ indicates that $\lambda_i$ precedes $\lambda_j$. In this paper, given $\mathcal{L}=\{A,B,C\}$, we will use the notation $\{A,C,B\}$ and $\{1,3,2\}$ interchangeably to represent the order $A \succ C \succ B$.} over $\mathcal{L}$ defined on the permutation space $y$. The goal of a label ranking algorithm is to learn a mapping $h: \mathcal{X} \rightarrow \mathcal{Y}$, where $h$ is chosen from a given hypothesis space $\mathcal{H}$, such that a predefined loss function $\ell: \mathcal{H} \times \mathcal{Y} \times \mathcal{Y} \rightarrow \mathbb{R}$ is minimized. The algorithm learns $h$ from a training set $\mathcal{T}=\{x_i,y_i\}_{i \in \{1, \ldots, n\}} \subseteq \mathcal{X} \times \mathcal{Y}$ of $n$ examples, where $x_i = \{x_{i,1}, x_{i,2}, \ldots, x_{i,m} \} \in \mathcal{X}$ and $ y_i = \{y_{i,1}, y_{i,2}, \dots, y_{i,k}\} \in y_{\mathcal{L}}$. Furthermore, we define $y_i^{-1} = \{y_{i,1}^{-1}, y_{i,2}^{-1}, \ldots, y_{i,k}^{-1}\}$ as the order of the labels in example $i$. Given that we are focusing on total orders, $y_i^{-1}$ is a permutation of the set $\{1, 2, \ldots, k\}$ where $y_{i,j}^{-1}$ is the rank of label $\lambda_j$ in example $i$.

Unlike classification, where for each instance $x \in \mathcal{X}$ there is an associated class $y_i \in \mathcal{L}$\footnote{Here, we use both $y_i$ to represent the target class (label) in classification and the target ranking in label ranking to clarify that they are both the target of the learning problem. We will explicitly state the task we are dealing with when it is not clear from the context.}, in label ranking problems there is a ranking of the labels associated with every instance $x$ and the goal is to predict it. This is also different from other ranking problems, such as in information retrieval or recommender systems. In these problems the target variable is a set of ratings or binary relevance labels for each item, and not a ranking.

The algorithms for label ranking can be divided into two main approaches: methods that transform the ranking problem into multiple binary problems and methods that were developed or adapted to predict the rankings. An example of the former is the ranking by pairwise comparisons~\citep{hullermeier}. Some examples of algorithms that are specific for rankings are: the predictive clustering trees method~\citep{todorovski2002}, the similarity-based k-Nearest Neighbor for label ranking~\citep{brazdil2003}, the probabilistic k-Nearest Neighbor for label ranking~\citep{cheng2009} and the linear utility transformation method~\citep{har2002,dekel2003}.

To assess the accuracy of the predicted rankings relative to the corresponding target rankings, a suitable loss function is needed. In this paper we compare two rankings using the Spearman correlation coefficient~\citep{brazdil2003,vembu2009}:
\begin{equation}
\label{ch2-eq00}
 \rho(y,\hat{y})=1-\frac{6\sum_{j=1}^k(y_j-\hat{y}_j)^2}{k^3-k}
\end{equation}
where $y$ and $\hat{y}$\footnote{ In the following, we will use $y_i$ and $y_i$ interchangeably to represent the target ranking.} are, respectively, the target and predicted rankings for a given instance. Two orders with all the labels placed in the same position will have a Spearman correlation of $+1$. Labels placed in reverse order will produce  correlation of $-1$. Thus, the higher the value of $\rho$ the more accurate the prediction is compared to target. The loss function is given by the mean Spearman correlation values (\ref{ch2-eq00}) between the predicted and target rankings, across all examples in the dataset:

\begin{equation}
\label{ch2-loss}
 \ell=\frac{\sum_{i=1}^n \rho(y_i,\hat{y}_i)}{n}
\end{equation}

An extensive survey of label ranking algorithms is given by~\cite{vembu2009}.

\section{The Naive Bayes Classifier}
\label{ch2-sec:naivebayes}

We follow~\cite{mitchell1997} to formalize the naive Bayes classifier. In classification, each instance $x_i\in\mathcal{X}$ is binded to class $y_i\in\mathcal{L}$. The task of a learner is to create a classifier from the training set $\mathcal{T}$.The classifier takes a new, unlabeled instance and assigns it to a class (label).

The naive Bayes method classifies a new instance $x_i$ by determining the most probable target value, $c_{MAP}(x_i)$\footnote{$MAP$ -- Maximum A Posteriori}, given the attribute values that describe the instance:
\begin{equation}
\label{ch2-eq01}
c_{MAP(x_i)}= \argmax_{\lambda \in \mathcal{L}} P(\lambda|x_{i,1}, x_{i,2}, \ldots, x_{i,m})
\end{equation}
where $x_{i,j}$ is the value of attribute $j$ for instance $i$.

The algorithm is based on the  Bayes theorem that establishes the probability of $A$ given $B$ as:
\begin{equation}
\label{ch2-eq04}
 P(A|B)=\frac{P(B|A)P(A)}{P(B)}
\end{equation}
Thus, the Bayes theorem provides a way to calculate the posterior probability of a hypothesis.

Using \ref{ch2-eq04}, we can rewrite \ref{ch2-eq01} as
\begin{align}
\label{ch2-eq02}
 c_{MAP(x_i)}= \argmax_{\lambda\in \mathcal{L}} \frac{P(x_{i,1},x_{i,2}, \ldots, x_{i,m}|\lambda)P(\lambda)}{P(x_{i,1},x_{i,2}, \ldots, x_{i,m})} \notag \\
=\argmax_{\lambda\in \mathcal{L}} P(x_{i,1},x_{i,2} \ldots x_{i,m}|\lambda)P(\lambda)
\end{align}



Computing the likelihood $P(x_{i,1}, x_{i,2}, \ldots, x_{i,m}|\lambda)$ is very complex and requires large amounts of data, in order to produce reliable estimates. Therefore, the naive Bayes classifier makes one simple, hence, naive, assumption that the attribute values are conditionally independent from each other. This implies that the probability of observing the conjunction $x_{i,1},x_{i,2},\ldots,x_{i,m}$ is the product of the probabilities for the individual attributes: $ P(x_{i,1},x_{i,2}, \ldots, x_{i,m}|
\lambda)=\prod_{j=1}^m P(x_{i,j}|\lambda)$. Substituting this expression into \ref{ch2-eq02}, we obtain the naive Bayes classifier:
\begin{equation}
 \label{ch2-eq03}
 c_{nb}(x_i)=\argmax_{\lambda\in \mathcal{L}} P\left(\lambda\right)\prod_{j=1}^m P\left(x_{i,j}|\lambda\right)
\end{equation}

\section{Adapting NB to Ranking}
\label{ch2-sec:adapting}
Consider the classic problem of the ``play/no play'' tennis based on  weather conditions. The naive Bayes classification algorithm can be successfully applied to this problem~\citep[chap. 6]{mitchell1997}. For illustration purposes, we extend this example application to the label ranking setting by replacing the target with a ranking on the preferences of a golf player regarding three golf courts on different days (\ref{ch2-tab01}). The last three columns represent the ranks of the golf courts A, B and C.

\begin{table}
\caption{Example of preferences of a golf player}
\ The table shows preferences of a golf player for the golf courts conditional on different weather conditions.
\begin{center}
\begin{tabular}{cccccccc}
\toprule
Day&Outlook&Temperature&Humidity&Wind&\multicolumn{3}{c}{Ranks}\\
\cline{6-8}
&&&&&A&B&C\\
\midrule
 1 & Sunny & Hot & High & Weak &   1 &   2 &   3 \\ 
  2 & Sunny & Hot & High & Strong &   2 &   3 &   1 \\ 
  3 & Overcast & Hot & High & Weak &   1 &   2 &   3 \\ 
  4 & Rain & Mild & High & Weak &   1 &   3 &   2 \\ 
  5 & Rain & Mild & High & Strong &   1 &   2 &   3 \\ 
  6 & Sunny & Mild & High & Strong &   3 &   2 &   1 \\ 
  
\bottomrule
\end{tabular}
\end{center}
\label{ch2-tab01}
\end{table}

As described earlier, the difference between classification and label ranking lies in the target variable, $y$. Therefore, to adapt NB for ranking we have to adapt the parts of the algorithm that depend on the target variable, namely:
\begin{itemize}
\item prior probability, $P(y)$
\item conditional probability, $P(x|y)$
\end{itemize}

The adaptation should take into account the differences in nature between label rankings and classes. For example, if we consider label ranking as a classification problem, then the prior probability of ranking $\{A,B,C\}$ on the data given in \ref{ch2-tab01} is $P(\{A,B,C\})$ = 3/6 = 0.5, which is quite high. On the other hand, the probability of $\{A,C,B\}$ is quite low, $P(\{A,C,B\})$ = 1/6 = 0.17. However, taking into account the stochastic nature of these rankings~\citep{cheng2009}, it is intuitively clear that the observation of $\{ A,B,C\}$ increases the probability of observing $\{A,C,B\}$ and vice-versa. This affects even rankings that are not observed in the available data. For example, the case of unobserved ranking $\{B,A,C\}$ in \ref{ch2-tab01} would not be entirely unexpected in the future considering a similar observed ranking $\{B,C,A\}$. %$P(\{C,A,B\}=2/6=0.33$.

One approach to deal with stochastic nature characteristic of label rankings is to use ranking distributions, such as the Mallows model (e.g., \citep{lebanon2002,cheng2009}). Alternatively, we may consider that the intuition described above is represented by varying similarity between rankings.

Similarity-based label ranking algorithms have two important properties:
\begin{itemize}
\item they assign non-zero probabilities even for rankings which have not been observed. This property is common to distribution-based methods;
\item they are based on the notion of similarity between rankings, which also underlies the evaluation measures that are commonly used. Better performance is naturally expected by aligning the algorithm with the evaluation measure.
\end{itemize}

Similarity and probability are different concepts and, in order to adapt NB for label ranking based on the concept of similarity, it is necessary to relate them. A parallel has been established between probabilities and the general Euclidean distance measure~\citep{vogt2007}. This work shows that maximizing the likelihood is equivalent to minimizing the distance (i.e., maximizing the similarity) in a Euclidean space.  Although not all assumptions required for that parallel hold when considering distance (or similarity) between rankings, given that the naive Bayes algorithm is known to be robust to violations of its assumptions, we propose a similarity-based adaptation of NB for label ranking.


In the following description, we will retain the probabilistic terminology (e.g., prior probability) from the original algorithm, even though it does not apply for similarity functions. However, in the mathematical notation, we will use the subscript $_{LR}$ to distinguish the concepts. Despite the abuse, we believe this makes the algorithm easier to understand.

We start by defining $\mathcal{S}$ as a similarity matrix between the target rankings in a training set, i.e. $\mathcal{S}_{n \times n}=\rho(y_i,y_j)$. The prior probability of a label ranking is given by:
\begin{equation}
P_{LR}(y) = \frac{\sum_{i=1}^{n} \rho(y,y_i)}{n}
\label{ch2-eq:prior}
\end{equation}

We say that the prior probability is the mean of similarity of a given rankings to all the others. We measure similarity  using the Spearman correlation coefficient (\ref{ch2-eq00}). \ref{ch2-eq:prior} shows the average similarity of one ranking relative to others. The greater the similarity between two particular rankings, the higher is the probability that the next unobserved  ranking will be similar to the known ranking. Take a look at panel A of  \ref{ch2-tab02} with the calculated prior probability for the unique rankings. We also added a column with prior probabilities considering the rankings as one class ($P(y)$).
\begin{table}
\caption{Comparison of probabilities}
\ The table shows the comparison of  values of prior (panel A) and conditional (panel B) probabilities of golf courts rankings from \ref{ch2-tab01} as a classification ($P$)   and as a label ranking ($P_{LR}$) problem.

\begin{tabu} to \textwidth{XXXcc}
\toprule
\multicolumn{5}{l}{\bfseries Panel A: prior probability} \\
\midrule
\multicolumn{3}{c}{$y$}&$P(y)$&$P_{LR}(y)$\\
\midrule
 A & B & C & 0.500 & 0.667 \\ 
  B & C & A & 0.167 & 0.542 \\ 
  A & C & B & 0.167 & 0.708 \\ 
  
\midrule
\end{tabu}
\begin{tabularx}{\textwidth}{XXXcc}
	\multicolumn{5}{l}{\bfseries Panel B: conditional probability} \\
	\midrule
	\multicolumn{3}{c}{$y$}&$P(Outlook=Sunny|y)$ &$P_{LR}(Outlook=Sunny|y)$\\
	\midrule
 A & B & C & 0.333 & 0.312 \\ 
  B & C & A & 1.000 & 0.615 \\ 
  A & C & B & 0.000 & 0.412 \\ 
  
	\bottomrule
\end{tabularx}
\label{ch2-tab02}
\end{table}
As stated above, the ranking $\{A,C,B\}$, due to its  similarity to the other two rankings, achieves a higher probability (0.708)\footnote{Since we measure $P_{LR}$ as a similarity between rankings, it would not sum to one as the in case of probability for classification.}.

The similarity of rankings based on the value $i$ of attribute $a$, ($v_{a,i}$),  or conditional probability of label rankings, is:
\begin{equation}
P_{LR}(v_{a,i}|y)= \frac{\sum_{i: x_{i,a} = v_{a,i}}\rho(y, y_i)}{|\{i: x_{i,a} = v_{a,i}\}|}
\label{ch2-eq:cond}
\end{equation}

Panel B of \ref{ch2-tab02} demonstrates the logic behind the conditional probabilities based on similarity. Notice that there are no examples with $Outlook=Sunny$ and a target ranking of $\{A,C,B\}$; thus, $P(Outlook=Sunny|\{A,C,B\})=0.000$. However, in the similarity approach, the probability of $\{A,C,B\}$ depends on the probability of similar rankings, yielding $P_{LR}(Outlook=Sunny|\{A,C,B\})=0.412$.


Applying \ref{ch2-eq03}, we get the estimated posterior probability of ranking $y$:
\begin{align}
P_{LR}(y|x_i)&=P_{LR}(y)\prod_{a=1}^m P_{LR}(x_{i,a}|y)=\\ \notag
& =\frac{\sum_{j=1}^{n} \rho(y,y_j)}{n}\left [ \prod_{a=1}^{m} \frac{\sum_{j: x_{j,a} = x_{i,a}}\rho(y, y_j)}{|\{j: x_{j,a} = x_{i,a}\}|}\right ]
\end{align}

The similarity-based adaptation of naive Bayes for label ranking will output the ranking with the higher $P_{LR}(y|x_i)$ value:
\begin{align}
\hat{y}&=\argmax_{y \in y_{\mathcal{L}} }P_{LR}(y|x_i)= \\ \notag
&=\argmax_{y \in y_{\mathcal{L}} }P_{LR}(y)\prod_{a=1}^m P_{LR}(x_{i,a}|y)
\end{align}

\section{Naive Bayes for label ranking: special cases}
\label{ch2-sec:cases}
\subsection{Continuous case}
\label{ch2-sec:nbr.cont}
The naive Bayes algorithm for label ranking mentioned above requires nominal variables in order to calculate the probabilities. In this section we extend the adaptation for the continuous case.

We propose to modify conditional label ranking probability  by utilizing Gaussian distribution of the independent variables; thus, applying traditional normal distribution approach. The naive Bayes for classification with continuous variables was implemented  in~\cite{bouckaert2005}.  We apply the same logic for  conditional  probability of label rankings and \ref{ch2-eq:cond} for the discrete case transforms to the continuous one as:

\begin{equation}
\label{ch2-cont}
P_{LR}(x_{i}|y)=\frac{1}{\sqrt{2\pi}\sigma_y}e^\frac{(x_i-\mu_y)^2}{2\sigma_y^2}
\end{equation}
where $\mu_y$ and $\sigma_y^2$ weighted  mean and weighted variance for LR, defined as follows:

\begin{align}
\label{ch2-mu}
\mu_y &=\frac{\sum_{i=1}^n  \rho(y,y_i) x_i}{\sum_{i=1}^n \rho(y,y_i)};&
\sigma_y^2&=\frac{\sum_{i=1}^n \rho(y,y_i) (x_i-\mu_y)^2}{\sum_{i=1}^n \rho(y,y_i)}
\end{align}

\subsection{Time series of rankings}
\label{ch2-sec:time}
The time dependent label ranking (TDLR) problem  takes the intertemporal dependence between the rankings into account. That is, rankings that are similar to the most recent ones are more likely to appear. % than very different ones.
 To capture this, we propose the weighted TDLR prior probability:

\begin{equation}
P_{TDLR}(y_t) =\frac{\sum_{t=1}^{n}  w_t \rho(y,y_t)}{ \sum_{t=1}^{n} w_t  }
\label{ch2-eq:timing}
\end{equation}
where $w_t = \{w_1, \ldots, w_{n}\} \rightarrow \mathbf{w}$  is the vector of weights calculated from the exponential function $\mathbf{w}=b ^{\frac{1-\{n\}_{1}^t } {n}}$. Parameter $b \in  \{1 \ldots \infty\}$ sets the degree of the ``memory'' for the past rankings, i.e.,  the larger $b$, the more weight is given to the most recent rankings. %; that is, how fast past rankings should diminish their importance.

As for the conditional label ranking probability, the equation for the weighted mean (\ref{ch2-cont}) becomes:
\begin{equation}
\label{ch2-mu.w}
\mu(x_{t,m}|y_t) = \frac{\sum_{t=1}^n  w_t \rho(y,y_t) x_{t,m}}{\sum_{t=1}^n \rho(y,y_t)}
\end{equation}
and variance:
\begin{equation}
\label{ch2-sigma}
\sigma_{w}^2(x_{t,m}|y)=\frac{\sum_{i=1}^n w_{t} \rho(y,y_t) [x_{t,m}-\mu(x_{t,m}|y)]^2}{\sum_{i=1}^n \rho(y,y_t)}
\end{equation}


\section{Data}
\label{ch2-sec:data}

Given the novelty of the label ranking problem, it is hard to find the available data for LR experiments. We follow the convention to use the KEBI data set\footnote{\url{https://www.uni-marburg.de/fb12/kebi/research/repository/}}. The description of individual set presented in \ref{ch2-data.descr}. Notice that there are two types of data. The explanation of each of them is given in~\cite{cheng2009}:
\begin{quotation}
[Type (A) data sets:] a naive Bayes classifier is first trained on the complete data set. Then, for each example, all the labels present in the data set are ordered with respect to the predicted class probabilities (in the case of ties, labels with lower index are ranked first) \ldots [Type (B):] for regression data, a certain number of (numerical) attributes is removed from the set of predictors, and each one is considered as a label. To obtain a ranking, the attributes are standardized and then ordered by size.
\end{quotation}

\begin{table}
\caption{Description of the label ranking dataset}
\label{ch2-data.descr}
\ The table depicts the data used in the label ranking experiments. Type A datasets is based on the naive Bayes classifier. Type B is from the regression data.
\begin{center}
\begin{tabular}{rcccc}
\toprule
Datasets & type & Instances & Features & Labels \\ 
  \midrule 
authorship & A & 841 &  70 &   4 \\ 
  glass & A & 214 &   9 &   6 \\ 
  iris & A & 150 &   4 &   3 \\ 
  segment & A & 2310 &  18 &   7 \\ 
  vehicle & A & 846 &  18 &   4 \\ 
  vowel & A & 528 &  10 &  11 \\ 
  wine & A & 178 &  13 &   3 \\ 
  bodyfat & B & 252 &   7 &   7 \\ 
  cpu-small & B & 8192 &   6 &   5 \\ 
  housing & B & 506 &   6 &   6 \\ 
  stock & B & 950 &   5 &   5 \\ 
  wisconsin & B & 194 &  16 &  16 \\ 
  cold & - & 2465 &  24 &   4 \\ 
  diau & - & 2465 &  24 &   7 \\ 
  dtt & - & 2465 &  24 &   4 \\ 
  heat & - & 2465 &  24 &   6 \\ 
  spo & - & 2465 &  24 &  11 \\ 
  
\bottomrule
\end{tabular}
\end{center}
\end{table}
%
% The algorithm proposed in the previous section was tested on some metalearning problems. Algorithm recommendation using  a metalearning approach has often been address as a label ranking problem~\cite{brazdil2003,todorovski2002}. Here, we provide a summary of a problem.
%
% Many different learning algorithms are available to data analysts nowadays. For instance, decision trees, neural networks, linear discriminants, support vector machines among others can be used in classification problems. The goal of data analysts is to use the one that will obtain the best performance on the problem at hand. Given that the performance of learning algorithms varies for different datasets, data analysts must select carefully which algorithm to use for each problem, in order to obtain satisfactory results.
%
% Therefore, we can say that a performance measure establishes a ranking of learning algorithms for each problem. For instance, Table~\ref{ch2-tbl:preferences} illustrates the ranking of four classification algorithms ($a_i$) on two datasets ($d_j$) defined by estimates of the classification accuracy of those algorithms on those datasets.
%
% \begin{table}
% \begin{center}
% \caption{Accuracy of four learning algorithms on two classification problems.}
% \begin{tabular}{ccccc}
% \hline
%  & $a_{1}$ & $a_{2}$ & $a_{3}$ & $a_{4}$ \\
% \hline
% $d_1$ & 90\% (1)& 61\% (3)& 82\% (2)& 55\% (4)\\
% $d_2$ & 84\% (2) & 86\% (1)& 60\%(4) & 79\% (3)\\
% \hline
% \end{tabular}
% \label{ch2-tbl:preferences}
% \end{center}
% \end{table}
%
% Selecting the algorithm by trying out all alternatives is generally not a viable option. As explained in~\cite{todorovski2002}:
% \begin{quote}
% In many cases, running an algorithm on a given task can be time consuming, especially when complex tasks are involved. It is therefore desirable to be able to predict the performance of a given algorithm on a given task from description and without actually running the algorithm.
% \end{quote}
% The learning approach to the problem of algorithm recommendation consists of using a learning algorithm to model the relation between the characteristics of learning problems (e.g., application domain, number of examples, proportion of symbolic attributes) and the relative performance (or ranking) of a set of algorithms~\cite{brazdil2003}.
% We refer to this approach as \textit{metalearning} because we are learning about the performance of learning algorithms.
%
% Metalearning approaches commonly cast the algorithm recommendation problem as a classification task.
% Therefore, the recommendation provided to the user consists of a single algorithm.
% In this approach, the examples are datasets and the classes are algorithms.
% However, this is not the most suitable form of recommendation.
% Although the computational cost of executing all the algorithms is very high, it is often the case that it is possible to run a few of the available algorithms.
% Therefore, it makes more sense to provide recommendation in the form of a ranking, i.e. address the problem using a label ranking approach, where the labels are the algorithms. The user can then execute the algorithms in the suggested order, until no computational resources (or time) are available.
%
% In the metalearning datasets, each example $(x_i, y_i)$ represents a machine learning  problem, referred to here as base-level dataset (BLD). The $x_i$ is the set of metafeatures that represent characteristics of the BLD (e.g., mutual information between symbolic attributes and the target) and the $y_i$ is the target ranking, representing the relative performance of a set of learning algorithms on the corresponding BLD. More details can be found in~\cite{brazdil2003}.

\section{Experiment Results}
\label{ch2-sec:results}
We empirically test the proposed adaptation of the naive Bayes algorithm for learning label rankings.

% \subsection{Experimental Setup}
%
% We used the following metalearning datasets in our experiments:
% \begin{itemize}
% \item \texttt{class}: these data represent the performance of ten algorithms on a set of $57$ classification BLD. The BLD are characterized by a set of metafeatures which obtained good results with the k-NN algorithm~\cite{brazdil2003}.
% \item \texttt{regr}: these data represent the performance of nine algorithms on a set of $42$ regression BLD. The set of metafeatures used here has also obtained good results previously~\cite{soares04}.
% \item  \texttt{svm-*}: we have tried four different   datasets describing the performance of different variants of the Support Vector Machines algorithm on the same $42$ regression BLD as in the previous set and also using the same set of metafeatures~\cite{soares+04}. The difference between the first three sets, \texttt{svm-5}, \texttt{svm-eps01} and \texttt{svm-21} is in the number of different values of the kernel parameter that were considered. The remaining dataset \texttt{svm-eps01} uses the same 11 alternative kernel parameters as \texttt{svm-11} but the value of the kernel parameter $\epsilon$ is 0.128 and not 0.001 as in the other sets.
% \end{itemize}
%
Given that the attributes in the datasets are numerical and the NB algorithm is for symbolic attributes, they must be discretized. We used a simple equal-width binning method using 10 bins. We also perform the experiments on continuous data applying the modified naive Bayes for continuous case outlined in \ref{ch2-sec:nbr.cont}. In addition, we compare the results with the state-of-the-art LR algorithm developed in~\cite{brazdil2003}; namely, k-NN (k=3) for label ranking.

The baseline is  a  simple method based on the mean rank of each label over all training examples~\citep{brazdil2009}.

\begin{equation}
\label{ch2-default.rank}
\hat{y}^{-1}_{j} = \frac{\sum_{i=1}^n y^{-1}_{i,j}}{n}
\end{equation}
where $y^{-1}_{i,j}$ is the rank of label $\lambda_j$ on dataset $i$. The final ranking is obtained by ordering the mean ranks and assigning them to the labels accordingly. This ranking is usually called the \emph{default ranking}, in parallel to the default class in classification.



The performance of the label ranking methods was estimated using a methodology that has been used previously for this purpose~\citep{brazdil2003}. It is based on 10-fold cross validation. The accuracy of the rankings predicted by methods was evaluated by comparing them to the target rankings (i.e., the rankings based on the observed performance of the algorithms) using the Spearman's correlation coefficient (\ref{ch2-eq00}). The code for all the examples in this paper has been written in R~\citep{rdl08}.



%\subsection{Results}

The results of the experiments are presented in \ref{ch2-results}. We report that the naive Bayes for label ranking for continuous case exhibits the maximum number of datasets for which it out-performed the baseline and is competitive with the state-of-the-art. The bold numbers in the table represent the value of the average Spearman correlation across 10-folds that are higher than that of the baseline.

For the continuous case, the naive Bayes for label ranking algorithm outperformed the baselines in 13 datasets out of 17. For the discretized case the number of outperformed datasets is 12. The state-of-the-art algorithm outperformed the baseline in continuous and discrete scenarios in 14 and 12 respectively. Observe, that given the different nature of datasets (types A or B) and different variable classes (continuous vs. nominal) both label ranking algorithms exhibit relatively great performance in predicting the rankings.


\begin{table}
\caption{Results of Label Ranking experiments on KEBI datasets}
\label{ch2-results}
\ The table depicts the results of label ranking experiments applied on KEBI dataset sorted by type of the dataset. We use 10-fold cross validation. Bold fonts means the algorithm outperformed the baseline.
\begin{center}
%\resizebox{\textwidth}{!}{%
\begin{tabular}{rcccccccc}
\toprule
Datasets & type & baseline & nbr.cont & nbr.disc & knn.cont & knn.disc \\ 
  \midrule 
authorship & A & 0.643 & 0.365 & \textbf{0.665} & \textbf{0.955} & \textbf{0.936} \\ 
  glass & A & 0.698 & 0.695 & \textbf{0.764} & \textbf{0.901} & \textbf{0.846} \\ 
  iris & A & 0.150 & \textbf{0.817} & \textbf{0.82} & \textbf{0.973} & \textbf{0.896} \\ 
  segment & A & 0.470 & \textbf{0.756} & \textbf{0.742} & \textbf{0.978} & \textbf{0.952} \\ 
  vehicle & A & 0.216 & \textbf{0.611} & \textbf{0.656} & \textbf{0.889} & \textbf{0.845} \\ 
  vowel & A & 0.250 & \textbf{0.747} & \textbf{0.405} & \textbf{0.947} & \textbf{0.875} \\ 
  wine & A & 0.346 & \textbf{0.781} & \textbf{0.483} & \textbf{0.942} & \textbf{0.892} \\ 
  bodyfat & B & -0.074 & \textbf{0.178} & \textbf{0.077} & \textbf{0.196} & \textbf{0.175} \\ 
  cpu-small & B & 0.259 & \textbf{0.343} & \textbf{0.315} & \textbf{0.504} & 0.126 \\ 
  housing & B & 0.069 & \textbf{0.604} & \textbf{0.557} & \textbf{0.819} & \textbf{0.628} \\ 
  stock & B & 0.075 & \textbf{0.666} & \textbf{0.414} & \textbf{0.962} & \textbf{0.823} \\ 
  wisconsin & B & -0.026 & \textbf{0.555} & \textbf{0.184} & \textbf{0.601} & \textbf{0.454} \\ 
  cold & - & 0.050 & \textbf{0.091} & 0.02 & \textbf{0.087} & \textbf{0.052} \\ 
  diau & - & 0.259 & 0.157 & 0.251 & 0.19 & 0.179 \\ 
  dtt & - & 0.124 & \textbf{0.143} & 0.105 & 0.09 & 0.087 \\ 
  heat & - & 0.035 & \textbf{0.054} & 0.029 & \textbf{0.056} & 0.033 \\ 
  spo & - & 0.204 & 0.113 & 0.182 & 0.11 & 0.099 \\ 
  
\bottomrule
\end{tabular}%}
\end{center}
\end{table}

%The naive Bayes algorithm is competitive to the state-of-the-art.

%As we mention, the label ranking problem is new for the Machine Learning literature. As a result, it is hard to get the real world data. We compare the results of our experiments to the KEBI datasets depending to the type of datasets (A,B, or no type). We observe that k-NN algorithm out-performs the baseline only for type B datasets. NB4LR with discritization of the variables partly out-performs in types A and B; and only NB4LR with continuous data is able to beat the baseline in all of the type B datasets and partly in both: type A and no type.

%\section{Conclusion}
%\label{ch2-sec:conclusion}


%In this paper we presented an adaptation of the naive Bayes algorithm for label ranking that is based on similarities of the rankings taking advantage of a parallel that can be established between the concepts of likelihood and distance. We tested the new algorithm on label ranking datasets and conclude that it consistently outperforms a baseline method and is competitive with the state-of-the-art.

%A number of issues remain open, which we plan to address in the future. Firstly, we are currently working on creating new datasets for ranking applications in different areas, including finance (e.g., predicting the rankings of the financial analysts based on their recommendations). These new datasets will enable us to better understand the behavior of the proposed algorithm. In addition, we assume that target rankings are total orders. In practice, this is often not true~\citep{cheng2010,brazdil2003}. We plan to address the problem of partial orders in the future.  Finally, we plan to compare the new method with existing ones.


%\begin{acknowledgements}
%This work was partially supported by FCT project Rank! (PTDC/EIA/81178/2006). We thank the anonymous referees for useful comments.
%\end{acknowledgements}
\begin{landscape}
%\thispagestyle{empty}
\begin{longtable}{p{6cm}p{4cm}p{6cm}p{4cm}}
\caption[Summary of Label Ranking models]{Summary of models} \\
\label{ch2:lr-summary}\\
\toprule
Category & Label ranking methods &Description& References \\
\midrule
%\endheadfirsthead
%\caption[]{(continued)}\\
\endfirsthead

\caption[]{(continued)} \\
\toprule
Category & Label ranking methods &Description& References \\
\midrule
\endhead

\multicolumn{4}{r}{\textit{\footnotesize{... continued next page}}}\\
\endfoot
\endlastfoot
\emph{Decomposition}: The LR problem is decomposed into small, simpler sub-problems (binary classification problems) that, on average, achieve the great performance in experiments but requires an ensemble of binary models &Constraint classification (CC) & Turns the LR problem into single binary classification problem in an extended space and learns LR model from the classifier & \cite{har2002} \\
 & Log-linear model (LL)& Learns the utility function for each individual label & \cite{dekel2003} \\
& Pairwise comparison (RPC)& Directly models individual preferences (without estimating utility function). An extension of pairwise classification  & \cite{hullermeier} \\
\midrule
\emph{Probabilistic}: leverages statistical probability models to develop LR methods. Good: provides the measure of reliability of prediction. Bad: requires storing the all training data in memory& Instance-base (Mallows).& Distance-based probability model that defines the probability of ranking according to its distance to a center ranking.&\cite{cheng2008} \\
&Decision trees &  Similar to conventional decision tree learning. The difference is that  the split criterion is at inner nodes and different criterion for stopping the recursing partitioning. & \cite{cheng2009} \\
&Instance-base (Plackett-Luce) & The probability is based on the scores of unassigned labels.& \cite{cheng2010} \\
&  Generalize linear models &  &\cite{cheng2010} \\
&  Gaussian mixture model & The model consist of mixtures defined by prototypes which are associated with preference judgment for each pair of labels.  &\cite{grbovic2012} \\
\midrule
\emph{Similarity}: replaces probability with similarity between the rankings. Minimizing the distance is equivalent to maximizing the likelihood (maximizing the similarity). Good: assigns non-zero probabilities that are not observed in data. Bad: shows moderate predicting accuracy&Naive Bayes &Adaptation of naive Bayes for classification. Adapts the prior and conditional probabilities in the realm of LR &\cite{aiguzhinov2010} \\
& Association rules & Adaptation of APRIORI. The goal is to discover frequent pairs of attributes associated with a ranking&\cite{desa2011}\\
& Multilayer perception & Adaptation of MLP. Adapts the error functions that guide the back-propagation leaning process and the method to generate a ranking from the output layer.  &\cite{ribeiro2012} \\
&Rank distance & The LR model learns rankings from the nearest neighbor &\cite{brazdil2003}\\
&Rule-based & The learning approach is based on reduction technique&\cite{gurrieri2012} \\
\bottomrule
%\end{tabular}
%\end{sidewaystable}
\end{longtable}
\end{landscape}
\processdelayedfloats
\cleardoublepage

\chapter{}
\label{ch3}

%\begin{abstract}
%\input{./abstract/ch3-abstract}
%\end{abstract}

%\textit{keywords}: financial analysts; rankings; state variables \\
%\textit{JEL}: G11








%\section{Introduction}
%\label{ch3-sec:introduction}

%The Efficient Market Hypothesis (EHM)~\citep{fama1970ecm} suggests that all public information available to investors is incorporated in prices and new information is immediately reflected in valuations. Yet there are information gathering costs and financial analysts are better than an average investor at processing this information which reflects in issued buy/ sell recommendations. These recommendations, like other news about the general economy as well as about a particular company, influence investors' perception and beliefs.

%Previous studies show that analysts stock recommendations have investment value ~\citep{womack1996,barber2001}. The literature also suggests further that foreknowledge of analyst forecast accuracy is valuable~\citep{brown2003,aiguzhinov2015a}. In line with academic research findings, practitioners too pay attention to analyst forecast accuracy rankings. On an annual basis, firms such as The Institutional Investor and StarMine\footnote{http://www.starmine.com} publish analysts ratings according to how well they performed, based partly on past earnings forecast accuracy.

%The importance of these ratings should not be ignored because the attention that the market gives to the recommendations of different analysts is expected to correlate with them. Typically, the performance of analysts is analyzed in terms of their individual characteristics (e.g., experience, background)~\citep{clement1999}. The disadvantage of this approach is that the collection of the necessary data is difficult and it is not always reliable. As for practitioners, they rely mostly on past accuracy to predict future accuracy.

%In this paper we follow an alternative approach.  We model the general behavior of rankings of analysts by using variables that characterize the context (state variables) rather than individual analyst characteristics. The model we propose uses the state variables to distinguish which of them  affects the rankings the most; hence, influence the analysts' forecast accuracy. In summary, our goal is not to understand  relative performance of the analysts  in terms of their characteristics but rather in terms of the characteristics of the context in which the analysts operate.


%To achieve this goal, we, first, build rankings of analyst based on their EPS forecasts accuracy. Then, we select the state variables that  are responsible in differences of analysts' rankings. Finally, we apply a Machine Learning label ranking algorithm to build a model that relates the rankings with the variables and calculates  a discriminative power of a variable.

%The paper is organized as follows: \ref{ch3-sec:ranking} provides the motivation for using rankings of the analysts; \ref{ch3-sec:ind.var} outlines the state variables that characterize the context; \ref{ch3-sec:labelranking} outlines the structure of the Machine Learning label ranking model and presents a methodology of building a ``variable-ranking'' relation; \ref{ch3-sec:data} describes the datasets used for the experiments; \ref{ch3-sec:exp_setup} summarizes the experiment setup;  \ref{ch3-sec:results} presents and discusses the results; finally, \ref{ch3-sec:conclusion} concludes this paper.


\section{Rankings as a measure of accuracy}
\label{ch3-sec:ranking}
In spite of the Efficient Market Hypothesis, it is commonly accepted that the recommendations of financial analysts yield an economic value to investors~\citep{womack1996}; moreover, recommendations of superior analysts have impact on the market~\citep{loh2011}. For this reason, researchers and practitioners have long been interested in understanding how financial analysts affect capital market efficiency~\citep{ramnath2008faf}.

Most researchers conclude that financial analysts are better at making EPS forecasts than mathematical models. Specifically,~\cite{fried1982,bouwman1987,brown1991} show that analysts are better at forecasting EPS values than any time series models (e.g., ARIMA). The analysts' superiority contributed to the fact that they utilize all available information at and after the date of time series model forecasts. Thus, the context in which analysts make decision matters for their accurate forecast.


In terms of the following advice of the most accurate analysts, it has been shown that the relative accuracy among financial analysts is more important than their absolute accuracy~\citep{aiguzhinov2015a}, e.g., in the context of analysts turnover rate~\citep{michaely1999}, or in creating value to investors~\citep{aiguzhinov2015a}. In addition, financial analysts with superior past accuracy have a greater impact on the market~\citep{park2000analyst}. It has also been shown that, under some assumptions, it is safe to assume that analysts with higher forecasting ability produce profitable stock recommendations~\citep{loh2006aef}. This fact is attributed to their deeper research and fundamental accounting knowledge. Furthermore, literature agrees that there is consistency in the superiority of these analysts over time~\citep{li2005persistence,hilary2013}.


Many studies try to correlate the EPS forecasts accuracy of financial analysts with their intrinsic characteristics. However, existing academic research on the behavior of financial analysts have important limitations~\citep{clement1999,brown2003,ramnath2008faf}, namely an incomplete characterization of the analysts and their recommendations.  For instance,~\cite{ramnath2008faf} address the question of what information affects the recommendations of analysts or how informative are their short-term earnings forecasts, using linear regression on a small sample of data. Despite the promising results, further work is necessary to improve both the methods and the characterization of the context of recommendations.

In this paper, we propose a novel approach in identifying variables that affect the rankings; hence, the relative accuracy of the analysts. The novelty lies in the new  methodology of modeling the relationship between the analysts' rankings and the state variables. To build the model, we are required to select the state variables and build analysts rankings.



% The variables that are responsible for the process of stock valuation by the analysts; and, hence, may influence the EPS forecast can be devided into three types:
% \begin{itemize}
% \item Analysts' specific factors (such as experience, skills, etc.)~\citep{clement1999,jacob1999,brown2003}
% \item Stock-issuing company specific factors (accounting fundamental variables)~\citep{mear1987,mcewen1999,lev1993}
% \item Macroeconomic factors~\citep{lev1993}
% \end{itemize}
%
% Factors that affect the accuracy in EPS forecasts which are based on the analyst's specific skills are irrelevant for our study. We focus our research on rankings of the analysts that operate in state of the world that is equally observed by all analysts; thus, for now, the individual characteristics are ignored in our study.
%
% What we really focus is what variables from observing the stock-issuing company financial statement affect the decision of the FAs in issuing the reports. The difficulty of observing the decision making process of FAs is responsible for the scare sources in the accounting literature.  The study of~\cite{mear1987} address the problem of importance of information for the FAs in risk and stock return judgements. The authors conducted an experiment study on 38 financial analysts with average investment experience of 7.4 years and average age of 31.4 years. The authors selected nine variables or cues that they believe affect the analysts decisions in judging the risk and return of a company. The authors provide a reasoning for such a selection:
% \begin{quotation}
% Considerable care and effort was taken in selecting this stimulus set. Interviews with financial analysts, surveys of stockbroker investment publications, and searches of the business and academic literature were made to create more realism in the experimental design. The final selection of stimuli was facilitated by an orthogonal factor analysis and pretested in a sample study
% \end{quotation}
%
%
% Table \ref{weights} presents the results of the study. It shows the average judgement weight of all analysts  for each of the variable. We can observe that the distribution of the variables is relatively uniformed suggesting to consider most of the variable in applying for our research.
%
% \begin{table}
% \caption{Subjective weights of the financial analysts for risk and returns judgements (reproduces from~\cite{mear1987} ) }
% \label{weights}
% \begin{center}
% \begin{tabular}{lrr}
% Variable&Risk judgement&Return judgement \\
% \hline
% Net Assets &   6.26&3.18 \\
% Proprietorship Ratio  &13.42& 7.79\\
% Liquidity	& 11.82 & 6.38\\
% Sales Growth	& 8.73&14.03\\
% Dividend Cover& 	9.23&9.78\\
% Industry	& 8.59&9.34\\
% Profitability & 	12.22&19.67 \\
% Valuation Ratio	& 7.21 &8.35\\
% Beta	& 13.46&13.99\\
% Variance of Returns	& 9.05&7.48\\
% \hline
% TOTAL	& 100.00&100.00
% \end{tabular}
% \end{center}
% \end{table}
%
% A different experiment but with the same idea was conducted by ~\cite{mcewen1999}.~\cite{mcewen1999} demonstrates that those analysts that look at the accounting information provide the more accurate EPS forecasts. In general, the authors state that:
% \begin{quotation}
% More accurate [analysts] emphasize income indicators, and they do so over longer time-horizons, while the less accurate subject emphasize other annual report components, especially the Footnotes. More accurate analysts also tend to use summary indicators, such as ratios, to a greater extent than do less accurate analysts.
% \end{quotation}
% The authors perform an experiment that utilizes a unique methodology to identify what information analysts use in their research. This methodology called Integrated Retinal Imaging System (IRIS). An anonymous brokerage firm in NY uses this system for physically disabled financial analysts to utilize eye movements instead of mouse or keyboard to select item on a computer. The study consisted of 60 sell-side analysts with mean age of 35.7, average years as a financial analysts was 8.9, and average year of employment with the firm was 7.3.
%
% The authors divided the group of analysts into two subsets: more  and less accurate. The ranks of the analysts calculated as an absolute analysts' EPS forecast error divided by actual EPS.  The goal of the experiment was to identify what  items from the annual report and 10-K would analysts from both subset use in their EPS forecasts.
%
%
%
% Reproduced from the paper, table \ref{hunton} shows the results of the study. It reports which of the two groups of the financial analysts (more accurate or less accurate) put the more emphasis in their research. For example, the less accurate analysts spent more time in analysing the Annual report whereas the more accurate analysts looked at Key ratios. Overall, the authors report that more accurate financial analysts use the following information for their EPS forecasts: key ratios,  five-year earnings summary, and older income information, whereas less accurate pay a lot of emphasis on footnotes.
%
% \begin{table}
% \caption{Significant differences in emphasis scores: more vs. less accurate (reproduces from~\cite{mcewen1999})}
% \label{hunton}
% \begin{center}
% \begin{tabular}{lc}
% Information item&Group that put a greater emphasis\\
% \hline
% 1995 Annual Report: & Less accurate\\
% Statement of Shareholders' Equity & Less \\
% Balance SheetLiabilities & Less \\
% Balance SheetAssets  & Less \\
% Management's Discussion\& Analysis  & Less \\
% Audit Report'  & Less \\
% Management's Letter to Shareholders  & Less \\
% Statement of Cash Flows  & Neither group\\
% Quarterly Summary  & Neither \\
% Income Statement & More accurate\\
% Footnotes: & Less \\
% Significant Accounting Policies  & Less \\
% Related Party Transactions & Less \\
% Pension Plan  & Less \\
% Leases & Less \\
% Commitments and Contingencies  & Less \\
% Accrued Expenses  & Less \\
% Income Taxes & Less \\
% Long-Term Debt & Less \\
% Shareholder's Equity & Less \\
% Merchandise Inventories & Less \\
% \hline
% 1995 Economic and Industry Information:  & Neither \\
% Industry Information & Neither \\
% Economic Information  & Neither \\
% \hline
% 1995 Company Information: & More \\
% Key Ratios & More \\
% Five-Year Earnings Summary & More \\
% Share Price Information& More \\
% Company Identification & Neither \\
% Officers and Directors & Neither \\
% 1994 Net Income& More \\
% 1993 Net Income& More \\
% 1992 1991 Net Income& More \\
% \hline
% \end{tabular}
% \end{center}
% \end{table}
%
%
% As far as our study is concerned, we can select most of the variables that had a greater emphasis for both groups. The difficulty would be in quantifying some of the variables, for example, the authors do not disclose what Key ratios were used in the study. The authors emphasise that the more accurate group of FAs looked at the income statement. This contradicts with~\cite{bouwman1987} which states that the income statement serves more to familiarize an analyst with the company and it is Segment Data of the annual report that goes into a reasoning part of the analysts decision.
%
%~\cite{lev1993} go further in identifying the set of fundamental variables that are used by analysts in the valuation of stocks. Using a guided search procedure where candidate fundamentals would be selected form the written reports of the analysts, the authors select  12 signals presented in table \ref{lev} (a signal is a combination of certain fundamental variables found in balance sheet or income statement).
%
% \begin{table}
% \caption{Twelve signals (reproduces from~\cite{lev1993})}
% \label{lev}
% \begin{center}
% \begin{tabular}{l}
% Signal\\
% \hline
% Inventory\\
% Accounts receivables\\
% Capital expenditures\\
% R \& D expenses\\
% Gross margin\\
% Sales and Admin. Expenses\\
% Provision and Doubtful Receivables\\
% Effective tax\\
% Order backlog\\
% Labor Force\\
% LIFO earnings\\
% Audit qualification\\
% \hline
% \end{tabular}
% \end{center}
% \end{table}
%
% The authors run a return-earnings regression to test these signals on year-by-year basis from 1974-1988. On the left-hand side of the regression is the annual return of a stock and the right-hand side has these 12 signals plus the annual percentage change in earnings. The sample size of the firms in the study varies from 140 to 180 per year. The  authors report the significance of each of the signal in a given year so each year there would be significant as well as insignificant signals. In addition, the authors condition the analysis on some macroeconomic variables such as inflation and real Gross National Product (GNP). They discover that some of the signal are sensitive to these conditions. For example, Accounts Receivables and Doubtful Receivables exhibit higher statistical significant during period of high inflation.
%
% Based on this literature analysis, we can select a number of the fundamental accounting variables and joint them with already defined set of variables described in~\cite{jegadeesh2004}.
%
%
% \begin{table}
% \caption{Summary of variables}
% \label{variables}
% \begin{center}
% \begin{tabular}{p{3cm} p{4cm} p{4cm} p{2cm} }
% Variable&Motivation&Measure&Citation\\
% \hline
% Earnings Variability (EVAR)&Usefulness of past earnings tend to decline with increase of EVAR&\textit{Value Line Profitability Index} or ROE/ROA&~\cite{luttman1995} \\
% \hline
% Market risk (BETA)&Security prices reflect earnings uncertainty&\textit{Value Line} beta&~\cite{luttman1995}\\
% \hline
% \end{tabular}
% \end{center}
% \end{table}
%
%
%




\section{Ranking characterization variables}
\label{ch3-sec:ind.var}
Several studies try to analyze  factors that affect the performance of analysts~\citep{clement1999,brown2003,jegadeesh2004}.  However, most of these papers look at the individual characteristics of analysts such as their job experience, affiliation,  education background, industry specializations. These variables are very important to characterize the relative performance of the analysts in general but they miss the ``state-of-the-world'' component, i.e., variables that affect all analysts at once. We believe that rankings of analysts capture this component in full.

A ranking is a result of  differences in opinion among the analysts concerning the future performance of a company.  This implies that there is  a variability (dispersion) in analysts' forecasts for a given stock in a given quarter~\citep{diether2002}. Thus, we can analyze  the analysts' forecasts dispersion in terms of its origin and factors that affect it; hence, assuming the same variables affect the rankings. It follows that the variation in rankings is due to the different ability of the analysts to interpret the informational environment (e.g., whether the market is bull or bear). We, thus, select and analyze variables that describe this environment.

To capture the full spectrum of the analyst's decision making process, we select  variables based on different levels of information availability: analyst-specific,  firm-specific  and general economy. In each level, we want a variable to be responsible for information asymmetry and uncertainty. Thus, we believe that these two domains are responsible for the differences in analysts' opinions.

\subsection{Analyst-specific variables}
On an analyst level, we want to capture the asymmetry of information  and uncertainty about the future among the analysts. Particularly,~\cite{barron2009} point our that the reason for analysts' dispersion is either the uncertainty or the information asymmetry: prior to earnings announcement the uncertainty component prevail, whereas around the time of earnings announcement, information asymmetry is responsible for changes in analysts' opinions.

We use the same set of variables defined in~\cite[page 333]{barron2009}:

\begin{eqnarray}
SE_{s}&=&(ACT_{s}-\overline{FE_s})^2 \nonumber\\
disp_s&=&\sum_{a=1}^{N} \frac{(FE_{a,s}-\overline{FE_s})^2}{(N-1)} \label{ch3-eq:disp}\\
uncert_s&=&\sum_{a=1}^{N} \left(1-\frac{1}{N}\right) \times disp_s + SE_s \label{ch3-eq:uncert}\\
assym_s& = & 1-\frac{SE_s-\frac{disp_s}{N}}{uncert_s} \label{ch3-eq:assym}
\end{eqnarray}
where $SE$ is the squared error in mean forecast; $\overline{FE}$ is the average per analyst $a$ EPS forecast error (see \ref{ch3:eps-rank});  and $N$ is the number of analysts in a given quarter for a given stock $s$.

\ref{ch3-eq:disp} calculates the dispersion among the analysts which is a variance of EPS forecasts of all analysts for a given stock. \ref{ch3-eq:uncert} defines the uncertainty component of the dispersion per~\cite{barron2009}. \ref{ch3-eq:assym} is the proxy for the information asymmetry.


\subsection{Firm-based variables}

To be consistent with the two paradigms that characterize the state of the analysts, we split the firm-based variables based on their influence on analysts' opinions. They are either the uncertainty or the information asymmetry.

\subsubsection{Uncertainty}

The following are the set of the variables and their definitions that we think are responsible for the information uncertainty component.

\paragraph{Business risk.} Business risk is associated with the uncertainty in operating results, especially, in operating earnings~\citep{hill1980}. An increase in business risk entails an increase in \emph{ex-ante} volatility of the reported earnings~\citep{parkash1995}.  We believe that  book-to-market ratio can serve as a proxy for the business risk measurement.
\begin{equation}
btm_s=\frac{EQUITY}{MKT.CAP}=\frac{Tot.assts-Tot.liab}{Stocks\times Price}
\end{equation}
where $Stocks$ is the number of stocks outstanding and $Price$ is the close stock price on last day of a quarter.

\paragraph{Financial risk.} Financial risk is responsible for the information uncertainty of the future earnings. More debt implies more variability in earnings as managers would try to maximize the value of a stock using the debt; thus, having high risk of default in the future or taking high risk investment projects. The debt-to-equity ratio is used to capture the financial risk~\citep{parkash1995}. We use short-term debt from the balance sheet (Notes payable) as a measure for debt.

\begin{equation}
dte_s=\frac{DEBT}{EQUITY}=\frac{ShortTermDebt}{Tot.assts-Tot.liab}
\end{equation}

\paragraph{Size.} The firm size can be used as a proxy for amount of information available for a firm. Thus, a larger firm has more news coverage which reduces the uncertainty. An investor is likely to find private information about a larger firm more valuable than the same information about a smaller firm~\citep{bhushan1989}.

Size is measured as the market value (MV) of the firm as following:
\begin{equation}
size_s= \log(Price \times Stocks)
\end{equation}
Consistent with the literature, we use log of market value.


\paragraph{Return variability.}
Return variability influences the uncertainty regarding future earnings~\citep{diether2002,henley2003}. An increase in variability of the abnormal returns is positively correlated with the uncertainty about the earnings; thus, affecting the dispersion among the analysts. To calculate the return variability, we use method outlined in~\cite{sousa2008}, where stock return volatility is decomposed into market and stock specific components as follow:
\begin{eqnarray}
\sigma^2_{mkt}&=&\sum_{d\in q} (R_{mkt,d}-\mu_{mkt})^2 \nonumber \\
\sigma^2_{s}&=&\sum_{d \in q} (R_{s,d}-R_{mkt,d})^2 \nonumber \\
s.ret_s=Var(R_{s,q})&=&\sigma^2_{mkt}+\sigma^2_{s} \label{ch3-eq:ret.vol}
\end{eqnarray}
where $R_{mkt,q}$ is the market return over sample period; $\mu_{mkt}$ is the mean of daily market returns; $R_{s,q}$ is an individual stock return; $d$ is the number of trading days in period $q$.

\subsubsection{Information asymmetry variables}
\paragraph{Accruals.}
Accruals, as a part of  earnings, is one of the variables that cause the information asymmetry between managers of a firm and investors. Studies have shown that presence of asymmetry is a necessary condition for the earnings management~\citep{trueman1988,richardson2000}. To be more specific, it is the discretionary part of the accruals that causes  the information inefficiency  in the earnings management~\citep{richardson2000,ahmed2005}. We calculated total accruals-to-total assets ratio defined in~\cite{creamer2009}:

\begin{eqnarray}
accr_s=\frac{\Delta C.As - \Delta Cash - (\Delta C.Lb. - \Delta C.Lb.D) - \Delta T - D\& A_q}{(T.As. - T.As._{q-4})/2}
\end{eqnarray}
where $\Delta X=X_q-X_{q-1}$; $C.As$ -- current assets; $C.Lb$ -- current liabilities; $C.Lb.D$ -- debt in current liabilities; $T$ -- deferred taxes; $D\&A$ -- depreciation and amortization; and $T.A$ -- total assets.



\paragraph{Sector-based variables.} The industry specific variables that cause the dispersion in the analysts' forecasts are also connected  with the uncertainty concept. One of the variables that is suggested  is the variability in the industry Producer Price Index (PPI)~\citep{henley2003}.
\begin{equation}
sec.ret = \sigma (\log PPI_{sec})
\end{equation}
where $\sigma (\log PPI_{sec})$ is the standard deviation of the log of SIC sectors' produce price index.


\subsection{Macroeconomics variables}
In the last set of the state variables, we  capture the macroeconomic conditions which affect the analysts' dispersion. For example, different states of the economy are based on  different levels of ``GNP vs. inflation'' combinations~\citep{lev1993,hope2005}. When economy is booming, i.e. ``high GNP-low inflation'' state,~\cite{lev1993} observe the significant increase in firms' Capital Expenditures coefficient. This implies that firms start enjoy capital investment due to the low cost of capital. This state of the economy produces less uncertainty. In the ``medium GNP-high inflation'' state of the economy, there is an increase in R\&D expenditures, which, from the above mentioned analysis, may spur high level of information asymmetry based on the increase R\&D activities. Finally, in the ``low GNP-high inflation'' state,~\cite{lev1993} observe the Doubtful Receivables coefficient is the largest implying that at this recession state many firms go bankrupt or default on the loans -- a signal of high uncertainty in the economy. All these states produce the dispersion of the analysts' forecasts.

We select the following set of the macroeconomic variables:
\begin{itemize}
\item $gnp$ = Gross National Product;
\item $infl$ = Inflation rate;
\item $t.bill$ = Interest rate (90-days T-bill rate);
\item $vix.ret$ = Market variability (CBOE VIX index)
\end{itemize}


\section{Ranking-based discriminative power}
\label{ch3-sec:labelranking}


We select the naive Bayes label ranking algorithm~\citep{aiguzhinov2010} as a tool to calculate the discriminative power. The basic idea lies in the similarity  among rankings conditional on a set of independent variables.

If we  define $\mathcal{S}$ as a similarity matrix between the rankings ($\mathcal{S}_{n \times n}=\rho(y_i,y_j)$), then the prior probability of a label ranking is given by:
\begin{equation}
P(y) = \frac{\sum_{i=1}^{n} \rho(y,y_i)}{n}
\end{equation}
where $\rho$ is the Spearman ranking correlation.


The conditional probability of the value $v$ of attribute $x$ ($x_{v}$) given the ranking $y$ is:
\begin{equation}
\label{ch3:eq-cond}
P(x_{v}|y)= \frac{\sum_{i: x_{i} = v}\rho(y, y_i)}{|\{i: x_{i} = v\}|}
\end{equation}

We propose that, the discriminative power of $x$ is based on the conditional ranking probabilities of values $x$ and they should: 1) be different from each other; 2) be different from the prior probability. Thus, given the prior ranking $P(y)$ and conditional probability $P(x_v|y)$, the discriminative value of $x$ can be found as follows:
\begin{equation}
\label{ch3:eq-dp}
\DP{}_{x}=\frac{1}{n}\sum_{t=1}^n \min_{\forall p \neq q} \left\{\lvert P(x_{v_p}|y_t) - P(x_{v_q}|y_t) )\rvert \right\} \times \left\{\lvert P(x_{v_p}|y_t)-P(y_t)\rvert\right\}
\end{equation}
The multiplicand of~\ref{ch3:eq-dp} finds the minimum absolute difference in conditional probabilities between different values of attribute $x$ given ranking $y$. The multiplier checks that the conditional label ranking probability of $x$ is different from the prior probability of ranking $y_t$. The case when $\DP{}=0$ means that $x$ does not discriminate; thus, we consider the cases when $\DP{}>0$.

We also measure the discriminative variable contribution as a variable's discriminative power share in total discriminative power of all variables:

\begin{equation}
\label{ch3:eq-dp-cont}
fracDP_{x} = \frac{ \DP{}_{x} }{\sum_{j=1}^J  \DP{}_{x=j} }
\end{equation}
where $J$ is the total number of independent variables.

Panel A of \ref{ch3-tab01} shows an example where we have  an artificial data for 5 quarters and rankings of 4 equity research firms ($A,B,C,D$). We assume that we identify some state variables $\{x_1, x_2\}$ that made the rankings as they are in a given quarter. Panel B (C) of the table shows the conditional ranking probabilities of $x_1$ ($x_2$).

\begin{table}
\caption{Example of Label Ranking problem}
\label{ch3-tab01}
\ The table presents an example of label ranking problem (Panel A). Panel B (panel C) shows the conditional label ranking probabilities for variable $x_1$ ($x_2$) obtained from \ref{ch3:eq-cond}

\begin{tabularx}{\textwidth}{l*{7}{Y}}
\toprule
\multicolumn{7}{l}{Panel A: Example of LR ranking problem} \\
\midrule
$t$&$x_1$&$x_2$&\multicolumn{4}{c}{Ranks}\\
\cmidrule{4-7}
&&&A&B&C&D\\
\midrule
 1 & a & b & 1 & 2 & 3 & 4 \\ 
  2 & b & a & 2 & 1 & 3 & 4 \\ 
  3 & c & a & 3 & 2 & 1 & 4 \\ 
  4 & d & a & 4 & 3 & 2 & 1 \\ 
  5 & e & a & 4 & 1 & 2 & 3 \\ 
  
\end{tabularx}

\begin{tabularx}{\textwidth}{l*{7}{Y}}
\midrule
\multicolumn{7}{l}{Panel B: conditional LR probability o f $x_1$} \\
 & a & b & c & d & e & priors \\ 
  \midrule 
1 & 0.34 & 0.31 & 0.21 & 0.00 & 0.14 & 0.58 \\ 
  2 & 0.26 & 0.29 & 0.21 & 0.03 & 0.21 & 0.68 \\ 
  3 & 0.17 & 0.20 & 0.29 & 0.11 & 0.23 & 0.70 \\ 
  4 & 0.00 & 0.05 & 0.19 & 0.48 & 0.29 & 0.42 \\ 
  5 & 0.11 & 0.20 & 0.23 & 0.17 & 0.29 & 0.70 \\ 
  
\end{tabularx}

\begin{tabularx}{\textwidth}{l*{7}{Y}}
\midrule
\multicolumn{7}{l}{Panel C: conditional LR probability of $x_2$} \\
 & b & a & a & a & a & priors \\ 
  \midrule 
1 & 0.31 & 0.34 & 0.34 & 0.34 & 0.34 & 0.58 \\ 
  2 & 0.29 & 0.26 & 0.26 & 0.26 & 0.26 & 0.68 \\ 
  3 & 0.20 & 0.17 & 0.17 & 0.17 & 0.17 & 0.70 \\ 
  4 & 0.05 & 0.00 & 0.00 & 0.00 & 0.00 & 0.42 \\ 
  5 & 0.20 & 0.11 & 0.11 & 0.11 & 0.11 & 0.70 \\ 
  
\bottomrule
\end{tabularx}
\end{table}

We apply \ref{ch3:eq-dp} to calculate a discriminative power of the variable. For an example from \ref{ch3-tab01}, these values are 0 and 0.007 for variables $x_1$ and $x_2$ respectively. Based on this example, we conclude that the most discriminative variable is $x_2$. The result is intuitive as the variable $x_2$ takes only two values $a$ and $b$ with value $a$ appearing 4 times; thus, it has more contributive effect on rankings.


\section{Data and preliminary results}
\label{ch3-sec:data}
We selected companies that are publicly traded in either NYSE, NASDAQ, or AMEX. The stocks accounting data was obtained from the Thomson One/Reuters Fundamental database. The analysts\footnote{We use words ``analyst''  even-though the database is for Equity Research Firms.} EPS forecasts data is from I/B/E/S for each company at study. The descriptive statistics of state variables is presented in \ref{ch3-tab:ind-vvs}.

\begin{table}
\caption{Descriptive statistics of independent variable}
\ The table presents the descriptive statistics of state variables that influence the ranking of the analysts.
\begin{center}
\begin{tabularx}{\linewidth}{r*{7}{Y}}
\toprule
Type&Variable & Stock & Median & Mean & std.dev&ACF (lag=1)\\
\midrule 
  \multirow{3}{*}{Analyst}&uncert &  988 & 0.001 & 0.629 & 10.881 & 0.983 \\ 
   &assym &  988 & 0.155 & 0.403 & 0.484 & 0.978 \\ 
   &disp &  988 & 0.000 & 0.006 & 0.125 & 0.971 \\ 
   \midrule 
 \multirow{5}{*}{Stock}&btm &  981 & 0.404 & 35.263 & 481.567 & 0.985 \\ 
   &size &  981 & 20.856 & 20.795 & 1.577 & 0.978 \\ 
   &dte &  608 & 0.004 & 0.131 & 0.651 & 0.984 \\ 
   &accr &  960 & -0.012 & 1.981 & 67.357 & 0.973 \\ 
   &s.ret &  988 & 0.029 & 0.054 & 0.081 & 0.975 \\ 
   &sec.ret &  988 & 0.021 & 0.008 & 0.119 & 0.977 \\ 
   \midrule 
 \multirow{4}{*}{Macro}&gnp &  988 & 0.014 & 0.013 & 0.005 & 0.975 \\ 
   &infl &  988 & 0.007 & 0.008 & 0.005 & 0.975 \\ 
   &vix.ret &  988 & -0.049 & 0.013 & 0.309 & 0.980 \\ 
   &t.bill &  988 & 0.049 & 0.048 & 0.023 & 0.979 \\ 
  
\bottomrule
\end{tabularx}
\end{center}
\label{ch3-tab:ind-vvs}
\end{table}



We apply a number of requirements for our analysts' data. We select stocks with minimum 12 quarters of coverage by at least one analyst. In addition, for the computational purpose, we require at least 3 analysts per stock in each quarter. We call this a \filtered{} set in contrast to \sample{} set that includes all analysts and stocks in the sample.

\ref{ch3-table:filtered.summary} outlines the number of stocks, analysts and total forecasts in \sample{} (Panel A) and \filtered{} datasets (Panel B). For \sample{} (\filtered{}) data we report 560~(202) unique analysts covering 3517~(1059) stocks during 84  quarters from 1989Q1 until 2009Q4. For this period there were 698291~(164445) issued forecasts.

\begin{table}
\caption{Summary of \sample{} and \filtered{} data}
\ The table presents the total number of stocks, analysts and EPS forecasts for \sample{} (Panel A) and \filtered{} (Panel B) data.
\begin{center}
\begin{tabularx}{\linewidth}{r*{4}{Y}}
\toprule
Sector & \# stocks & \# analysts & \# forecasts \\
\multicolumn{4}{l}{\textbf{Panel A: \sample{} data}}\\
\midrule
 Consumer Discretionary & 567 & 336 & 144 754 \\ 
  Consumer Staples & 155 & 215 & 28 593 \\ 
  Energy & 277 & 205 & 88 289 \\ 
  Financials & 650 & 228 & 93 656 \\ 
  Health Care & 526 & 309 & 76 680 \\ 
  IT & 697 & 413 & 159 012 \\ 
  Industrials & 464 & 320 & 80 331 \\ 
  Materials &  64 & 163 & 12 119 \\ 
  Telecom Services &  14 &  93 & 1 882 \\ 
  Utilities & 103 & 120 & 12 975 \\ 
   \midrule 
Total & 3 517 & 560 & 698 291 \\ 
  
\midrule
\end{tabularx}
\begin{tabularx}{\linewidth}{r*{4}{Y}}
\multicolumn{4}{l}{\textbf{Panel B: \filtered{} data}}\\
\midrule
 Consumer Discretionary & 214 & 124 & 34 199 \\ 
  Consumer Staples &  42 &  61 & 7 155 \\ 
  Energy &  99 &  63 & 24 819 \\ 
  Financials & 178 &  76 & 24 606 \\ 
  Health Care & 118 &  91 & 16 791 \\ 
  IT & 221 & 123 & 34 590 \\ 
  Industrials & 146 & 101 & 17 900 \\ 
  Materials &  25 &  46 & 3 183 \\ 
  Telecom Services &   4 &  14 & 328 \\ 
  Utilities &  12 &  18 & 874 \\ 
   \midrule 
Total & 1 059 & 202 & 164 445 \\ 
  
\bottomrule
\end{tabularx}
\label{ch3-table:filtered.summary}
\end{center}
\end{table}


\ref{ch3-table:forecasts-analyst} presents the descriptive statistics of \sample{} (Panel A) and \filtered{} (Panel B) data from the ``per analyst'' perspective. Concretely, for the \sample{} (\filtered{}) data the total number of  ``$Analyst \times Forecasts$'' observations is 11796~(7034). Each analyst, on average,  issued 59.2~(41.15) forecasts per quarter, and, if we factor in stocks, the average forecasts per stock per quarter becomes 1.35~(1.65). We also report a share of analysts that revise their EPS forecasts within a quarter. For \sample{} (\filtered{}) data 76.16\%~(79.09)\% of analysts revise their EPS forecasts. Finally, on average,  analysts follow stocks for 4.49~(13.71) quarters.

\begin{table}
\small\addtolength{\tabcolsep}{-2pt}
\caption{Descriptive statistics of forecasts per analyst}
\ The table presents the descriptive statistics  for \sample{} (Panel A) and \filtered{} (Panel B) data. Namely, the table shows the total number of analyst-forecast observations, the average number of forecast per quarter, the average number of following stocks per analyst, the average number of forecasts per stock per analyst, share of analysts that make forecast revisions, and, finally, the average number of quarters a analyst follows a stock.
\begin{center}
%\resizebox{\textwidth}{!}{%
%\begin{tabular}{rcccccc}
\begin{tabularx}{\linewidth}{r*{7}{Y}}
 \toprule
 &Obsrv & Frcst/q & Stocks & Frcst/stock&Rev.& follow time, q \\
 \multicolumn{7}{l}{\textbf{Panel A: \sample{} data}}\\
  \midrule
 Consumer Discretionary & 7 405 & 19.55 & 11.65 & 1.40 & 0.72 & 5.06 \\ 
  Consumer Staples & 3 797 & 7.53 & 4.81 & 1.36 & 0.60 & 5.29 \\ 
  Energy & 3 541 & 24.93 & 12.68 & 1.50 & 0.71 & 5.74 \\ 
  Financials & 4 506 & 20.78 & 13.00 & 1.38 & 0.66 & 5.52 \\ 
  Health Care & 5 812 & 13.19 & 8.87 & 1.29 & 0.63 & 4.14 \\ 
  IT & 8 018 & 19.83 & 12.89 & 1.34 & 0.69 & 4.67 \\ 
  Industrials & 6 290 & 12.77 & 8.47 & 1.32 & 0.65 & 5.04 \\ 
  Materials & 2 531 & 4.79 & 3.00 & 1.44 & 0.56 & 6.26 \\ 
  Telecom Services & 821 & 2.29 & 1.70 & 1.28 & 0.39 & 4.85 \\ 
  Utilities & 1 662 & 7.81 & 5.53 & 1.27 & 0.52 & 4.34 \\ 
   \midrule 
Total & 11 796 & 59.20 & 36.83 & 1.35 & 0.76 & 4.49 \\ 
  
\midrule
\end{tabularx}
%\begin{tabular}{rcccccc}
\begin{tabularx}{\linewidth}{r*{7}{Y}}
\multicolumn{7}{l}{\textbf{Panel B: \filtered{} data}}\\
%  \cline{2-5}
% & Forecasts & Brokers & Forecast/stock & follow time, q \\
 \midrule
 Consumer Discretionary & 3 966 & 15.81 & 8.62 & 1.70 & 0.76 & 13.71 \\ 
  Consumer Staples & 1 967 & 6.07 & 3.64 & 1.58 & 0.65 & 13.90 \\ 
  Energy & 2 212 & 23.78 & 11.22 & 1.96 & 0.80 & 17.84 \\ 
  Financials & 2 719 & 15.66 & 9.05 & 1.66 & 0.70 & 15.19 \\ 
  Health Care & 2 708 & 10.08 & 6.20 & 1.49 & 0.67 & 13.13 \\ 
  IT & 4 007 & 14.13 & 8.63 & 1.56 & 0.74 & 13.62 \\ 
  Industrials & 3 036 & 9.36 & 5.90 & 1.55 & 0.70 & 12.90 \\ 
  Materials & 1 230 & 4.39 & 2.59 & 1.71 & 0.62 & 13.45 \\ 
  Telecom Services & 236 & 1.89 & 1.39 & 1.40 & 0.40 & 11.93 \\ 
  Utilities & 421 & 3.46 & 2.08 & 1.64 & 0.58 & 12.30 \\ 
   \midrule 
Total & 7 034 & 41.15 & 23.38 & 1.65 & 0.79 & 13.71 \\ 
  
\bottomrule
\end{tabularx}
\label{ch3-table:forecasts-analyst}
\end{center}
\end{table}
The similar descriptive analysis but from the ``per stock'' perspective presented in \ref{ch3-table:forecasts-stock}. Namely, for the \sample{} (\filtered{}) data the total number of  ``$Stock \times Forecasts$'' observations is 112992~(30073). Each stock, on average, receives 6.18~(9.62) forecasts per quarter.  The average forecasts per analyst per quarters is 1.44~(1.67). On average, 61.6\%~(86.82\%) of stocks receive a revision of EPS forecasts within a quarter for \sample{} (\filtered{}) dataset. Finally, on average,  a stock is followed by analysts for 6.76~(14.8) quarters.


\begin{table}
\small\addtolength{\tabcolsep}{-2pt}
\caption{Descriptive statistics of forecasts per stock}
\ The table presents the descriptive statistics per stock for \sample{} (Panel A) and \filtered{} (Panel B) data. Namely, the table shows the total number of stock-forecast observations,  the average number of forecast per quarter per stock, the average number of following analysts per stock, the average number of forecasts per analyst per stock, share of stocks that got their forecast revised by analysts ,and, finally, the average number of quarters a stock being followed by a analyst.
\begin{center}
%\begin{tabular}{rcccc}
\begin{tabularx}{\linewidth}{r*{7}{Y}}
 \toprule
 &Obsrv & Frcst/q & analysts & Frcst/analyst &Rev.&follow time, q \\
 \multicolumn{7}{l}{\textbf{Panel A: \sample{} data}}\\
  \midrule
 Consumer Discretionary & 20 772 & 6.97 & 4.15 & 1.47 & 0.66 & 7.19 \\ 
  Consumer Staples & 5 321 & 5.37 & 3.43 & 1.42 & 0.63 & 6.56 \\ 
  Energy & 8 325 & 10.61 & 5.39 & 1.67 & 0.76 & 7.62 \\ 
  Financials & 17 222 & 5.44 & 3.40 & 1.49 & 0.58 & 7.20 \\ 
  Health Care & 14 407 & 5.32 & 3.58 & 1.33 & 0.57 & 5.45 \\ 
  IT & 22 469 & 7.08 & 4.60 & 1.40 & 0.64 & 6.63 \\ 
  Industrials & 17 187 & 4.67 & 3.10 & 1.41 & 0.59 & 7.09 \\ 
  Materials & 2 518 & 4.81 & 3.02 & 1.44 & 0.62 & 7.28 \\ 
  Telecom Services & 399 & 4.72 & 3.51 & 1.33 & 0.56 & 5.41 \\ 
  Utilities & 4 372 & 2.97 & 2.10 & 1.27 & 0.42 & 5.69 \\ 
   \midrule 
Total & 112 992 & 6.18 & 3.84 & 1.44 & 0.62 & 6.76 \\ 
  
\midrule
\end{tabularx}
\begin{tabularx}{\linewidth}{r*{7}{Y}}
\multicolumn{7}{l}{\textbf{Panel B: \filtered{} data}}\\
%  \cline{2-5}
% & Forecasts & Brokers & Forecast/broker & follow time, q \\
 \midrule
 Consumer Discretionary & 6 254 & 10.03 & 5.47 & 1.74 & 0.89 & 15.08 \\ 
  Consumer Staples & 1 499 & 7.96 & 4.77 & 1.64 & 0.86 & 15.03 \\ 
  Energy & 3 432 & 15.33 & 7.23 & 2.01 & 0.94 & 18.80 \\ 
  Financials & 4 782 & 8.90 & 5.15 & 1.65 & 0.86 & 14.91 \\ 
  Health Care & 3 235 & 8.44 & 5.19 & 1.58 & 0.84 & 13.95 \\ 
  IT & 5 780 & 9.79 & 5.98 & 1.59 & 0.87 & 13.75 \\ 
  Industrials & 4 017 & 7.08 & 4.46 & 1.57 & 0.83 & 14.23 \\ 
  Materials & 760 & 7.10 & 4.19 & 1.65 & 0.85 & 14.47 \\ 
  Telecom Services &  80 & 5.59 & 4.10 & 1.35 & 0.72 & 11.50 \\ 
  Utilities & 234 & 6.23 & 3.74 & 1.65 & 0.82 & 11.08 \\ 
   \midrule 
Total & 30 073 & 9.62 & 5.47 & 1.67 & 0.87 & 14.80 \\ 
  
\bottomrule
\end{tabularx}
\label{ch3-table:forecasts-stock}
\end{center}
\end{table}

\begin{figure}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}
\includegraphics[width=\maxwidth]{figure/chap3-fig-num-1} 

\end{knitrout}
\caption{Total number of EPS forecasts.}
\ The plot shows the log of total number of forecasts per quarter for \sample{}d and \filtered{} data sets.
\label{ch3-fig:tot}
\end{figure}

\begin{figure}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}
\includegraphics[width=\maxwidth]{figure/chap3-fig-mean-f-1} 

\end{knitrout}
\caption{Average number of EPS forecasts}
\ The plot depicts the average number of EPS forecasts per analyst (left panel) and per stock (right panel) for \sample{} and \filtered{} datasets.
\label{ch3-fig:mean-f}
\end{figure}

% \begin{figure}
% <<chap3-fig-tot-stocks,echo=FALSE, include=TRUE,results='hide',fig.width=10.7,fig.height=8.3>>=
%
% ## Calculates number of Brokers per stock and number of stocks per broker
% plot.data <- rbind(num.f.dt[,.N,by=.(q.id,Stock,data.type)][,mean(N),by=.(q.id,data.type)][,perspective:='brokers/stock'],
% num.f.dt[,.N,by=.(q.id,Broker,data.type)][,mean(N),by=.(q.id,data.type)][,perspective:='stocks/broker'])
%
% ggplot(plot.data,aes(x=as.Date(q.id),y=V1,color=data.type))+theme_bw()+geom_smooth(method='loess',se=F)+xlab('Quarters')+theme(plot.title = element_text(colour = "Black"),legend.position='top',legend.title=element_blank(),strip.text=element_text(size=20),text=element_text(size=20,family='Times'))+geom_line()+ggtitle('Average number of brokers/stock and of stocks/broker')+facet_wrap(~perspective,scale='free_y')+ylab('Count')#+geom_line(stat = "hline", yintercept = "mean")
%
% @
% \caption{Average number of brokers (stocks) per stock (broker)}
% \ The figure shows the average number of brokers per stock and of stocks per broker for sample and filtered datasets.
% \label{ch3-fig:mean-stock}
% \end{figure}

\begin{figure}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}
\includegraphics[width=\maxwidth]{figure/chap3-fig-rev-1} 

\end{knitrout}
\caption{Revisions of forecasts}
\ The plot shows the average percent of analysts (stocks) that revise (got revised) their forecasts for \sample{} and \filtered{} dataset. Horizontal panels shows the number of revisions per quarter from 1 revision per quarter (top panel) to 5 (bottom panel).
\label{ch3-fig:rev}
\end{figure}
Figures \vpagerefrange{ch3-fig:tot}{ch3-fig:rev} depict some per quarter statistics. \ref{ch3-fig:tot} plots log of total number of EPS forecasts for both datasets. We observe that, while both datasets experience a constant growth in issuing forecasts, at the end of the sample period the \filtered{} datasets shows a decline which can be contributed to the sub-prime crisis of 2007-2009. When looked at the per quarter forecast statistics in \ref{ch3-fig:mean-f}, we observe that the analysts in \filtered{} dataset issued fewer forecasts per quarter compared to those of the \sample{} dataset. \ref{ch3-fig:rev} plots the average percent of analysts that revise their forecasts (revise from 1 time (top panel) to 5 times (bottom panel) per quarter). We observe that the analysts in \filtered{} datasets, on average, are more active in revising their EPS forecasts. As we observe, despite the smaller number of stocks and total issued forecasts, the \filtered{} dataset selects analysts that actively revise their forecasts and has a longer duration of a stock coverage when compared to those of the \sample{} dataset. Because of this reason, we use \filtered{} dataset to build the rankings.

\section{Experiment setup}
\label{ch3-sec:exp_setup}
\subsection{Rankings of financial analysts}
\label{ch3:eps-rank}
%%% change s to i for stock
Analysts are ranked on the basis of Proportional Mean Absolute Forecast Error (PMAFE) that measures the accuracy of a forecast ~\citep{clement1999,brown2001,ertimur2007}. First,  we define the forecast  error  $FE_{a,s}$ as an absolute value of the difference between an analyst's $a$ forecasted EPS  and actual EPS for each stock $s$:

\begin{equation}
FE_{a,s}=|{ACT_{s}-PRED_{a,s}}|
\end{equation}
The PMAFE is given as:
\begin{equation}
PMAFE_{a,s}= \frac{FE_{a,s}}{\overline{FE_{s}}}
\end{equation}
where $ACT_{s}$ and $PRED_{a,s}$ are the actual quarterly EPS and  analyst $a$'s EPS forecast for stock $s$ respectively.

Second, we rank analysts based on their PMAFE score:
\begin{equation}
\label{ch3-eps:rank}
rank_{a,s}=\mathrm{rank}_{a=1}^{N} \left\{ PMAFE_{a,s} \right\}
\end{equation}



\subsubsection{Ranking contingency results}
\label{ch3-tab:rank-contin}

We analyze the analysts' ranking consistency based on the process outlined in \cite{aiguzhinov2015a}.  Namely, we split the rankings into three terciles (\textit{top}, \textit{medium}, \textit{bottom}). In one particular quarter ($t$), we place  analysts at one of these bins which corresponds to a tercile. We, then,  check analysts position at the immediate next quarter ($t+1$) and after one year ($t+4$).

Beforehand, we convert the rankings into scores as follows:
\begin{equation}
\label{ch3-eq:score}
score_{a,s}=\frac{rank_{a,s}}{\max{rank_s}}
\end{equation}

To get the cross-sectional values of scores across different stocks, we take the average of $score_{a,s}$
\begin{equation}
\label{ch3-eq:mean-score}
\overline{score_{a}}= \frac{1}{k} \sum_{s=1}^{k} score_{a,s}
\end{equation}
where $k$ is number of stocks followed by a particular analyst $a$.

\ref{ch3-rank-stat} summarizes the resulted contingency table. We observe that analysts exhibit strong ranking consistency as, on average, they stay at the same tercile. The table demonstrates that 50.05\% and  28.83\%~(46.75\% and 30.83\%) of the analysts  remained in the top and bottom terciles, respectively, after one quarter (year).

%Panel B shows that for the \filtered{} set eps.cont.tab[1,1,1]*100\% and  eps.cont.tab[3,3,1]*100\% (eps.cont.tab[1,1,2]*100\% and  eps.cont.tab[3,3,2]*100\%) of analysts stayed at the top and bottom terciles, respectively, after one quarter (year). Observe, that analysts from the \filtered{} set are more likely to stay in the top bin then those from the full \sample{} dataset.


\begin{table}
  \caption{Analysts' rankings contingency}
\label{ch3-rank-stat}
\ The contingency table shows changes in analysts'  \textit{top}, \textit{middle}, \textit{bottom} ranking bins. Panel A (B) is the results of the rankings based on the \sample{} (\filtered{}) dataset.
\begin{tabularx}{\linewidth}{r*{6}{Y}}
    \toprule
&&$top$&$middle$&$bottom$&$Sum$\\
\midrule
%\multicolumn{6}{l}{\textbf{Panel A: \sample{}}}\\
\multirow{10}{*}{$t$}&&\multicolumn{4}{c}{$t+1$} \\
%' %&&$top$&$middle$&$bottom$&Sum\\
%' <<chap3-rank-full,echo=F,results='asis'>>=
%' #t+1
%' tab.r <- acast(rbind(melt(sample.cont.tab[,,'t']),data.table(melt(apply(eps.cont.tab,c(1,3),sum)))[Var2=='t']),Var1~Var2,value.var='value')*100
%' print(xtable(tab.r,hline = F,digits=1),only.contents=T,include.colnames=F,include.rownames=F,hline.after=NULL,add.to.row = list(pos=list(0,1,2),command=c('&$top$&','&$middle$&','&$bottom$&')),sanitize.text.function = function(x) x)
%' # t+4
%' tab.r <- acast(rbind(melt(sample.cont.tab[,,'t+4']),data.table(melt(apply(eps.cont.tab,c(1,3),sum)))[Var2=='t']),Var1~Var2,value.var='value')*100
%' #colnames(tab.r) <- c('$top$','$middle$','$bottom$','Sum')
%' print(xtable(tab.r,hline = F,digits=1),only.contents=T,include.colnames=F,include.rownames=F,hline.after=NULL,add.to.row=list(pos=list(0,0,1,2),command=c('&&\\multicolumn{4}{c}{$t+4$}\\\\ \n','&$top$&','&$middle$&','&$bottom$&')),sanitize.text.function = function(x) x)
%' @
%' \midrule
%' \end{tabularx}
%' \begin{tabularx}{\linewidth}{r*{6}{Y}}
%' \multicolumn{6}{l}{\textbf{Panel B: \filtered{}}}\\
%\multirow{10}{*}{$t$}&&\multicolumn{4}{c}{$t+1$} \\
%&&$top$&$middle$&$bottom$&Sum\\
  &$top$&50.1 & 25.5 & 25.2 & 100.8 \\ 
   &$middle$&49.8 & 25.7 & 25.5 & 101.1 \\ 
   &$bottom$&46.0 & 26.3 & 28.8 & 101.1 \\ 
    &&\multicolumn{4}{c}{$t+4$}\\ 
 &$top$&46.8 & 28.4 & 27.0 & 100.8 \\ 
   &$middle$&46.0 & 27.7 & 28.3 & 101.1 \\ 
   &$bottom$&44.8 & 27.7 & 30.8 & 101.1 \\ 
  
\bottomrule
\end{tabularx}
\end{table}



\subsection{Dynamic states}
As we have mentioned above, we want to capture the state of the world in which the analysts operate. For this reason, it is necessary to take into account the dynamics of independent variables from one time period to another. We propose the following methods:
\begin{itemize}
\item \last{}: no dynamics in the state of the  variables, i.e., independent variables used as they are: $x_{\Delta{t}}=x_{t}$;
\item  \diff{}: first-difference  of the variables, i.e., $x_{\Delta{t}}=x_t-x_{t-1}$;
\item  \random{}: in time series decomposition of the independent variables, it is an unobserved component: $x_{\Delta{t}}=T(t)+S(t)+\epsilon (t)$, where $T(t)$- trend, $S(t)$ - seasonal part and $\epsilon (t)$ - random part of time series decomposition.
\item  \rollsd{}: rolling 8 quarters standard deviation of the independent variables~\citep{zivot2003}:
\begin{eqnarray}
\mu_t(8)&=&\frac{1}{8}\sum_{j=0}^7 x_{t-j} \nonumber \\
\sigma^2_t(8)&=&\frac{1}{7}\sum_{j=0}^7 (x_{t-j}-\mu(8))^2
\end{eqnarray}

\end{itemize}
Each of these methods produces a different set of attributes. By building a discriminative model on each one of them separately, we get different sets of discriminative power of the variables.


\section{Results}
\label{ch3-sec:results}

We report the result of the discriminative power of the variables in terms of their contribution in affecting the  rankings of financial analysts~(\ref{ch3:eq-dp-cont}). Panel A of the \ref{ch3-tab:dp-cont} shows the case of analyst specific variables. We report that \emph{uncert} is the most contributive variable for the all dynamic states with its maximum contribution occurring at the \last{} state (14.25\%). The least contributive variable of the analysts specific variables is the \emph{assym}, the asymmetry of information, with the smallest share of contribution to the discriminative power (2.23\%, 1.73\%, 1.62\%, 0.16\% for the \last{}, \diff{}, \random{}, and \rollsd{} states respectively).  Thus, our model of the discriminative power suggests that in the \last{} state, of all analysts' specific variables,  the rankings are most affected by the earnings uncertainty.

Panel B of \ref{ch3-tab:dp-cont} presents the contribution of stock specific variables. We report that in each of the dynamic states the \emph{s.ret} (\emph{size}) showed the maximum (minimum) contribution to the difference in analysts' opinions regarding EPS forecasts (8.68\% (0.23\%), 9.39\% (0.4\%), 13.66\% (0.76\%), 12.76\% (1.3\%) for the \last{}, \diff{}, \random{}, and \rollsd{} states respectively). As we defined above, the variability of stock returns is the measure of uncertainty about future earnings; thus, we report that, similar to the analyst-specific variables, the uncertainty is responsible for the rankings when consider stock-specific variables.


Finally, panel C shows the case of macroeconomic variables. Contrary to the previous variable types, the distribution of the most contributive  individual variable differs  across  different states. For the \last{} state, it is \emph{infl} (27.59\%)  whereas for the all others states it is \emph{t.bill} (25.69\%, 21.98\%, 26.02\% for the \diff{}, \random{}, and \rollsd{} states respectively). The least contributive variable is the \emph{vix.ret}: 0.38\%, 0.45\%, 1.64\%, and 0.92\% for the \diff{}, \random{}, and \rollsd{} states respectively.


The cross panel analysis of \ref{ch3-tab:dp-cont} show that the macroeconomic variables are the most contributive of all. Specifically, the \rollsd{} state accounts for 73.56\% of total contribution in differences in rankings; for other states the share of these variables are (in a decreasing order) 61.32\%, 59.98\%, and 58.32\% for the \diff{}, \last{}, and \random{} states respectively.  \ref{ch3-fig:mean-dp} depicts the plot of total contribution of each of the variable conditional on variable types. Thus, we conclude that the condition of the economy represented by the GNP, inflation rate, stock market volatility, and interest rate  is the one that influence the most analysts' opinions about future stocks' performance with respect to their earnings.


We also perform a hypothesis pairwise test of whether the discriminative power of variables in the dynamic states is significantly different from those in the \last{} state. We report results in \ref{ch3-table:test}. Panel A shows significance of analysts specific variables in dynamic states. We reject the \emph{null} hypothesis at 1\% significance level in all variables of this category for the states \diff{} and \random{}, and variables \emph{uncert} and \emph{assym} for the \rollsd{} state. We fail to reject the \emph{null} for the variable \emph{disp} for the \rollsd{} state at 10\% significance. Panel B presents the case of stock-specific variables and we report that we reject the \emph{null} for all variables for all dynamic states except \emph{accr} for the \rollsd{} state. Finally, panel C shows the case of macroeconomic variables and we also reject the \emph{null} for all the variables and states except \emph{infl} in the \diff{} state.


\ref{ch3-fig:time-dp} plots the percentage split of the average \DP{} between the analyst-, stock-specific, and macroeconomic variables per quarter for each of the states. We observe that, as mentioned above,  macroeconomic variables are the most contributive components to the analysts' rankings. In the \diff{} state, it is visible that at the end of the sample period (years 2006--2009) analyst-specific variables become more contributive. %\ref{ch3-fig:mean-dp-agg} shows the average \DP{} across states for the analysts-, stock-specific, and macroeconomic variables.

%\ref{ch3-table:dp} shows the results of identifying the discriminative power of the variables for different dynamic states of the variables. Panel A presents the case for the analyst specific variables. We report that paste0('\\',names(which.max(stat.type[1,,2]))){} is the dynamic state that exhibit the most contribution to the analysts' rankings as it resulted with highest value of \DP{} among other states.   %(the average \DP{} of max(stat.type[1,,2])).

%The stock specific variables are also more contributive in paste0('\\',names(which.max(stat.type[2,,2]))){} (max(stat.type[2,,2])) as well as the macroeconomic variables (max(stat.type[3,,2])) which are the most contributive among all the others variable types. Thus, the paste0('\\',names(which.max(stat.type[1,,2]))){} state of the variables is the one that contributes most to the ranking.

%'
%' \begin{landscape}
%' \begin{table}
%' \small\addtolength{\tabcolsep}{-2pt}
%' \caption{Discriminate power of the variables, \DP{} $>0$}
%' \ The table presents the discriminative power for cases when \DP{} $>0$. Different types of state of the variables are considered:  \last{} is the state with no dynamics in values of the variables, \diff{} is the state with first-difference in values, \random{} is the state that captures the random part of values time-series decomposition,  and \rollsd{} is the state of variable sliding 8 quarters standard deviation. \DP{} is the discriminative power of a variable obtained from \ref{ch3:eq-dp}.
%' \begin{tabu} to \linewidth{r*{13}{Y}}
%' \toprule
%'  Variable&\multicolumn{3}{c}{\last{}} &\multicolumn{3}{c}{\diff{}}& \multicolumn{3}{c}{\random{}}&\multicolumn{3}{c}{\rollsd{}}\\
%' &median&mean&st.dev&median&mean&st.dev&median&mean&st.dev&median&mean&st.dev\\
%' \midrule
%' <<chap3-dp-median,echo=F,results='asis',warning=F>>=
%' print(xtable(data.table(vvs=rownames(res.dp[,1,]),res.dp.all)),display=c('s','s',rep('f',10)),only.contents=T,include.colnames=F,include.rownames=F,hline.after=NULL,add.to.row=list(pos=list(c(3,4,10,11,15),0,4,11),command=c('\\midrule \n','\\multicolumn{13}{l}{\\textbf{Panel A: Analyst}}\\\\ \n','\\multicolumn{13}{l}{\\textbf{Panel B: Stock}}\\\\ \n','\\multicolumn{13}{l}{\\textbf{Panel C: Macro}}\\\\ \n')))
%' @
%' \bottomrule
%' \label{ch3-table:dp}
%' \end{tabu}
%' \end{table}
%' \end{landscape}


%The least ranking contributive state for the analyst specific variables is gsub('\\.','',paste0('\\',names(which.min(stat.type[1,,2])))){} (min(stat.type[1,,2])). For the stock specific and the macroeconomic variables the least contribution  to the rankings occurs in paste0('\\',names(which.min(stat.type[2,,2]))){} (average \DP{} values of min(stat.type[2,,2]) and min(stat.type[3,,2]) for stock and macro variables respectively). \ref{ch3-fig:mean-dp-agg} plots a bar chart of the variable types for different aggregation methods. Observe that cumulatively for the all type of the variables, paste0('\\',names(which.min(stat.type[2,,2]))){} state is the least to affect the rankings while, as mentioned above, paste0('\\',names(which.max(stat.type[3,,2]))){} is the state that affects the rankings the most.


\begin{table}
\caption{Contribution of each of the variable to rankings, in \%}
\ The table shows the percent of contribution of the variable's \DP{} value to total value of \DP{}. State \last{} is the state with no dynamics in values of the variables, \diff{} is the state with first-difference in values, \random{} is the state that captures the random part of values time-series decomposition,  and \rollsd{} is the state of variable sliding 8 quarters standard deviation.
\label{ch3-tab:dp-cont}
\begin{tabu} to \linewidth{r*{5}{Y}}
\toprule
Variable&\last{}&\diff{}&\random{}&\rollsd{} \\
\midrule
  \multicolumn{5}{l}{\textbf{Panel A: Analyst}}\\ 
uncert & 14.25 & 12.49 & 9.71 & 1.25 \\ 
  assym & 2.23 & 1.73 & 1.62 & 0.16 \\ 
  disp & 9.00 & 8.81 & 4.79 & 0.67 \\ 
   \midrule 
Total & 25.47 & 23.03 & 16.11 & 2.08 \\ 
   \midrule 
 \multicolumn{5}{l}{\textbf{Panel B: Stock}}\\ 
btm & 0.78 & 1.91 & 3.04 & 3.37 \\ 
  size & 0.23 & 0.40 & 0.76 & 1.30 \\ 
  dte & 1.68 & 2.52 & 2.83 & 4.05 \\ 
  accr & 1.74 & 0.77 & 1.96 & 1.31 \\ 
  s.ret & 8.68 & 9.39 & 13.66 & 12.76 \\ 
  sec.ret & 1.43 & 0.66 & 3.31 & 1.57 \\ 
   \midrule 
Total & 14.54 & 15.65 & 25.57 & 24.37 \\ 
   \midrule 
 \multicolumn{5}{l}{\textbf{Panel C: Macro}}\\ 
gnp & 22.03 & 17.53 & 18.12 & 24.26 \\ 
  infl & 27.59 & 17.65 & 16.58 & 22.37 \\ 
  vix.ret & 0.38 & 0.45 & 1.64 & 0.92 \\ 
  t.bill & 9.99 & 25.69 & 21.98 & 26.02 \\ 
   \midrule 
Total & 59.98 & 61.32 & 58.32 & 73.56 \\ 
  

\bottomrule
\end{tabu}
\end{table}


\begin{table}
\caption{Significance of dynamic states}
\ This table shows the results of pairwise t-test of \last{} state of the variables vs. the dynamic states. The null is that the difference in \DP{} values is zero. State \last{} is the state with no dynamics in values of the variables, \diff{} is the state with first-difference in values, \random{} is the state that captures the random part of values time-series decomposition,  and \rollsd{} is the state of values sliding 8 quarters standard deviation.
\begin{tabularx}{\linewidth}{l*{7}{Y}}
\toprule
Variable&\multicolumn{2}{c}{\diff{}}& \multicolumn{2}{c}{\random{}}&\multicolumn{2}{c}{\rollsd{}}\\

& t value& Pr$(>\vert t\vert)$ &t value& Pr$(>\vert t\vert)$ &t value& Pr$(>\vert t\vert)$ \\
\midrule

  \multicolumn{7}{l}{\textbf{Panel A: Analyst}}\\ 
uncert & 9.31 & 0.00 & 15.88 & 0.00 & 7.61 & 0.00 \\ 
  assym & 5.57 & 0.00 & 8.24 & 0.00 & 3.46 & 0.00 \\ 
  disp & 2.53 & 0.01 & 9.74 & 0.00 & 1.61 & 0.11 \\ 
   \midrule 
 \multicolumn{7}{l}{\textbf{Panel B: Stock}}\\ 
btm & 8.70 & 0.00 & 13.04 & 0.00 & 12.78 & 0.00 \\ 
  size & 4.32 & 0.00 & 5.91 & 0.00 & 8.57 & 0.00 \\ 
  dte & 8.61 & 0.00 & 6.80 & 0.00 & 4.61 & 0.00 \\ 
  accr & -3.16 & 0.00 & 3.63 & 0.00 & -0.73 & 0.47 \\ 
  s.ret & 11.46 & 0.00 & 22.24 & 0.00 & 17.68 & 0.00 \\ 
  sec.ret & -4.15 & 0.00 & 8.62 & 0.00 & 1.90 & 0.06 \\ 
   \midrule 
 \multicolumn{7}{l}{\textbf{Panel C: Macro}}\\ 
gnp & 9.35 & 0.00 & 31.81 & 0.00 & 24.94 & 0.00 \\ 
  infl & -0.67 & 0.50 & 26.21 & 0.00 & 19.35 & 0.00 \\ 
  vix.ret & 1.63 & 0.10 & 7.23 & 0.00 & 4.17 & 0.00 \\ 
  t.bill & 40.58 & 0.00 & 29.88 & 0.00 & 43.02 & 0.00 \\ 
  
\bottomrule
\label{ch3-table:test}
\end{tabularx}
\end{table}


\begin{figure}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}
\includegraphics[width=\maxwidth]{figure/chap3-fig-dp-1} 

\end{knitrout}
\caption{The discriminative power of the variables.}
\ The plot depicts the discriminative power of variables. State \last{} is the state with no dynamics in values of the variables, \diff{} is the state with first-difference in values, \random{} is the state that captures the random part of values time-series decomposition,  and \rollsd{} is the state of variable sliding 8 quarters standard deviation.
\label{ch3-fig:mean-dp}
\end{figure}


\begin{figure}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}
\includegraphics[width=\maxwidth]{figure/chap3-fig-dp-q-id-1} 

\end{knitrout}
\caption{Distribution of discriminative power per quarter}
\ The plot shows the composition of discriminative power across variable categories. The states  are: \last{} is the state with no dynamics in values of the variables, \diff{} is the state with first-difference in values, \random{} is the state that captures the random part of values time-series decomposition,  and \rollsd{} is the state of variable sliding 8 quarters standard deviation.
\label{ch3-fig:time-dp}
\end{figure}

%'
%' \begin{figure}
%' <<chap3-fig-agg,echo=FALSE, include=TRUE,results='hide',fig.width=10.7,fig.height=8.3>>=
%' ggplot(na.omit(metric.vvs)[round(metric)>0][,mean(metric),by=.(method,vvs.type)],aes(x=method,y=V1,fill=vvs.type))+geom_bar(stat='identity')+theme_bw()+ggtitle('Average discriminative power and variable aggregation')+theme(plot.title = element_text(colour = "Black"),legend.position='top',legend.title=element_blank(),strip.text=element_text(size=20),strip.text.y = element_text(angle = 0),text=element_text(size=20,family='Times'),legend.text=element_text(size=20,family='Times'))+ylab('DP')
%' @
%' \caption{The average discriminative power across states}
%' \ The plot depicts the average discriminative power of variable types. State \last{} is the state with no dynamics in values of the variables, \diff{} is the state with first-difference in values, \random{} is the state that captures the random part of values time-series decomposition,  and \rollsd{} is the state of variable sliding 8 quarters standard deviation.
%' \label{ch3-fig:mean-dp-agg}
%' \end{figure}
%'


%Analyzing the results on the individual variable level, for the case  of analyst specific variables, \emph{disp} (\emph{assym}) is the variable that has the highest (lowest) value of average \DP{} across all states of the world. In the case of stock specific variables, \emph{s.ret} benefits the most to the rakings of the analysts in every state of the world and the least beneficial are \emph{sec.ret} (in the states of  \last{} and  \rollsd{}) and \emph{size} (in the states of  \diff{} and \random{}). The case of macroeconomic variables shows that the most contributive variables are \emph{infl} (\last{} and \random{}) and \emph{t.bill} (\diff{} and \rollsd{}) while the \emph{vix.ret} is the least in every state of the world.









%\section{Conclusion}
%\label{ch3-sec:conclusion}
%Some institutions, such as StarMine, rank financial analysts based on their accuracy and investment value performance. These rankings are published and are relevant: stocks favored by top-ranked analysts will probably receive more attention from investors. Therefore, there is a growing interest in understanding the relative performance of analysts. Typical approaches are based on individual characteristics of those analysts or past analyst forecasting accuracy. Here, we follow an alternative approach that links the general behavior of rankings of analysts to variables that explain the uncertainty and information asymmetry on analyst-specific, stock-specific, and macroeconomic levels.

%We introduce a new approach, based on the naive Bayes Label Ranking algorithm, in identifying the discriminative power of a variable; thus,  its contribution to the rankings conditional on different states of the world: static state, first-difference, random part of time-series decomposition, and sliding standard deviation.

%We report that for the analysts- and stock-specific variables, the uncertainty about future stock performance is the most contributive to the changes in rankings. The macroeconomic variables influence rankings the most considering all variables at once.

%For the future work we would like to take the findings of this paper and apply them to the problem of predicting the actual rankings of the analysts.
\processdelayedfloats
\cleardoublepage

\chapter{}
\label{ch4}
%\begin{abstract}
%\input{./abstract/ch4-abstract}
%\end{abstract}





%<<set-parent, echo=FALSE, cache=FALSE>>=
%set_parent('~/Dropbox/workspace/Projects/Thesis/thesis.Rnw')
%@



%\section{Introduction}
%\label{ch4-sec:introduction}


%Rankings of financial analysts is not new in finance. Many agencies develop their procedures to evaluate analysts based on their performance either in forecasting or stock recommendations. Some institutions even hold a ``Red Carpet'' event to recognize the top analysts. On one hand, for market participants, the rankings may signal who is the best analysts. On the other hand, studies have shown that following the best analysts' recommendations of buy-sell stocks have statistically insignificant benefits.

%In this paper we have an objective to show that rankings can serve as inputs for trading; that is, they can be a direct input for strategy. We base our research on the main assumption that analysts at the top ranks are the best analyst and they are worth to be followed. Naturally, instead of relying on personal expertise in selecting the expected returns, it is best to refer to the specialist in the field, namely, the financial analysts. Using the analysts' price target information we  create a vector of expected returns and use it as a starting point for appropriate trading strategy.

%Given the objective of the paper, we have to solve two problems. First, we need to predict the rankings of the analyst and, second, translate these rankings into an operational input for the trading strategy.

%For the first problem, we take advantage of the algorithm used for prediction of rankings developed in \cite{aiguzhinov2010} and adapt it for the case of analysts' rankings. In short, this algorithm is based on the Bayesian probability and the similarities between the rankings.
%The solution for the second problem relies on the Black-Litterman (BL) model~\citep{black1992}. We are particularly confident in this choice of tools given that the BL model and ranking algorithm are both based on the Bayesian framework. Given the results form \cite{aiguzhinov2015a}, we base our rankings on analysts' target prices as it has been shown that strategies based on these rankings are the ones that yield the highest cumulative annualized return.

%The paper is organized as follows: \ref{ch4-sec:ranking} provides motivation on use of rankings; \ref{ch4-sec:lr} outlines label ranking algorithm; \ref{ch4-sec:vvs} summarizes the selection of the independent variables that affect the analysts' rankings; \ref{ch4:sec-tr} describes a methodology of building the rankings; \ref{ch4:inf-set} discusses information environments that influence analysts' decisions; \ref{ch4-sec:trading} outlines the steps of the trading strategy; \ref{ch4-sec:data} describes data used in the study; \ref{ch4-sec:results} discusses the results, and \ref{ch4-sec:conclusion} concludes.

\section{Rankings of financial  analysts}
\label{ch4-sec:ranking}
In  financial literature there has been a long debate on whether financial analysts produce valuable  advice. Some argue that following the advice of financial analysts,  translated as recommendations of buying, holding, or selling a particular stock, does not yield  abnormal returns, i.e.,  returns that are above the required return to compensate for risk. The Efficient Market Hypothesis~\citep{fama1970ecm} states that financial markets are efficient and that any public available information  regarding a stock would be immediately reflected in prices; hence, it would be  impossible to generate abnormal returns based upon past information.

Yet, several authors have since stressed that  there are information-gathering costs and information is not immediately reflected on prices~\citep{grossman1980iie}. As such, prices may not  reflect all the available information at all time because if this were the case, those who spent resources to collect and analyze   information would not have an incentive to do it, because there would not get any compensation for it.

Many trading strategies try to forecast the price movements relying on the historical prices or estimate the intrinsic value of a company. Obviously, this type of research is associated with significant amount of up-front costs to acquire databases, software, etc. On the other hand, financial analysts have these tools and, presumably, skills to identify  stocks that worth be invested. Thus, for an investor, it is cheaper to follow the recommendations of financial analysts rather than perform a proper stock market analysis.


Some authors show that financial analysts' recommendations create value to investors \citep{womack1996,barber2001}\footnote{\cite{womack1996} finds that  post-recommendation excess returns are not mean-reverting, but are significant and in the direction forecast by the analysts.~\cite{barber2001} finds that over the period of 1986-1996 a portfolio of stocks with the most (least) favorable consensus analyst recommendations yields an average abnormal return of 4.13 (-4.91)\%.}. Assuming that some analysts produce valuable advice it makes sense to rank analysts based on the accuracy of their recommendations.

StarMine rankings are based on financial analysts' accuracy either on TP or EPS forecasts. To rank analysts based on EPS forecasts, StarMine developed a proprietary metric called a Single-stock Estimating Score (SES). This score measures ``... [a] relative accuracy; that is, analysts are compared against their peers. An analyst's SES can range from 0 to 100, with 50 representing the average analyst. To get a score higher than 50, an analyst must make estimates that are both significantly different from and more accurate than other analysts' estimates''\footnote{\url{http://excellence.thomsonreuters.com/award/starmine?award=Analyst+Awards&award_group=Overall+Analyst+Awards}}.


As for target price ranking, StarMine's methodology compares the portfolios based on analysts recommendations. Portfolios are constructed as follows. For each ``Buy'' recommendation, the portfolio is one unit long the stock and simultaneously one unit short the benchmark. ``Strong buy'' gets a larger investment of two units long the stock and two units short the benchmark. ``Hold'' invests one unit in the benchmark (i.e., an excess return of zero). ``Sell'' recommendations work in the reverse way. StarMine re-balances its calculations at the end of each month to adjust for analysts revisions (adding, dropping or altering a rating), and when a stock enters or exits an industry grouping.


Recent evidence suggests that top ranked financial analyst affect market participants: prices seem to react more to the recommendations issued by the top-ranked analysts~\citep{emery2009}. As such, StarMine ranking based models can be used to identify such analysts and generate superior estimates (e.g., SmartEstimates\footnote{\url{http://www.starmine.com/index.phtml?page_set=sm_products&sub_page_set=sm_professional&topic=analytical&section=accurate_estimates}}).



The goal of our study is to predict  StarMine rankings. With this purpose, we adapt a Machine Learning  algorithm to predict rankings  given a set of variables that characterize these rankings. We, further, apply the predicted rankings  to build active trading strategies to evaluate quality of predictions against the consensus strategy (giving equal weights to analysts' recommendations).


\input{./sections/lr}
\input{./sections/vvs}


\section{Target rankings of financial analysts}
\label{ch4:sec-tr}
 Analysts are ranked on the basis of Proportional Mean Absolute Forecast Error (PMAFE) that measures the accuracy of a forecast ~\citep{clement1999,brown2001,ertimur2007}. We use both target price and EPS accuracy to build the rankings.

\subsection{Target Price ranking}
\label{ch4:rank}
%%% change s to i for stock
 We define the forecast daily error  $FE_{j}$ as the absolute value of the difference between analyst' target price $TP_{j}$ and the daily stock price $P$ for each stock:

\begin{equation}
	\label{ch4-dfe}
	FE_{j}^{TP}=|{P-TP_{j}}|
\end{equation}
The PMAFE is given as:
\begin{equation}
	\label{ch4-tp:pmafe}
	PMAFE_{j}^{TP}=\frac{FE_{j}^{TP}}{\overline{FE^{TP}}}
\end{equation}
where $\overline{{FE}^{TP}}$ is the average forecasting error across analysts. The target price is fixed over the quarter unless it gets revised.

The rank is average analyst's $PMAFE^{TP}$ over a particular quarter:
\begin{equation}
	\overline{PMAFE_{j}^{TP}}=\frac{1}{d} \sum_{i=1}^{d} PMAFE_{j,i}^{TP}
\end{equation}

\begin{equation}
	\label{ch4-tp:rank}
	rank_{j}=\mathrm{rank}_{j=1}^{k} \left\{ \overline{PMAFE_{j}^{TP}} \right\}
\end{equation}
%where $d$ are the number of trading days in a quarter and $k$ is the number of analysts with a valid TP.


\subsection{EPS ranking}
\label{ch4:sec-eps}
To compute the EPS rankings, we apply the same procedure as above:
\begin{equation}
	FE_{j}^{EPS}=|{ACT-PRED_{j}}|
\end{equation}
\begin{equation}
	PMAFE_{j}^{EPS}= \frac{FE_{j}^{EPS}}{\overline{FE^{EPS}}}
\end{equation}
\begin{equation}
	\label{ch4-eps:rank}
	rank_{j}=\mathrm{rank}_{j=1}^{k} \left\{ PMAFE_{j}^{EPS} \right\}
\end{equation}
where $ACT$ and $PRED_{j}$ are the actual quarterly EPS and  analyst $j$'s EPS forecast for stock.




\section{Analysts' information environment}
\label{ch4:inf-set}
To proceed with the ranking prediction, we need to establish which information we  will be using to initially rank analysts.

\subsection{Past information sets}
Different analysts' ranks are obtained  if we select different time horizons. If we use only the most  recent information, we will capture the recent performance of the analysts. This, of course, is more sensitive to unique episodes (e.g., a quarter which has been surprisingly good or bad). If, alternatively, we opt to incorporate the entire analyst performance, the ranking is less affected by such events, yet it may not reflect the current analyst ability. We use two information sets: the first uses only the  information about the analyst' performance in period $t-1$; the second, uses all the available  information for that particular analyst. We call the former the \naive{} rankings and the latter the \default{} rankings.

In addition to these rankings,  we also create a hypothetical scenario that assumes we anticipate perfectly the future analyst accuracy performance  that would only be available at the end of $t$.
We call this the \tr{} rankings.

Formalizing information sets considered are:
\begin{itemize}
	\item  the \tr{} rankings%-- a perfect foresight information:
	\begin{equation}
		\label{rank:true}
		\mathrm{rank}_{j,t}=\mathrm{rank}_{j,t}
	\end{equation}

	\item  the \naive{} rankings % -- $t-$ information:
	\begin{equation}
		\label{rank:naive}
		\mathrm{rank}_{j,t}=\mathrm{rank}_{j,t-1}
	\end{equation}

	\item  the \default{}  rankings%-- the entire history of analysts
	\begin{equation}
		\label{rank:default}
		\mathrm{rank}_{j,t} = \frac{1}{T} \sum_{t=1}^{T} \mathrm{rank}_{j,t}
	\end{equation}

\end{itemize}
where $\mathrm{rank}_{j,t}$ is analyst $j$  rank at time $t$. These rankings will serve as  baselines to assess the quality of the predicted rankings.
%expected rank-weighted stock return (\ref{ch4-rankq}).

\subsection{Dynamic states}
For ranking predictions, the past  information sets   are no longer valid as we model the variables that affect analysts' performance  with \tr{} rankings. For this reason, it is necessary to take into account the dynamics of independent variables from one time period to another. We propose the following methods:
\begin{itemize}
	\item \last{}: no dynamics in the state of the  variables, i.e., independent variables used as they are: $x_{\Delta{t}}=x_{t}$;
	\item  \diff{}: first-difference  of the variables, i.e., $x_{\Delta{t}}=x_t-x_{t-1}$;
	\item  \random{}: in time series decomposition of the independent variables, it is an unobserved component: $x_{\Delta{t}}=T(t)+S(t)+\epsilon (t)$, where $T(t)$- trend, $S(t)$ - seasonal part and $\epsilon (t)$ - random part of time series decomposition.
	\item  \rollsd{}: rolling 8 quarters standard deviation of the independent variables~\citep{zivot2003}:
	\begin{eqnarray}
		\mu_t(8)&=&\frac{1}{8}\sum_{i=0}^7 x_{t-i} \nonumber \\
		\sigma^2_t(8)&=&\frac{1}{7}\sum_{i=0}^7 (x_{t-i}-\mu(8))^2
	\end{eqnarray}

\end{itemize}
Each of these methods produces a different set of attributes which corresponds to different predicted rankings. Overall, in each time period we would have seven different rankings: one is the perfect foresight ranking (\tr{}), two are based on the different sizes of analysts' past information (\naive{} and \default{}), and the rest are from the predicted model with different dynamic states (\last{},  \diff{}, \random{}, and  \rollsd{}).


We selected  variables that describe the information environment consistent with \cite{aiguzhinov2015b}. We use variable that have more than 10\% contribution to the rankings. \ref{ch4:tab-dp} demonstrates the total discriminative power of state variables for different states for EPS ranking. The variables that contribute the most to the rankings are:
\{\emph{uncert}; \emph{disp}; \emph{s.ret}; \emph{gnp}; \emph{infl}; \emph{t.bill}\}
.

\section{Trading Strategy}
\label{ch4-sec:trading}


We follow the Black-Litterman procedure developed in~\cite{aiguzhinov2015a}:
\begin{enumerate}
\item For each stock, at the beginning of quarter $t$, we use predicted  rankings of all analysts that we expect to be at the end of the quarter $t$;
\item Based on these  predicted rankings and analysts' price targets,  we define $Q_{t}$ and $\Omega_{t}$ (see (\ref{ch4-def-q})  and (\ref{ch4-def-omega}));
\item Using market information available at the last day of quarter $t-1$, we obtain the market inputs;
\item Apply BL model to get  optimized portfolio weights and buy/sell stocks accordingly;
\end{enumerate}

The model requires form an investor two inputs: the vector of expected returns and the confidence of these returns. The vector of returns is where we rely on the knowledge of the analysts. We use two types of expected returns: 1) ones that are based on the consensus among analysts about future stock performance; 2) ones that are based on the rankings of analysts.

\subsection{Defining $Q$}
\label{ch4-def-q}

For the consensus strategy, we use median of expected returns for a particular stock:
\begin{equation}
\label{ch4-consq}
Q_{cons}= \mathrm{median} \left\{r_{j}\right\}
%\frac{1}{N} \sum_{j=1}^{N} r_{j,i}
\end{equation}
%$N$ is the number of analysts with a valid TP report and
where $r_{j}=TP_{j}/P-1$  is last known analyst's $j$ expected return computed using the analyst price target $TP_{j}$ and stock price $P$\footnote{Consistent with the literature, we use stock price 3 days \emph{ex-ante} the TP announcement. This is done to avoid any information leakage around new TP announcement day~\citep{bonini2010}}.

For the strategies that weight the analysts' estimates of expected return the weight of each analyst $j$ is based on his/her rank such that the top analyst has the weight of 1 and then the weights diminish as the rank increases.

\begin{equation}
\label{ch4-eq:weight}
w_{j}=1-\frac{\mathrm{rank}_{j}-\min{ \{\mathrm{rank} \} }}{\max{\{\mathrm{rank} \}}}
\end{equation}
where $\mathrm{rank}_j$ is the predicted analyst $j$ rank (\ref{ch4:inf-set})

The expected rank-weighted return is thus:
\begin{equation}
\label{ch4-rankq}
Q_{rank}=\frac{\sum_{j=1}^{k} (w_{j} \times r_{j})}{\sum_{j=1}^{k} w_{j}}
\end{equation}

%This represents the perfect foresight strategy. The perfect foresight refers to analyst rankings not stock prices. Therefore, it serves a performance reference point to evaluate the other trading strategies and performance of the label ranking model.


\subsection{Defining the confidence of expected returns $\Omega$}
\label{ch4-def-omega}
The confidence of $Q$ is given by the coefficient of variation (CV) of forecasting errors:

\begin{equation}
\label{ch4-eq-cv}
\mathrm{CV} = \frac{\sigma (FE)}{\overline{FE}}
\end{equation}
where $\sigma$ and $\overline{FE}$ are the standard deviation and the mean of the forecast errors across analysts for TP. A low value of $\mathrm{CV}$ reflects consensual estimates of future prices.






\section{Data and experimental setup}
\label{ch4-sec:data}
\subsection{Database and sample}
We focus our sample on the  S\&P500 stocks. The period of the experiments runs from the first quarter of 2001 until the last quarter of 2009. We get the analysts price target and EPS forecast data from ThomsonReuters I/B/E/S dataset; the list of S\&P constituents and stock daily prices data are from DataStream as well as the market capitalization data.

Over the sample period, the total number of Equity Research Firms (ERF)\footnote{We use words ``analyst'' and ``Equity Research Firm'' interchangeably.} in TP dataset is 477, covering 502  stocks. Given the fact that financial analysts commonly issue TP with the one year horizon\footnote{According to Wharton Research Data Services (WRDS), 92.33\% of all price targets reported in I/B/E/S have a 12-month horizon~\citep{glushkov2009}.}, we assume that analysts keep their TP forecasts valid for one calendar year unless it is revised. After one year we assume that TP recommendation expires.

Consistent with other studies on analysts' expected returns that work with price targets ~\citep{bradshaw2002,brav2003,da2011}, we truncate the sample of $TP/P-1$ at the 5\textsuperscript{th} percentile (values below -0.14) and at the 95\textsuperscript{th} percentile (values above 0.99). This is done due to occurrence of the extreme values. Most of these extreme values are driven by misalignment errors found on I/B/E/S data\footnote{We found some differences between the  DataStream and I/B/E/S the databases. In some cases the stock-splits and the dividends were not properly adjusted.}. To implement ranking, we require that a stock had at least three equity research firms per quarter and that a equity research firm has to be active in covering a particular stock for at least 3 years (12 quarters). After all the  data requirements, our final sample number of equity research firms issued target prices is 152 covering 419 S\&P500 stocks. Overall, the number of observations ($\mathrm{Stock} \times \mathrm{ERF} \times  \mathrm{Quarter}$) is reduced  from 134336 (initial) to 90743 (filtered).

In the case of EPS forecasts, the initial file of quarterly EPS forecast consists of 437 ERFs covering 516 stocks. Considering the ranking data requirement, our final sample of EPS forecasts consist of  157 ERFs covering 402 S\&P500 stocks. The total number of observation is 80185.

\ref{ch4-tab:ret-stat} presents descriptive statistics of the price targets (panel A) and EPS forecasts (panel B). We observe that, on average, the analysts issue 5.52 and 5.6 of price targets and EPS forecasts per quarter respectively.

\subsection{Ranking contingency results}
\label{ch4-tab:rank-contin}

We check for analysts' ranking consistency as follows.   In one particular quarter ($t$), we place  analysts at one of the bins which corresponds to a tercile: \textit{top}, \textit{medium}, \textit{bottom}. We, then,  check analysts position at the immediate next quarter ($t+1$) and after one year ($t+4$).

Beforehand, we convert the rankings into scores as follows:
\begin{equation}
%\label{eq:score}
\mathrm{score}_{j}=\frac{\mathrm{rank}_{j}}{\max{\mathrm{rank}}}
\end{equation}

To get the cross-sectional values of scores across different stocks, we take the average of $\mathrm{score}_{j}$
\begin{equation}
\label{ch4-eq:mean-score}
\overline{\mathrm{score}_{j}}= \frac{1}{M} \sum_{i=1}^{M} \mathrm{score}_{j,i}
\end{equation}
where $M$ is number of stocks followed by a particular analyst $j$.

\ref{ch4:tab-rank-stat} shows a contingency analysis of the ranks. Panel A shows the dynamics of each tercile for rankings based on target price  accuracy for the \naive{} and the \default{} rankings. We observe that analysts exhibit strong ranking consistency as, on average, they stay at the same tercile after one quarter. For the \naive{} case, of the \textit{top} (\textit{bottom}) most accurate (inaccurate) analysts in the previous quarter 67.79\% (69.69\%) remain in that same tercile after one quarter. After one year the corresponding figures are lower respectively 46.04\% and 41.08\% for the \textit{top} and \textit{bottom} terciles. In case of the \default{}, the analyst consistency is even more profound with 92.35\% (92.13\%) of analysts that stayed on \textit{top} (\textit{bottom}) in previous quarter remained in the same tercile in the next quarter. Even after one year, the consistency does not change much with 81.76\% of analysts stayed on \textit{top} and 79.6\% remained at the \textit{bottom}.

In the case of EPS (panel B), for the  \naive{} ranking   67.87\% and  54.86\% (51.88\% and  35.29\%) of the analysts  remained in the \textit{top} and \textit{bottom} terciles, respectively,  after one quarter (year). For the case of \default{} ranking, 67.19\% and  53.03\% (53.1\% and  33.71\%) of the analysts stayed on \textit{top} and \textit{bottom} respectively after one quarter (year).

These results are consistent with the recent findings of~\cite{hilary2013} on analyst forecast consistency.

%'
%' \begin{table}
%' \caption{Example of label ranking}
%' \ The table shows the example of label ranking problem. In this example, we have three brokers and values of independent variables $x_1 \ldots x_4$. Our goal is to predict the rankings for the period $t+1$, given the values of independent variables and rankings known up to period $t$. For example, at  $t=3$ \true{} is $\{1,2,3\}$, \naive{} is $\{2,3,1\}$, and \default{} is $\{A=(1+2)/2,B=(2+3)/2,C=(3+1)/2\} \Rightarrow \{1.5,2.5,2.0\} \Rightarrow \{1,3,2\}$.
%' \begin{center}
%'  \begin{tabular}{cccccccc}
%' \toprule
%' Period & $x_1$ & $x_2$ & $x_3$ & $x_4$ &\multicolumn{3}{c}{Ranks}\\
%' \cline{6-8}
%' &&&&&Alex&Brown&Credit\\
%' \midrule
%' <<ch4-table.rank,echo=FALSE,results='asis'>>=
%' data <- read.csv("~/Dropbox/workspace/Naive.Bayes.separate.functions/cont.data.csv", header = T, sep=",")
%' print(xtable(data[1:7,2:8],display=c('f','f','f','f','f','d','d','d')),only.contents=T,include.colnames=FALSE,include.rownames=T,hline.after=NULL, format.args=list(big.mark = " ", decimal.mark = "."))
%' @
%' \bottomrule
%'  \end{tabular}
%'  \end{center}
%' \label{ch4-tab:ranking-example}
%' \end{table}


% Descritive rankings
%\begin{landscape}
\begin{table}
\caption{Analysts' ranking consistency}
\label{ch4:tab-rank-stat}
\ This contingency table shows changes in analysts'  \textit{top}, \textit{middle}, \textit{bottom} ranking bins. Panel A (Panel B) depicts the dynamics of the analysts' ranks  based on the accuracy in target prices (EPS forecasts). Rankings of the \naive{} is the case of ranking information know at $t-1$ and the \default{}  is the case of using all ranking information for up to $t-1$. %State \last{} is the last known value of the independent variables; \diff{} is the first differencing; \random{} is the random part of the time series decomposition, and \rollsd{} is the moving standard deviation for previous 8 quarters.

\begin{tabularx}{\linewidth}{r*{10}{Y}}
    \toprule
&\multicolumn{3}{c}{$top_t$}&\multicolumn{3}{c}{$middle_t$}&\multicolumn{3}{c}{$bottom_t$} \\
\midrule
&$top$&$mid$&$bottom$&$top$&$mid$&$bottom$&$top$&$mid$&$bottom$\\
\midrule
\multicolumn{10}{l}{\textbf{Panel A: TP}}\\
  &&\multicolumn{8}{c}{$t+1$}\\ 
\naive{} & 67.8 & 22.1 & 10.1 & 29.9 & 48.3 & 21.8 & 13.3 & 17.0 & 69.7 \\ 
  \default{} & 92.3 & 7.2 & 0.5 & 9.0 & 83.3 & 7.8 & 0.5 & 7.4 & 92.1 \\ 
    &&\multicolumn{8}{c}{$t+4$}\\ 
\naive{} & 46.0 & 28.1 & 25.9 & 39.2 & 29.4 & 31.4 & 32.3 & 26.7 & 41.1 \\ 
  \default{} & 81.8 & 15.2 & 3.0 & 19.2 & 64.3 & 16.5 & 2.9 & 17.5 & 79.6 \\ 
  
\end{tabularx}
\begin{tabularx}{\linewidth}{r*{10}{Y}}
\midrule
\multicolumn{10}{l}{\textbf{Panel B: EPS}}\\
\midrule
%&&$top$&$middle$&$bottom$&Sum\\
  &&\multicolumn{8}{c}{$t+1$}\\ 
\naive{} & 48.3 & 26.2 & 26.1 & 48.1 & 26.3 & 25.9 & 45.8 & 26.3 & 28.7 \\ 
  \default{} & 89.7 & 9.0 & 1.3 & 11.5 & 78.0 & 10.5 & 1.2 & 10.1 & 88.8 \\ 
    &&\multicolumn{8}{c}{$t+4$}\\ 
\naive{} & 47.0 & 28.2 & 26.1 & 45.0 & 27.2 & 28.5 & 43.5 & 27.9 & 31.4 \\ 
  \default{} & 79.3 & 16.6 & 4.1 & 20.9 & 60.2 & 18.9 & 4.2 & 18.9 & 76.9 \\ 
  
\bottomrule
\end{tabularx}
\end{table}
%\end{landscape}

\subsection{Views: descriptive statistics}


\ref{ch4:view-stat} presents the descriptive statistics of the analysts' expected returns. The expected returns  are computed comparing TP estimates with actual prices. To form the smart strategies we compute rank-weighted estimates where weights are given  by the TP rankings.

\cite{bradshaw2002} reports analyst average expected returns for the period of 2000--2009 and 206 ERFs of 24\%.~\cite{da2011} report an average expected return of 40\% for the period of 1996--2004.~\cite{zhou2013} finds an average expected return of 96\% for the sample period of 2000--2009. These figures suggest that analysts are overly optimistic.
%these values from the historical perspective, i.e., how the values of expected stock returns go inline with historical stock returns.~\cite{bodie2009} show that arithimatic average rate of return for the U.S. large stocks (S\&P 500) for the period of 1926--2005 is 10.17\% and the average rate of excess return is 8.39\% with the risk premium estimated 6--8\%. While it is not the best idea to extrapolate the historical values, still, we can say that the expected stocks return should be around 14--16\%. Clearly, the values presented in the literature shows that analysts are very optimistic in issuing target price reports.

Panel A of \ref{ch4:view-stat} show the statistics for the consensus expectations as defined in \ref{ch4-consq}. As mentioned above in \ref{ch4-def-q}, the consensus views have equal weights among the analysts, regardless of their ranks; thus, for the cases of \tr{}, \naive{}, and \default{}, the median is the same regardless of knowing or not the present or past rankings ($Q_{cons}$ in  \ref{ch4-consq}). As such, the mean, median, and standard deviation are the same and independent of analysts' information environment. However, since views also include the confidence (\ref{ch4-eq-cv}), which is based on analysts past performance, the results of the trading strategy based on consensus expectations will be different for the \naive{} and the \default{} information sets.


%and for the \same{} sample of stocks the mean is test.a['Total',,'CONS','mean','same'][[2]]*100 \%.

Panel B of the table shows the TP accuracy weighted average expected returns. For each information environment (\tr{}, \naive{}, \default{}, \last{}, \diff{}, \random{}, and \rollsd{} ) the average expected return  is respectively 0.15\%, 0.16\%,  0.12\%, 0.14\%, 0.14\%, 0.14\%, and 0.14\%.

%and for the \same{} (test.a['Total',,'PT','mean','same'][[1]]*100 \%, test.a['Total',,'PT','mean','same'][[2]]*100 \%, and test.a['Total',,'PT','mean','same'][[3]]*100\%)

%Panel C shows the EPS based weighted expected return. The average return for the \tr{}, \naive{}, and \default{} information sets are respectively test.a['Total',,'EPS','mean','all'][[1]] \%, test.a['Total',,'EPS','mean','all'][[2]]\%, and test.a['Total',,'EPS','mean','all'][[3]]\%.

\section{Empirical Results}
\label{ch4-sec:results}

\subsection{Ranking predictions}

\begin{table}
\caption{Discriminative power contribution}
\ The table demonstrates the contributions (in \%) of each of the variables to changes in analysts' rankings. State \last{} is the state with no dynamics in values of the variables, \diff{} is the state with first-difference in values, \random{} is the state that captures the random part of values time-series decomposition,  and \rollsd{} is the state of values sliding 8 quarters standard deviation.

\begin{tabu} to \linewidth{r*{5}{Y}}
\toprule
Variable&\last{}&\diff{}&\random{}&\rollsd{} \\
\midrule

 uncert & 12.84 & 17.53 & 9.69 & 0.03 \\ 
  assym & 3.88 & 4.79 & 3.17 & 0.01 \\ 
  disp & 10.52 & 12.27 & 5.83 & 0.09 \\ 
  btm & 2.34 & 2.47 & 5.72 & 4.29 \\ 
  size & 0.39 & 0.32 & 1.30 & 1.54 \\ 
  dte & 4.46 & 2.72 & 4.81 & 3.47 \\ 
  accr & 0.90 & 0.77 & 1.21 & 1.42 \\ 
  s.ret & 7.48 & 8.11 & 12.83 & 19.38 \\ 
  sec.ret & 0.13 & 1.49 & 3.65 & 2.21 \\ 
  gnp & 12.81 & 15.56 & 18.44 & 27.31 \\ 
  infl & 18.16 & 24.90 & 13.43 & 20.83 \\ 
  vix.ret & 0.08 & 0.22 & 1.43 & 0.19 \\ 
  t.bill & 26.01 & 8.86 & 18.47 & 19.22 \\ 
   \midrule 
Total & 100.00 & 100.00 & 100.00 & 100.00 \\ 
  

\bottomrule
\end{tabu}
\label{ch4:tab-dp}
\end{table}



We report the ranking prediction results in \ref{ch4:tab-rank}. Panel A (B)  presents annual and total ranking accuracy measured as the average Spearman rank correlation between the \true{} and all other label ranking methods obtained from the analysts' target price (EPS) forecasts. Specifically, the table shows  if the label ranking model can better predict rankings than a ``no-model'' setup.  In panel A, we observe that  predicted rankings based on target prices are constantly outperformed the \default{} baseline but fail  the  \naive{}. Thus, our model can predict more accurately rankings of those analysts' whose  relative performance in setting target prices is based on the whole history.

The average accuracy of the predicted rankings based on the EPS forecasts (panel B) demonstrates quite different results. We report that rankings predicted  with the \diff{} state of the variables are more accurate  than those of the baselines. Thus, our label ranking model can predict rankings of analysts' who issue EPS forecast with higher accuracy than those obtained not just from  the \default{} analysts' past performance information (as in case of the target prices) but also from the \naive{} set.

\ref{ch4:fig-accur} depicts the reported average Spearman correlations. We apply the local polynomial regression fitting~\citep{cleveland1992} to smooth the series for a better presentation. The top panel of \ref{ch4:fig-accur} plots the average accuracy of rankings based on the price targets. We observe the accuracy of rankings based on the \naive{} information are constantly above all others; only by the end of the sample period it begins to drop. The \default{} case, on the contrary, drops rapidly at the beginning of the sample period and continues to decrease in value until the end of the period. The accuracy of predicted rankings falls in between of the \naive{} and the \default{} cases. The cases of \diff{} and \random{} show a constant increase in accuracy starting from 2005-2006  and, by the end of the sample period, the \random{} almost reaches the level of the accuracy of \naive{} case while the \diff{} is still in the upward trend.

The bottom panel of \ref{ch4:fig-accur} plots the case of the EPS based rankings. Looking at baselines rankings, i.e., rankings that obtained without label ranking model, we observe that for the first half of the sample period (from 2000 until 2004) the \naive{} rankings  demonstrate a constant increase in the average ranking accuracy. This means that during this period, rankings of the analysts, on average, did not change from one quarter to another. Starting from 2004, the average accuracy of the \naive{} rankings begins to decrease until the end of the sample period. A somewhat interesting pattern shows the average accuracy of the \default{} ranking: similar to the \naive{}, it also has been increasing until 2005 but running below the \naive{}. However, after 2005, the \default{} ranking accuracy outran the \naive{}. Moreover, in times of the financial crisis of 2007-2009, the \default{} demonstrated an increasing trend in ranking accuracy. As far as the predicted rankings are concerned, we observe that the \diff{} average accuracy has been constantly above the other methods and only during the period of crisis it dropped significantly. In fact, except for the \random{}, all predicted methods reach the maximum average accuracy at around 2007, a pre-crisis time, and all, except for the \default{}, show a downward trend afterwards.

Overall, the results of experiment of the predicting the rankings show that it is possible to model the independent variables under different dynamic states with the rankings. Moreover, the label ranking model can predict the rankings that outperform, in terms of average accuracy, the ones  obtained from the past analysts' relative performance. Concretely, we report that the predicted rankings based on the EPS forecasts outperform the baselines under the \diff{} dynamic state of independent variables.




\subsection{Trading strategies}

We perform  a back-test of  trading  strategy consistent with  \cite{aiguzhinov2015a}; namely, we build the ``smart estimates'' from rankings based on the  price targets. The results are presented in \ref{ch4-tab:strategy}. Panel A reports the performance of \Market{} (passive strategy). This strategy showed annualized cumulative return of -3.03\% and annualized Sharpe ratio of -0.18. The average number of stocks used per quarter is 499.98 and the turnover ratio of strategy is 0.05 which demonstrates the ins/outs of the S\&P 500 constituents list.

Panel B of the table demonstrates the results of trading with consensus among analysts about price targets. The annualized cumulative return of this strategy under the \recent{} (\default{}) information set is 0.12\% (0.31\%) and the Sharpe ratio is  0.01 (0.02). This strategy outperforms the \Market{} in both of the information sets with the \default{} outrunning the \naive{} in terms of the annualized cumulative returns.

Panel C of the table demonstrates the results of trading based on analysts' rankings. We observe that consistent with our assumption, the \true{} resulted in the maximum possible annual cumulative return and the Sharpe ratio (4.32\% and  0.29 respectively). Given the hypothetical assumption of the \true{} set, it is not feasible to implement. Tthe next best feasible strategy is
\diff{}
which is based on the predicted rankings from our label ranking model. This strategy yields an annualized cumulative return of 0.83\% and the Sharpe ratio of 0.05 which are higher than those of the \textit{CONS} and \Market{} strategies. Moreover, we report that all strategies based on the predicted rankings with the dynamic states yield higher annualized cumulative returns than those  based on the  \naive{} and the \default{} rankings. The analysis of the sub-periods performance of the \diff{} strategy is depicted in~\ref{ch4-tab:substrategy}. The table shows the value of Sharp ratio for the 5-year periods. We observe that the \diff{} strategy was dominant in most of the periods.

To test the significance of the annualized cumulative returns of strategies based on the predicted rankings we perform a null-hypothesis pairwise test when \emph{null} is the difference in returns is zero. \ref{ch4:tab-sig} presents the test results. We report that the returns of the strategies based on the predicted rankings with the \diff{} and the \random{} dynamic states are statistically significant at 1\% when compared with the returns of all other strategies. The test accepts the \emph{null} for the returns obtained for the strategies based on the predicted rankings of the \last{} and the \rollsd{} states.


\ref{ch4-fig:bl-results} plots the graphical representation of the cumulative returns for all  trading strategies. We observe that for the case of strategy based on the analysts' rankings, the \true{} strategy is always on top of all  others. This implies that in the settings where analysts' expected returns and rankings are based on price targets, an investor can gain a maximum results from trading strategy.


%\section{Conclusion}
%\label{ch4-sec:conclusion}
%Some institutions, such as StarMine, rank financial analysts based on their accuracy and investment value performance. These rankings are published and are relevant: stocks favored by top-ranked analysts will probably receive more attention from investors. Therefore, there is a growing interest in understanding the relative performance of analysts.

%In this paper we show that it is possible to model analysts' rankings and variables that affect them. With recent findings from Machine Learning body of research in label ranking, we adapted the algorithm to predict the rankings of financial analysts based on price targets and EPS forecasts. We report that, in case of price targets,  our predicted rankings are more accurate than those obtained from using information of  the whole history of analysts performance. For the rankings based on EPS forecasts, our model is able to predict rankings that are better than those of the \default{} and the \naive{} baselines. Moreover, the supremacy of our model above the baselines occurs when  the variables that characterize analysts' information environment exhibit a stationary behavior expressed as the first-difference.

%We also performed a back-test of active trading using the predicted rankings as inputs for the Black-Litterman model. The results showed that the strategies based on the analysts' rankings outperform, in terms of the annualized cumulative return, a  strategy based on the analysts' consensus. Of the ranking based trading strategies, the maximum annualized cumulative return yields  a strategy that is based on the predicted rankings with the first-difference of state variables.

%The results of our work open many opportunities for future research. For example, in this paper we use the classical interpretation of the Black-Litterman model where risk is measured as a standard deviation. Recent work suggests utilizing more complex measures such as value-at-risk and high-moments approaches.

%%We analyzed the rankings and concluded that most errors in rankings were done at the bottom. Consistent with the literature, we confirm that there exists a subset of analysts who issue informative forecasts and this subset is consisted form one period to another.
%We perform two tasks of the label ranking problem of the financial analysts. First, we successfully perform the predicting part of the problem by adapting the existing LR algorithm. Our results were able to outperform both of the baselines: the naive rankings and the default. Based on the average ranking accuracy,  the best result of the experiment was achieved with a method in which the attributes were aggregated applying the rolling standard deviation. This finding suggests that analysts, in the  process of their interpretation of information, rely on  stability of the time series at least for 8 quarters.

%%We applied the forecasted rankings to the simulations of stock trading and reported a profitable trading strategy based on the annualized cumulative returns. We created a perfect foresight portfolio in which we would know the actual rankings \textit{ex-ante}. The portfolio based on these rankings out-performs the market. We conclude that, rankings can identify the best analysts and leveraging the recommendations of these analysts produces the profitable outcomes. In addition, we conclude that the best possible scenario of trading strategies is the one that base the rankings on price target errors. It follows, that investors are better off analysts who issue the price target forecast.

%%For the future research we would like to develop new methods in forecasting the rankings of the analyst that can out-perform the simple last period ranking method.



\begin{table}
	\caption{Example of label ranking problem}
	\begin{center}
		\begin{tabular}{cccccccc}
			\toprule
			Period & $\mathcal{V}_1$ & $\mathcal{V}_2$ & $\mathcal{V}_3$ & $\mathcal{V}_4$ &\multicolumn{3}{c}{Ranks}\\
			\cline{6-8}
			&&&&&Alex&Brown&Credit\\
			\midrule
   1 & $x_{1,1}$ & $x_{1,2}$ & $x_{1,3}$ & $x_{1,4}$ &   1 &   2 &   3 \\ 
    2 & $x_{2,1}$ & $x_{2,2}$ & $x_{2,3}$ & $x_{2,4}$ &   2 &   3 &   1 \\ 
    3 & $x_{3,1}$ & $x_{3,2}$ & $x_{3,3}$ & $x_{3,4}$ &   1 &   2 &   3 \\ 
    4 & $x_{4,1}$ & $x_{4,2}$ & $x_{4,3}$ & $x_{4,4}$ &   3 &   2 &   1 \\ 
    5 & $x_{5,1}$ & $x_{5,2}$ & $x_{5,3}$ & $x_{5,4}$ &   3 &   2 &   1 \\ 
    6 & $x_{6,1}$ & $x_{6,2}$ & $x_{6,3}$ & $x_{6,4}$ &   2 &   1 &   3 \\ 
    7 & $x_{7,1}$ & $x_{7,2}$ & $x_{7,3}$ & $x_{7,4}$ &   1 &   2 &   3 \\ 
  
			\bottomrule
		\end{tabular}
	\end{center}
	\label{ch4-tab:ranking-example}
\end{table}



\begin{table}
  \caption{Sample Statistics}
  \label{ch4-tab:ret-stat}
\ This table shows the average number of target prices (panel A) and EPS forecasts (panel B) per stock per quarter.

\begin{tabularx}{\linewidth}{r*{6}{Y}}
\toprule
    & Min&Mean&Median&Max&Std.dev\\
\midrule
\multicolumn{6}{l}{\textbf{Panel A: TP}} \\
\midrule
 1999 &    3 & 4.144 &    4 &   10 & 1.549 \\ 
  2000 &    3 & 4.512 &    4 &   14 & 1.806 \\ 
  2001 &    3 & 4.873 &    4 &   16 & 2.188 \\ 
  2002 &    3 & 5.436 &    5 &   19 & 2.733 \\ 
  2003 &    3 & 5.763 &    5 &   21 & 3.073 \\ 
  2004 &    3 & 5.931 &    5 &   21 & 3.177 \\ 
  2005 &    3 & 6.042 &    5 &   21 & 3.207 \\ 
  2006 &    3 & 5.991 &    5 &   20 & 3.094 \\ 
  2007 &    3 & 5.755 &    5 &   20 & 2.953 \\ 
  2008 &    3 & 5.325 &    5 &   18 & 2.602 \\ 
  2009 &    3 & 4.535 &    4 &   18 & 1.958 \\ 
   \midrule 
Total &    3 & 5.522 &    5 &   21 & 2.858 \\ 
  
%\bottomrule
\end{tabularx}
%\end{table}

\begin{tabularx}{\linewidth}{r*{6}{Y}}
\midrule
\multicolumn{6}{l}{\textbf{Panel B: EPS}} \\
\midrule
 1999 &    3 & 5.165 &    4 &   17 & 2.758 \\ 
  2000 &    3 & 5.017 &    4 &   17 & 2.561 \\ 
  2001 &    3 & 5.418 &    5 &   18 & 2.758 \\ 
  2002 &    3 & 5.532 &    5 &   21 & 2.965 \\ 
  2003 &    3 & 5.643 &    5 &   24 & 3.034 \\ 
  2004 &    3 & 5.835 &    5 &   22 & 3.293 \\ 
  2005 &    3 & 5.933 &    5 &   24 & 3.328 \\ 
  2006 &    3 & 6.042 &    5 &   26 & 3.383 \\ 
  2007 &    3 & 5.841 &    5 &   22 & 3.103 \\ 
  2008 &    3 & 5.318 &    4 &   22 & 2.653 \\ 
  2009 &    3 & 4.998 &    4 &   20 & 2.425 \\ 
   \midrule 
Total &    3 & 5.601 &    5 &   26 & 3.024 \\ 
  
\bottomrule
\end{tabularx}
\end{table}


\begin{table}
  \caption{Descriptive statistics of views}
  \label{ch4:view-stat}
\ This table shows the descriptive statistics of views (expected returns) based on the consensus (median) among the analysts (panel A) and target price rankings (panel B). State \last{} is the state with no dynamics in values of the variables, \diff{} is the state with first-difference in values, \random{} is the state that captures the random part of values time-series decomposition,  and \rollsd{} is the state of values sliding 8 quarters standard deviation.

\begin{tabularx}{\linewidth}{r*{4}{Y}}
\toprule
& Mean (in \%)&Median (in \%)&Std.dev\\
\midrule
  \multicolumn{4}{c}{\textbf{Panel A: Consensus}} \\ 
 \midrule 
\tr{} & 18.610 & 16.889 & 0.120 \\ 
  \naive{} & 18.610 & 16.889 & 0.120 \\ 
  \default{} & 18.610 & 16.889 & 0.120 \\ 
  \last{} & 18.610 & 16.889 & 0.120 \\ 
  \diff{} & 18.610 & 16.889 & 0.120 \\ 
  \random{} & 18.610 & 16.889 & 0.120 \\ 
  \rollsd{} & 18.610 & 16.889 & 0.120 \\ 
  
\end{tabularx}

\begin{tabularx}{\linewidth}{r*{7}{Y}}
  \midrule
  \multicolumn{4}{c}{\textbf{Panel B: TP}} \\ 
 \midrule 
\tr{} & 14.876 & 13.380 & 0.096 \\ 
  \naive{} & 15.742 & 14.314 & 0.098 \\ 
  \default{} & 12.459 & 10.591 & 0.089 \\ 
  \last{} & 13.714 & 12.118 & 0.094 \\ 
  \diff{} & 13.701 & 12.042 & 0.095 \\ 
  \random{} & 13.910 & 12.267 & 0.094 \\ 
  \rollsd{} & 13.690 & 12.119 & 0.095 \\ 
  
\bottomrule
\end{tabularx}
\end{table}
%'   \begin{tabularx}{\linewidth}{r*{7}{Y}}
%'   \midrule
%' <<ch4desc-q-eps,echo=F,results='asis'>>=
%' print(xtable(acast(melt(test.a[12,,'EPS',1:3]),Var1~Var2,value.var='value'),display=c('s','f','f','f'),digits=3),only.contents=T,include.colnames=FALSE,hline.after=NULL, format.args=list(big.mark = " ", decimal.mark = "."),add.to.row=list(pos=list(0,0),command=c('\\multicolumn{4}{c}{\\textbf{Panel C: EPS}} \\\\ \n','\\midrule \n')),sanitize.text.function = function(x) x)
%' @
%' \bottomrule
% \end{tabularx}
%\end{table}


\begin{table}
\caption{Average ranking accuracy}
\ The table presents the average Spearman correlation between \tr{} and  predicted rankings that are based on accuracy of price target (panel A) and on EPS forecasts (panel B) compared to baselines: \tr{} shows the case of the known future information; \naive{} is the case of ranking information know at $t-1$, and the \default{}  is the case of using all ranking information for up to $t-1$. State \last{} is the state with no dynamics in values of the variables, \diff{} is the state with first-difference in values, \random{} is the state that captures the random part of values time-series decomposition,  and \rollsd{} is the state of values sliding 8 quarters standard deviation.

\begin{tabu} to \linewidth{r*{8}{Y}}
\toprule
Year&\true{}&\naive{}&\default{}&\last{}&\diff{}&\random{}&\rollsd{} \\
\midrule
\multicolumn{8}{l}{\textbf{Panel A: TP}} \\
\midrule
 2000 & 1.0000 & 0.5988 & 0.4918 & 0.5695 & 0.5706 & 0.5312 & 0.5728 \\ 
  2001 & 1.0000 & 0.6600 & 0.4515 & 0.5355 & 0.6166 & 0.6058 & 0.6030 \\ 
  2002 & 1.0000 & 0.6092 & 0.3770 & 0.3960 & 0.4436 & 0.5310 & 0.4374 \\ 
  2003 & 1.0000 & 0.6168 & 0.3379 & 0.4812 & 0.4771 & 0.5372 & 0.5515 \\ 
  2004 & 1.0000 & 0.5500 & 0.2542 & 0.4031 & 0.2559 & 0.4610 & 0.2517 \\ 
  2005 & 1.0000 & 0.5862 & 0.2965 & 0.5114 & 0.4622 & 0.4100 & 0.5466 \\ 
  2006 & 1.0000 & 0.5908 & 0.3087 & 0.5323 & 0.4583 & 0.5804 & 0.4279 \\ 
  2007 & 1.0000 & 0.5787 & 0.2440 & 0.4823 & 0.2443 & 0.4703 & 0.3483 \\ 
  2008 & 1.0000 & 0.6573 & 0.3012 & 0.4720 & 0.5380 & 0.5247 & 0.6346 \\ 
  2009 & 1.0000 & 0.5720 & 0.2093 & 0.3902 & 0.4434 & 0.5507 & 0.3231 \\ 
   \midrule 
Total & 1.0000 & 0.6007 & 0.3175 & 0.4746 & 0.4415 & 0.5167 & 0.4642 \\ 
  
\midrule
\end{tabu}

\begin{tabu} to \linewidth{r*{8}{Y}}
\multicolumn{8}{l}{\textbf{Panel B: EPS}} \\
\midrule
 2000 & 1.0000 & 0.0344 & 0.0381 & 0.0420 & 0.0501 & 0.0461 & 0.0486 \\ 
  2001 & 1.0000 & -0.0028 & -0.0147 & -0.0178 & -0.0026 & -0.0271 & -0.0081 \\ 
  2002 & 1.0000 & 0.0386 & 0.0132 & -0.0001 & -0.0049 & 0.0089 & 0.0113 \\ 
  2003 & 1.0000 & 0.0489 & 0.0279 & 0.0380 & 0.0632 & 0.0263 & 0.0431 \\ 
  2004 & 1.0000 & 0.0255 & 0.0397 & 0.0021 & 0.0092 & 0.0184 & -0.0414 \\ 
  2005 & 1.0000 & 0.0465 & 0.0369 & 0.0450 & 0.0297 & 0.0246 & 0.0380 \\ 
  2006 & 1.0000 & 0.0187 & 0.0416 & 0.0162 & 0.0367 & 0.0062 & 0.0332 \\ 
  2007 & 1.0000 & 0.0358 & 0.0246 & 0.0468 & 0.0535 & 0.0392 & 0.0469 \\ 
  2008 & 1.0000 & 0.0066 & 0.0222 & 0.0318 & 0.0110 & 0.0194 & 0.0110 \\ 
  2009 & 1.0000 & 0.0106 & 0.0240 & 0.0261 & 0.0198 & 0.0087 & 0.0228 \\ 
   \midrule 
Total & 1.0000 & 0.0261 & 0.0264 & 0.0243 & 0.0268 & 0.0173 & 0.0205 \\ 
  

\bottomrule
\end{tabu}
\label{ch4:tab-rank}
\end{table}

\begin{figure}

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}
\includegraphics[width=\maxwidth]{figure/ch4-rank-fig-1} 

\end{knitrout}
\caption{Average Spearman correlation ($\rho$)}
\ The figure plots the smoothed series of the average Spearman correlation between \tr{} and predicted rankings resulted from static and dynamic states of variables: \last{} is the state with no dynamics in values of the variables, \diff{} is the state with first-difference in values, \random{} is the state that captures the random part of values time-series decomposition,  and \rollsd{} is the state of values sliding 8 quarters standard deviation. Smoothing  is done by applying a local polynomial regression fitting (loess smoothing)~\citep{cleveland1992}. The top panel shows results of rankings based on the EPS forecast accuracy, the bottom are based on the price target accuracy.
\label{ch4:fig-accur}
\end{figure}


\begin{table}
  \caption{Trading strategy performance}
  \label{ch4-tab:strategy}
  \ The table presents the annualized cumulative statistics of the strategy performance based on PT rankings. \true{} is actual ranking of the analysts. \naive{} is the rankings from the last period. \default{} is the average rank of an analyst for up to the last period. Trading period is from  2000Q1 until 2009Q4. Panel A presents the results from the passive strategy. Panel B summarizes the results of the strategy with rankings based on consensus in price targets. Panel C shows the case  of the strategy with rankings based on price targets. State \last{} is the state with no dynamics in values of the variables; \diff{} is the state with first-difference in values; \random{} is the state that captures the random part of values time-series decomposition;  and \rollsd{} is the state of values sliding 8 quarters standard deviation.
  \begin{tabularx}{\linewidth}{r*{5}{Y}}
    \toprule
Strategy&Annualized cum. return (in \%)&Annualized Std. dev (in \%)&Sharpe ratio&Average num. stock&Average turnover rate \\  \midrule 
 \multicolumn{5}{l}{\textbf{Panel A}} \\ 
\textit{Market} & -3.032 & 16.654 & -0.182 &  499 & 0.053 \\ 
  
  \end{tabularx}
\begin{tabularx}{\linewidth}{r*{5}{Y}}
    \midrule
    \multicolumn{5}{l}{\textbf{Panel B: CONS}} \\
    \midrule
 \naive{} & 0.116 & 15.948 & 0.007 &  283 & 0.256 \\ 
  \default{} & 0.314 & 15.773 & 0.020 &  283 & 0.228 \\ 
  
  \end{tabularx}

\begin{tabularx}{\linewidth}{r*{5}{Y}}
    \midrule
    \multicolumn{5}{l}{\textbf{Panel C: TP}} \\
    \midrule
 \true{} & 4.325 & 14.697 & 0.294 &  283 & 0.345 \\ 
  \naive{} & 0.282 & 15.662 & 0.018 &  284 & 0.264 \\ 
  \default{} & 0.689 & 15.565 & 0.044 &  284 & 0.256 \\ 
  \last{} & 0.547 & 15.759 & 0.035 &  251 & 0.266 \\ 
  \diff{} & 0.830 & 15.742 & 0.053 &  251 & 0.276 \\ 
  \random{} & 0.690 & 15.715 & 0.044 &  251 & 0.262 \\ 
  \rollsd{} & 0.738 & 15.726 & 0.047 &  251 & 0.270 \\ 
  
\bottomrule
\end{tabularx}
\end{table}
%'   \begin{tabularx}{\linewidth}{r*{5}{Y}}
%'     \midrule
%'     \multicolumn{5}{l}{\textbf{Panel D: EPS}} \\
%'     \midrule
%' <<ch4-bl-eps,echo=F,results='asis'>>=
%' results.final <- bl.results[1:7,1,,eps]
%' rownames(results.final) <- paste0('\\',gsub('[[:punct:]]|[[:digit:]]','',c(baselines,methods)),'{}')
%' print(xtable(results.final,display=c('s','f','f','f','d','f'),digits=3),only.contents=T,include.colnames=FALSE,hline.after=NULL, format.args=list(big.mark = " ", decimal.mark = "."),sanitize.text.function = function(x) x)
%' @
%'     \bottomrule
%'   \end{tabularx}
%\end{table}



\begin{landscape}
\begin{table}
%\small\addtolength{\tabcolsep}{-2pt}
\caption{Trading strategy performance: Sharpe ratio}
\label{ch4-tab:substrategy}
\ This table presents the Sharpe ratio of each of the trading strategies: the passive (\textit{Market}) and the active (consensus and smart estimates) calculated for different holding periods. Panel A represents the perfect foresight information set; panels B and C show, respectively, the recent and the all history analysts' performance. State \last{} is the state with no dynamics in values of the variables, \diff{} is the state with first-difference in values, \random{} is the state that captures the random part of values time-series decomposition,  and \rollsd{} is the state of values sliding 8 quarters standard deviation.

\begin{tabu} to \linewidth{r*{9}{Y}}
\toprule

%<<ch4-sr-con,echo=F,results='asis'>>=
%cat(c('Periods','&',paste0('\\',gsub('[[:punct:]]|[[:digit:]]','',c(baselines,methods,'market')),'{}',collapse = "&"),'\\\\'))
%cat('\\midrule')
%print(xtable(periods.array[,1,,'yes','sr',con],display=c('s',rep('f',8)),digits=3),only.contents=T,include.colnames=F,hline.after=NULL, format.args=list(big.mark = " ", decimal.mark = "."),add.to.row=list(pos=list(0,0),command=c('\\multicolumn{9}{l}{\\textbf{Panel A: \\textit{CONS}}} \\\\\n','\\midrule \n')))
%@
%\midrule
%\end{tabu}


%\begin{tabu} to \linewidth{r*{9}{Y}}
Periods & \Market{}&\true{}&\naive{}&\default{}&\last{}&\diff{}&\random{}&\rollsd{} \\\midrule 2000Q1/2004Q4 & -0.201 & 0.395 & 0.168 & 0.205 & 0.218 & 0.224 & 0.223 & 0.208 \\ 
  2001Q1/2005Q4 & -0.093 & 0.425 & 0.214 & 0.249 & 0.232 & 0.242 & 0.239 & 0.222 \\ 
  2002Q1/2006Q4 & 0.196 & 0.757 & 0.490 & 0.464 & 0.525 & 0.538 & 0.529 & 0.511 \\ 
  2003Q1/2007Q4 & 0.925 & 1.915 & 1.248 & 1.245 & 1.248 & 1.305 & 1.250 & 1.303 \\ 
  2004Q1/2008Q4 & -0.435 & 0.070 & -0.305 & -0.289 & -0.308 & -0.292 & -0.308 & -0.283 \\ 
  2005Q1/2009Q4 & -0.158 & 0.189 & -0.125 & -0.105 & -0.136 & -0.106 & -0.124 & -0.106 \\ 
   \midrule 
All period & -0.182 & 0.294 & 0.018 & 0.044 & 0.035 & 0.053 & 0.044 & 0.047 \\ 
  
 \bottomrule
 \end{tabu}
 \end{table}
\end{landscape}
%'
%'  \begin{tabu} to \linewidth{r*{9}{Y}}
%'  <<ch4-sr-eps,echo=F,results='asis'>>=
%'  print(xtable(periods.array[,1,,'yes','sr',eps],display=c('s',rep('f',8)),digits=3),only.contents=T,include.colnames=F,hline.after=NULL, format.args=list(big.mark = " ", decimal.mark = "."),add.to.row=list(pos=list(0,0),command=c('\\multicolumn{9}{l}{\\textbf{Panel C: \\textit{EPS}}} \\\\\n','\\midrule \n')))
%'  @
%' \bottomrule
%' \end{tabu}


\begin{landscape}
\begin{table}
\caption{Significance of cumulative returns}
\label{ch4:tab-sig}
\ The table demonstrates a pairwise  test in difference of the cumulative returns of all strategies vs. those based on the predicted rankings.  Case of \tr{} shows  the known future information; \naive{} is the case of ranking information know at $t-1$, and the \default{} is the case of using all ranking information for up to $t-1$. State \last{} is the state with no dynamics in values of the variables, \diff{} is the state with first-difference in values, \random{} is the state that captures the random part of values time-series decomposition,  and \rollsd{} is the state of values sliding 8 quarters standard deviation.
\begin{tabularx}{\linewidth}{l*{9}{Y}}
  \toprule
  &\multicolumn{2}{c}{\last{}}&\multicolumn{2}{c}{\diff{}}&\multicolumn{2}{c}{\random{}}&\multicolumn{2}{c}{\rollsd{}}\\
  &t value& Pr$(>\vert t\vert)$ & t value & Pr$(>\vert t\vert)$&t value& Pr$(>\vert t\vert)$ & t value & Pr$(>\vert t\vert)$\\
\midrule
 \tr{} & 8.329 & 0.000 & 8.383 & 0.000 & 8.237 & 0.000 & 8.698 & 0.000 \\ 
  \naive{} & -28.526 & 0.000 & -23.989 & 0.000 & -29.860 & 0.000 & -24.316 & 0.000 \\ 
  \default{} & -5.521 & 0.000 & -13.647 & 0.000 & -8.831 & 0.000 & -8.929 & 0.000 \\ 
  \last{} & - & - & -6.934 & 0.000 & -8.509 & 0.000 & -0.910 & 0.368 \\ 
  \diff{} & 6.934 & 0.000 & - & - & 4.521 & 0.000 & 10.374 & 0.000 \\ 
  \random{} & 8.509 & 0.000 & -4.521 & 0.000 & - & - & 2.731 & 0.009 \\ 
  \rollsd{} & 0.910 & 0.368 & -10.374 & 0.000 & -2.731 & 0.009 & - & - \\ 
  \Market{} & -15.357 & 0.000 & -15.168 & 0.000 & -15.361 & 0.000 & -15.166 & 0.000 \\ 
  
\bottomrule
\end{tabularx}
\end{table}
\end{landscape}

\begin{figure}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}
\includegraphics[width=\maxwidth]{figure/ch4-bl-results-fig-1} 

\end{knitrout}
\caption{Performance of BL model}
\label{ch4-fig:bl-results}
\ In this figure we show the quarterly performance of the cumulative portfolio wealth for all strategies.  State \last{} is the state with no dynamics in values of the variables, \diff{} is the state with first-difference in values, \random{} is the state that captures the random part of values time-series decomposition,  and \rollsd{} is the state of values sliding 8 quarters standard deviation.
\end{figure}
\processdelayedfloats
\cleardoublepage

\chapter{Conclusion}
\label{conclusion}
\input{./conclusion/conclusion}
\cleardoublepage


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  %% BIBLIOGRAPHY
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\bookmarksetup{startatroot}								% Reset pdf bookmark to initial
	\addtocontents{toc}{\bigskip}							% To add a little space after the last entry on TOC
	\renewcommand{\bibname}{References}					% Change "Bibliography" to "References" if needed
\phantomsection
	\addcontentsline{toc}{chapter}{References}			% Change "Bibliography" to "References" if needed
	\begin{singlespace}
%		\bstctlcite{IEEEexample:BSTcontrol}					% For avoiding the dashed line in IEEE format (Do not modify)
%		\nocite{*}											% Use with "Bibliography", comment if you are using "References"
		%\bibliographystyle{IEEEtranN}						% Use with "References", comment if you are using "Bibliography" (sorted by order of appearance)
		\bibliographystyle{chicago}						% Use with "Bibliography", comment if you are using "References" (alphabetically sorted)
		\bibliography{./thesis}							% Bibliography/References [.bib file]
	\end{singlespace}
	\clearemptydoublepage


	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	%% Index
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\PrintIndex
%\clearemptydoublepage

	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	%% APPENDIX
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	%\appendix
	%\input{Appendix}					% Appendix [.tex file]
	%\clearemptydoublepage

\end{document}
